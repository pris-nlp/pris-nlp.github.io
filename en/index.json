[{"authors":["WeiranXu"],"categories":null,"content":"His research focuses on solving text processing problems based on machine learning and pattern recognition, such as text classification, information retrieval, information extraction and propensity judgment. Since 1997, he has been engaged in the field of pattern recognition and machine learning, and since 2003, he has been specialized in the research of text data machine learning. He participated in TREC, TAC, 863, COAE and other related evaluations since 2004, and won the first place in individual and comprehensive scores for many times. He was responsible for building a series of prototype systems; he has participated in a number of National Natural Science Foundation of China, 863 projects and major national science and technology projects as a major member. He has published nearly 10 papers including ACL, AAAI, SIGIR and other top conference papers, more than 10 SCI indexed journal papers, more than 50 EI indexed papers.\nThe network contains all kinds of useful information, and the bottleneck is how to get it automatically. His long-term research has been to enable computers to automatically understand the content of text and provide services to people voluntarily. There is still a lot of room for improvement in the capabilities of machines compared to the capabilities of people. But it is still very difficult for machines to comprehensively surpass people in the present situation.\nAt present, the main research problem is to organize the content of text with entity or event as the center, so as to solve the problems of information extraction, information retrieval, text classification and tendency judgment. The main theories and methods adopted are representation learning theory and complex network theory. Deep Learning in Representation Learning (or Feature Learning) has achieved excellent results in image and speech processing. The theory of representation learning is still in the preliminary stage of research, and the commonly used methods mainly include \u0026ldquo;probability model\u0026rdquo;, \u0026ldquo;automatic coding\u0026rdquo; and \u0026ldquo;manifold learning\u0026rdquo;. The dynamic model proposed by Professor Jun Guo in our lab based on complex network has good effect on mining and representing various factors and their correlation relations. Therefore, applying it in the framework of presentation learning theory will better solve the problem of text content extraction and representation.\n","date":1738454400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1622937600,"objectID":"f7f1828bc4d3f77885779338384d5198","permalink":"https://pris-nlp.github.io/en/author/weiran-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/weiran-xu/","section":"authors","summary":"His research focuses on solving text processing problems based on machine learning and pattern recognition, such as text classification, information retrieval, information extraction and propensity judgment. Since 1997, he has been engaged in the field of pattern recognition and machine learning, and since 2003, he has been specialized in the research of text data machine learning.","tags":null,"title":"Weiran Xu","type":"authors"},{"authors":["PengdaQin"],"categories":null,"content":"","date":1581033600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"f7764d6cc9ff989e72c6123449dee748","permalink":"https://pris-nlp.github.io/en/author/pengda-qin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/pengda-qin/","section":"authors","summary":"","tags":null,"title":"Pengda Qin","type":"authors"},{"authors":["KeqingHe"],"categories":null,"content":"","date":1738454400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1622937600,"objectID":"4098ec9d59fd00cafd4762e6cc26dab1","permalink":"https://pris-nlp.github.io/en/author/keqing-he/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/keqing-he/","section":"authors","summary":"","tags":null,"title":"Keqing He","type":"authors"},{"authors":["LuluZhao"],"categories":null,"content":"","date":1738454400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"db012a69ae77d6b0e8bbe33ad0999e1f","permalink":"https://pris-nlp.github.io/en/author/lulu-zhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/lulu-zhao/","section":"authors","summary":"","tags":null,"title":"Lulu Zhao","type":"authors"},{"authors":["HongXu"],"categories":null,"content":"","date":1653177600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"1ce4c3f558e380e8ef5a17f048f8c829","permalink":"https://pris-nlp.github.io/en/author/hong-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/hong-xu/","section":"authors","summary":"","tags":null,"title":"Hong Xu","type":"authors"},{"authors":["SihongLiu"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"a8f151fc5ba417733a4a5c9df5c51c0a","permalink":"https://pris-nlp.github.io/en/author/sihong-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/sihong-liu/","section":"authors","summary":"","tags":null,"title":"Sihong Liu","type":"authors"},{"authors":["JiayuZhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"53131a4ccf884d9876c3518f72fe372b","permalink":"https://pris-nlp.github.io/en/author/jiayu-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/jiayu-zhang/","section":"authors","summary":"","tags":null,"title":"Jiayu Zhang","type":"authors"},{"authors":["JingboShi"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"41ff297ad3a3e5415e45e1ed5a816859","permalink":"https://pris-nlp.github.io/en/author/jingbo-shi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/jingbo-shi/","section":"authors","summary":"","tags":null,"title":"Jingbo Shi","type":"authors"},{"authors":["SongyanLiu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bb21e411d0504449ccd9aa0f3548468f","permalink":"https://pris-nlp.github.io/en/author/songyan-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/songyan-liu/","section":"authors","summary":"","tags":null,"title":"Songyan Liu","type":"authors"},{"authors":["YufengLi"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5d4e3a9cfc62f79e5f803eae101f31a5","permalink":"https://pris-nlp.github.io/en/author/yufeng-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/yufeng-li/","section":"authors","summary":"","tags":null,"title":"Yufeng Li","type":"authors"},{"authors":["ZongaiXie"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ba7cbdc78e4130149754f5e69dcdbe8c","permalink":"https://pris-nlp.github.io/en/author/zongai-xie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/zongai-xie/","section":"authors","summary":"","tags":null,"title":"Zongai Xie","type":"authors"},{"authors":["YuanmengYan"],"categories":null,"content":"","date":1669852800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1622937600,"objectID":"fada719e3feff979780c87df53610f4a","permalink":"https://pris-nlp.github.io/en/author/yuanmeng-yan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/yuanmeng-yan/","section":"authors","summary":"","tags":null,"title":"Yuanmeng Yan","type":"authors"},{"authors":["ZhiyuanZeng"],"categories":null,"content":"","date":1669852800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"e87837e276534898a855fdeaea23acd4","permalink":"https://pris-nlp.github.io/en/author/zhiyuan-zeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/zhiyuan-zeng/","section":"authors","summary":"","tags":null,"title":"Zhiyuan Zeng","type":"authors"},{"authors":["YuejieLei"],"categories":null,"content":"","date":1636243200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"ab928303fde91cb59bef21655c1d6356","permalink":"https://pris-nlp.github.io/en/author/yuejie-lei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/yuejie-lei/","section":"authors","summary":"","tags":null,"title":"Yuejie Lei","type":"authors"},{"authors":["ZijunLiu"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"295e050fed27ea220e899cfa8debd2ff","permalink":"https://pris-nlp.github.io/en/author/zijun-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/zijun-liu/","section":"authors","summary":"","tags":null,"title":"Zijun Liu","type":"authors"},{"authors":["YananWu"],"categories":null,"content":"","date":1738454400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1622937600,"objectID":"cc8ce7c377a7104416cd440a54027525","permalink":"https://pris-nlp.github.io/en/author/yanan-wu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/yanan-wu/","section":"authors","summary":"","tags":null,"title":"Yanan Wu","type":"authors"},{"authors":["LiwenWang"],"categories":null,"content":"","date":1697760000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1622937600,"objectID":"adada2facae11e3f486b44e919b8db2a","permalink":"https://pris-nlp.github.io/en/author/liwen-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/liwen-wang/","section":"authors","summary":"","tags":null,"title":"Liwen Wang","type":"authors"},{"authors":["XuefengLi"],"categories":null,"content":"","date":1697760000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"9d7c1c0c900bc56efb9e418a468b6172","permalink":"https://pris-nlp.github.io/en/author/xuefeng-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/xuefeng-li/","section":"authors","summary":"","tags":null,"title":"Xuefeng Li","type":"authors"},{"authors":["HaoLei"],"categories":null,"content":"","date":1683417600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"ca6b7927c276a8d431611c5bb76f3334","permalink":"https://pris-nlp.github.io/en/author/hao-lei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/hao-lei/","section":"authors","summary":"","tags":null,"title":"Hao Lei","type":"authors"},{"authors":["FujiaZheng"],"categories":null,"content":"","date":1657497600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"953924eb0a5f7b1fc0a5c25f4c4f989c","permalink":"https://pris-nlp.github.io/en/author/fujia-zheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/fujia-zheng/","section":"authors","summary":"","tags":null,"title":"Fujia Zheng","type":"authors"},{"authors":["GuantingDong"],"categories":null,"content":"","date":1738454400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"eb44dbee8694f917cc30af1ed6e380a6","permalink":"https://pris-nlp.github.io/en/author/guanting-dong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/guanting-dong/","section":"authors","summary":"","tags":null,"title":"Guanting Dong","type":"authors"},{"authors":["YutaoMu"],"categories":null,"content":"","date":1707868800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"3f2a530a96fc0a7015690a0de3bf8fef","permalink":"https://pris-nlp.github.io/en/author/yutao-mu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/yutao-mu/","section":"authors","summary":"","tags":null,"title":"Yutao Mu","type":"authors"},{"authors":["ChenZeng"],"categories":null,"content":"","date":1697760000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"f967eaa9aadcd6a0176d7f81f87f031e","permalink":"https://pris-nlp.github.io/en/author/chen-zeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/chen-zeng/","section":"authors","summary":"","tags":null,"title":"Chen Zeng","type":"authors"},{"authors":["DaichiGuo"],"categories":null,"content":"","date":1697760000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"da5ab810d8039861073d5dc563cefc8f","permalink":"https://pris-nlp.github.io/en/author/daichi-guo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/daichi-guo/","section":"authors","summary":"","tags":null,"title":"Daichi Guo","type":"authors"},{"authors":["Qixiang Gao"],"categories":null,"content":"","date":1683417600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"15a8a8196173924896cf2b574ccdffd8","permalink":"https://pris-nlp.github.io/en/author/qixiang-gao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/qixiang-gao/","section":"authors","summary":"","tags":null,"title":"Qixiang Gao","type":"authors"},{"authors":["RuotongGeng"],"categories":null,"content":"","date":1683417600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"a7dbd44e4cdf66c3e4433fa1e6f3afe0","permalink":"https://pris-nlp.github.io/en/author/ruotong-geng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/ruotong-geng/","section":"authors","summary":"","tags":null,"title":"Ruotong Geng","type":"authors"},{"authors":["QianyuCao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e9b3152c58f76bd04c003e496373a415","permalink":"https://pris-nlp.github.io/en/author/qianyu-cao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/qianyu-cao/","section":"authors","summary":"","tags":null,"title":"Qianyu Cao","type":"authors"},{"authors":["Pei Wang"],"categories":null,"content":"","date":1738454400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"19c0da4b715ec83728e4cb6f1234cd8d","permalink":"https://pris-nlp.github.io/en/author/pei-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/pei-wang/","section":"authors","summary":"","tags":null,"title":"Pei Wang","type":"authors"},{"authors":["WeihaoZeng"],"categories":null,"content":"","date":1738454400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1650844800,"objectID":"2b545b98f6c5cfccc2d06f5586ef14bd","permalink":"https://pris-nlp.github.io/en/author/weihao-zeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/weihao-zeng/","section":"authors","summary":"","tags":null,"title":"Weihao Zeng","type":"authors"},{"authors":["XiaoshuaiSong"],"categories":null,"content":"","date":1738454400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"41beca47d511de13e4826ff010f30d49","permalink":"https://pris-nlp.github.io/en/author/xiaoshuai-song/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/xiaoshuai-song/","section":"authors","summary":"","tags":null,"title":"Xiaoshuai Song","type":"authors"},{"authors":["JinxuZhao"],"categories":null,"content":"","date":1707782400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"7c23d488463d63e3f6dfe311bf3f2226","permalink":"https://pris-nlp.github.io/en/author/jinxu-zhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/jinxu-zhao/","section":"authors","summary":"","tags":null,"title":"Jinxu Zhao","type":"authors"},{"authors":["TingfengHui"],"categories":null,"content":"","date":1697760000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"1f8ba1d00f3108548beb008d3d0268a2","permalink":"https://pris-nlp.github.io/en/author/tingfeng-hui/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/tingfeng-hui/","section":"authors","summary":"","tags":null,"title":"Tingfeng Hui","type":"authors"},{"authors":["ZechenWang"],"categories":null,"content":"","date":1697760000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1652929200,"objectID":"f79004723c17f8b8115e2c2df6f3097c","permalink":"https://pris-nlp.github.io/en/author/zechen-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/zechen-wang/","section":"authors","summary":"","tags":null,"title":"Zechen Wang","type":"authors"},{"authors":["FangweiXue"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"30086d888ad6a835f2fbc2f112135ddf","permalink":"https://pris-nlp.github.io/en/author/fangwei-xue/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/fangwei-xue/","section":"authors","summary":"","tags":null,"title":"Fangwei Xue","type":"authors"},{"authors":["DayuanFU"],"categories":null,"content":"","date":1738454400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"76be4d7efcf74f3864fbcee0462d4384","permalink":"https://pris-nlp.github.io/en/author/dayuan-fu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/dayuan-fu/","section":"authors","summary":"","tags":null,"title":"Dayuan Fu","type":"authors"},{"authors":["YejieWang"],"categories":null,"content":"","date":1738454400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"087cfd32fce529e7f4fe68ccd6c8eac5","permalink":"https://pris-nlp.github.io/en/author/yejie-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/yejie-wang/","section":"authors","summary":"","tags":null,"title":"Yejie Wang","type":"authors"},{"authors":["MuxiDiao"],"categories":null,"content":"","date":1718150400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"27485895db0cbffc38667796c3ff196a","permalink":"https://pris-nlp.github.io/en/author/muxi-diao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/muxi-diao/","section":"authors","summary":"","tags":null,"title":"Muxi Diao","type":"authors"},{"authors":["ZhengyangWang"],"categories":null,"content":"","date":1718150400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"36b865d02940947e0952da7dae3ea843","permalink":"https://pris-nlp.github.io/en/author/zhengyang-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/zhengyang-wang/","section":"authors","summary":"","tags":null,"title":"Zhengyang Wang","type":"authors"},{"authors":["Lixiong Qin"],"categories":null,"content":"","date":1710374400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"5a39439b1fa96de7145933e0bdd8031d","permalink":"https://pris-nlp.github.io/en/author/lixiong-qin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/lixiong-qin/","section":"authors","summary":"","tags":null,"title":"Lixiong Qin","type":"authors"},{"authors":["WentaoHong"],"categories":null,"content":"","date":1738454400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"9ee93e58203c9d0be21f3fb3c902a962","permalink":"https://pris-nlp.github.io/en/author/wentao-hong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/wentao-hong/","section":"authors","summary":"","tags":null,"title":"Wentao Hong","type":"authors"},{"authors":["ZhuomaGongQue"],"categories":null,"content":"","date":1738454400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"5a38a4ddd005bf78c6e5263d0e0d5b01","permalink":"https://pris-nlp.github.io/en/author/zhuoma-gongque/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/zhuoma-gongque/","section":"authors","summary":"","tags":null,"title":"Zhuoma GongQue","type":"authors"},{"authors":["ZhexuWang"],"categories":null,"content":"","date":1718150400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"d0b3cedd2cac68f64386c54bfcdb2dc1","permalink":"https://pris-nlp.github.io/en/author/zhexu-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/zhexu-wang/","section":"authors","summary":"","tags":null,"title":"Zhexu Wang","type":"authors"},{"authors":["YukaiXu"],"categories":null,"content":"","date":1711843200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"28cfbab71bb202b1a3f2d8327a767198","permalink":"https://pris-nlp.github.io/en/author/yukai-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/yukai-xu/","section":"authors","summary":"","tags":null,"title":"Yukai Xu","type":"authors"},{"authors":["YuchenLiu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2c52b0e5e71aa66429bc6803da6fbe0c","permalink":"https://pris-nlp.github.io/en/author/yuchen-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/yuchen-liu/","section":"authors","summary":"","tags":null,"title":"Yuchen Liu","type":"authors"},{"authors":["JianingYu"],"categories":null,"content":"","date":1718150400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600523752,"objectID":"5e4c7440372d65dbf6aca266eb18dc7b","permalink":"https://pris-nlp.github.io/en/author/jianing-yu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/jianing-yu/","section":"authors","summary":"","tags":null,"title":"Jianing Yu","type":"authors"},{"authors":["YanxuChen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"87bfe365c7627a2b577f9e6e744df63a","permalink":"https://pris-nlp.github.io/en/author/yanxu-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/yanxu-chen/","section":"authors","summary":"","tags":null,"title":"Yanxu Chen","type":"authors"},{"authors":["ZheYin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e0a0672a3f3e6186b23b8b860befea88","permalink":"https://pris-nlp.github.io/en/author/zhe-yin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/zhe-yin/","section":"authors","summary":"","tags":null,"title":"Zhe Yin","type":"authors"},{"authors":["HuaXu"],"categories":[],"content":"","date":1409557409,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1409557409,"objectID":"aa585aa49362c42e9f2830d01bcf8f37","permalink":"https://pris-nlp.github.io/en/textbook/datamining-methodandapplication/","publishdate":"2014-09-01T15:43:29+08:00","relpermalink":"/en/textbook/datamining-methodandapplication/","section":"textbook","summary":"Mainly based on the teaching practice and accumulation of the Data Mining Methods and Applications course set up by Tsinghua University, referring to the teaching system of relevant courses of famous foreign universities in recent years, systematically introducing the basic concepts and basic principles of data mining; combining some typical applications Examples show general patterns and ideas for solving problems with data mining thinking methods.","tags":["Data Mining"],"title":"Data Mining: Methodology and Applications","type":"textbook"},{"authors":["HuaXu"],"categories":[],"content":"","date":1201851809,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1201851809,"objectID":"2bfeedb392926ea9bc043feb5ad0e8ed","permalink":"https://pris-nlp.github.io/en/monograph/petrinettheoryandapplications/","publishdate":"2008-02-01T15:43:29+08:00","relpermalink":"/en/monograph/petrinettheoryandapplications/","section":"monograph","summary":"Chapter 12: Timed Hierarchical Object-oriented Petri Net, I-Tech Education and Publishing, Vienna, Austria, 2008, pp.253-280, ISNN:978-3-902613-12-7 (Hua Hu participated in the writing, published in February 2008)","tags":[],"title":"Petri Net: Theory and Applications","type":"monograph"},{"authors":["HuaXu"],"categories":[],"content":"","date":1504251809,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504251809,"objectID":"cb3a9f5f79958edae4960ecccacd7726","permalink":"https://pris-nlp.github.io/en/textbook/datamining-methodandapplication-case/","publishdate":"2017-09-01T15:43:29+08:00","relpermalink":"/en/textbook/datamining-methodandapplication-case/","section":"textbook","summary":"Mainly based on the teaching practice and accumulation of the Data Mining Methods and Applications course set up by Tsinghua University, referring to the teaching system of relevant courses of famous foreign universities in recent years, systematically introducing the basic concepts and basic principles of data mining; combining some typical applications Examples show general patterns and ideas for solving problems with data mining thinking methods.","tags":["Data Mining"],"title":"Data Mining: Methods and Applications - Application Cases","type":"textbook"},{"authors":["HuaXu"],"categories":[],"content":"","date":1470037409,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470037409,"objectID":"f3c91f32b83aaa1a5de73ea397ed32b4","permalink":"https://pris-nlp.github.io/en/textbook/bigdatatechnologyandindustryapplications/","publishdate":"2016-08-01T15:43:29+08:00","relpermalink":"/en/textbook/bigdatatechnologyandindustryapplications/","section":"textbook","summary":"How to define big data? How to apply big data? What is big data thinking? How to learn big data? How to build a big data platform? How to apply big data in the industry? This series of problems are very confusing problems in the current era of big data boom. Big Data Technology and Industry Application faces these questions directly, answers the above questions from the perspective of practitioners, and hopes to provide some help to beginners in the big data industry.","tags":["Big Data"],"title":"Big Data Technology and Industry Applications","type":"textbook"},{"authors":["HuaXu"],"categories":[],"content":"","date":1454312609,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454312609,"objectID":"4949ddee0a08346cb8809beaaa742b23","permalink":"https://pris-nlp.github.io/en/monograph/sentimentanalysisandontologyengineering/","publishdate":"2016-02-01T15:43:29+08:00","relpermalink":"/en/monograph/sentimentanalysisandontologyengineering/","section":"monograph","summary":"Chapter 10: Chinese Micro-Blog Emotion Classification by Exploiting Linguistic Features and SVMperf ), Springer International Publishing, 2016, pp. 221-236, ISNN:978-3-319-30317-8 (Hua Hu participated in the writing, published in February 2016)","tags":[],"title":"Sentiment Analysis and Ontology Engineering","type":"monograph"},{"authors":["HuaXu"],"categories":[],"content":"","date":1201851809,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1201851809,"objectID":"2c5c9a0907335dbc37fce7e6ceebe45b","permalink":"https://pris-nlp.github.io/en/monograph/recentadvancesinmulti-robotsystems/","publishdate":"2008-02-01T15:43:29+08:00","relpermalink":"/en/monograph/recentadvancesinmulti-robotsystems/","section":"monograph","summary":"Chapter 13: A Novel Modeling Method for Cooperative Multi-robot Systems Using Fuzzy Timed Agent Based Petri Nets ), I-Tech Education and Publishing, Vienna, Austria, 2008, pp.249-262, ISNN:978-3-902613-24-0 (Hua Hu participated in the writing, published in February 2008)","tags":[],"title":"Recent Advances in Multi-robot Systems","type":"monograph"},{"authors":["JunhuiDeng"],"categories":[],"content":"","date":1107243809,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1107243809,"objectID":"30837e0151d041aff588987056d3a925","permalink":"https://pris-nlp.github.io/en/textbook/%E8%AE%A1%E7%AE%97%E5%87%A0%E4%BD%95%E7%AE%97%E6%B3%95%E4%B8%8E%E5%BA%94%E7%94%A8/","publishdate":"2005-02-01T15:43:29+08:00","relpermalink":"/en/textbook/%E8%AE%A1%E7%AE%97%E5%87%A0%E4%BD%95%E7%AE%97%E6%B3%95%E4%B8%8E%E5%BA%94%E7%94%A8/","section":"textbook","summary":"The first four chapters of \"Computational Geometry: Algorithms and Applications (3rd Edition)\" discuss geometric algorithms, including geometric intersection, triangulation, linear programming, etc. The random algorithm involved is also \"Computational Geometry: Algorithms and Applications (Third Edition)\" a distinctive feature. Chapters 5 to 10 introduce a variety of geometric structures. Chapters 11 to 16 continue to discuss several geometric algorithms and their data structures based on practical problems, they are also further deepening of the content of the first 10 chapters. \"Computational Geometry: Algorithms and Applications (3rd Edition)\" is not only comprehensive in content, but also closely related to practical applications, with prominent points. It has in-depth explanations, and each chapter has \"notes and comments\" and \"exercises\" for the convenience of readers A deeper understanding has been used as teaching materials by many universities around the world.","tags":[],"title":"Computational Geometry: Algorithms and Applications","type":"textbook"},{"authors":["JunhuiDeng"],"categories":[],"content":"","date":1138779809,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1138779809,"objectID":"c70ac8a450da7476c774fb168d54bc54","permalink":"https://pris-nlp.github.io/en/textbook/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E8%AF%AD%E8%A8%80%E6%8F%8F%E8%BF%B0/","publishdate":"2006-02-01T15:43:29+08:00","relpermalink":"/en/textbook/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E8%AF%AD%E8%A8%80%E6%8F%8F%E8%BF%B0/","section":"textbook","summary":"This book fully demonstrates the application of object-oriented technology in modern data structure theory, generally using abstraction, encapsulation, and inheritance technologies. This book not only introduces the basic data structure, but also introduces the application, implementation and analysis methods of the algorithm in combination with specific problems. The book also unifies various graph algorithms through the traversal algorithm framework and implements it based on the traversal algorithm template. Unique among similar textbooks.","tags":[],"title":"Data Structures and Algorithms (Java Description)","type":"textbook"},{"authors":["JunhuiDeng"],"categories":[],"content":"","date":1075621409,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1075621409,"objectID":"7dedbd79296664918c876247a7356cba","permalink":"https://pris-nlp.github.io/en/monograph/%E8%B4%A8%E9%87%8F%E8%BD%AF%E4%BB%B6%E7%AE%A1%E7%90%86/","publishdate":"2004-02-01T15:43:29+08:00","relpermalink":"/en/monograph/%E8%B4%A8%E9%87%8F%E8%BD%AF%E4%BB%B6%E7%AE%A1%E7%90%86/","section":"monograph","summary":"Tsinghua University Press (Jun. 2004) ISBN: 7-302-08298-7 （Original Work：Gerald M. Weinberg, Quality Software Management:Systems Thinking ,Dorset House (Sep. 1991), ISBN: 0-932-63322-6.）","tags":[],"title":"Quality Software Management (Volume 1)-System Thinking","type":"monograph"},{"authors":["JunhuiDeng"],"categories":[],"content":"","date":1044085409,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1044085409,"objectID":"edca37e22f48ff86ab5a6e634b5b3a8a","permalink":"https://pris-nlp.github.io/en/monograph/%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91%E5%BF%83%E7%90%86%E5%AD%A6%E9%93%B6%E5%B9%B4%E7%BA%AA%E5%BF%B5%E7%89%88/","publishdate":"2003-02-01T15:43:29+08:00","relpermalink":"/en/monograph/%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91%E5%BF%83%E7%90%86%E5%AD%A6%E9%93%B6%E5%B9%B4%E7%BA%AA%E5%BF%B5%E7%89%88/","section":"monograph","summary":"Tsinghua University Press (Sep. 2003) ISBN: 7-302-07026-1 （Original Work：Gerald M. Weinberg, The Psychology of Computer Programming: Silver Anniversary Edition, Dorset House (Sep. 1998), ISBN: 0-932-63342-0.）","tags":[],"title":"The Psychology of Computer Programming","type":"monograph"},{"authors":["HuaXu"],"categories":[],"content":"","date":1602301409,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602301409,"objectID":"d9809ef0bb72c0fdeb8f093635124ba3","permalink":"https://pris-nlp.github.io/en/textbook/%E6%96%87%E6%9C%AC%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/","publishdate":"2020-10-10T11:43:29+08:00","relpermalink":"/en/textbook/%E6%96%87%E6%9C%AC%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/","section":"textbook","summary":"This book explains the text big data sentiment analysis technology from multiple dimensions. The content covers natural language processing and text emotion and sentiment methods, as well as sentiment analysis of Weibo texts and their incentives, position analysis of topic-oriented comments, texts Representation method and its application in emotion classification. The book is organized in a modular manner, with strong theoretical and clear organization. The author team implemented the main methods in the book with a serious and rigorous scientific attitude, and described the effects of various methods. This book can provide help for the study and scientific research work of college students in related majors (such as computer science and technology, software engineering, etc.). It also has a higher reference value for engineering and technical personnel engaged in text mining and natural language processing.","tags":[],"title":"Big Data Sentiment Analysis in Text","type":"textbook"},{"authors":["Yutao Mu","Guanting Dong","Ruotong Geng","Qixiang Gao","Daichi Guo","Chen Zeng","Hao Lei"],"categories":[],"content":"   Yutao Mu \nPostgraduate Student \nPh.D., Peking University       Guanting Dong \nPostgraduate Student \nPh.D., Renmin University of China       Ruotong Geng \nPostgraduate Student \nBeijing Bytedance Technology Co.,Ltd.       Qixiang Gao \nPostgraduate Student \nAlibaba (Beijing) Software Services Co.,Ltd.       Daichi Guo \nPostgraduate Student \nCITIC Securities Co.,Ltd.       Chen Zeng \nPostgraduate Student \nChinalco Capital Holdings Co.,Ltd.       Hao Lei \nPostgraduate Student \nBYD Auto Industry Co.,Ltd.   ","date":1719187200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719187200,"objectID":"6c3c5ac41912531c1e4615e055523635","permalink":"https://pris-nlp.github.io/en/career/2024%E5%B1%8A%E6%AF%95%E4%B8%9A%E7%94%9F%E5%8E%BB%E5%90%91/","publishdate":"2024-06-24T00:00:00Z","relpermalink":"/en/career/2024%E5%B1%8A%E6%AF%95%E4%B8%9A%E7%94%9F%E5%8E%BB%E5%90%91/","section":"career","summary":" ","tags":[],"title":"2024 Graduate Career","type":"career"},{"authors":["Dayuan Fu","Keqing He","Yejie Wang","Wentao Hong","Zhuoma GongQue","Weihao Zeng","WeiWang","JingangWang","XunliangCai","Weiran Xu"],"categories":[],"content":"","date":1738454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"445aa42974243986cbcaf5c865d6c197","permalink":"https://pris-nlp.github.io/en/publication/agentrefine-enhancing-agent-generalization-through-refinement-tuning/large-scale-relation-learning-for-question-answering-over-knowledge-bases-with-pre-trained-language-models/","publishdate":"2025-01-03T08:55:19.366191Z","relpermalink":"/en/publication/agentrefine-enhancing-agent-generalization-through-refinement-tuning/large-scale-relation-learning-for-question-answering-over-knowledge-bases-with-pre-trained-language-models/","section":"publication","summary":"Large Language Model (LLM) based agents have proved their ability to perform complex tasks like humans. However, there is still a large gap between open-sourced LLMs and commercial models like the GPT series. In this paper, we focus on improving the agent generalization capabilities of LLMs via instruction tuning. We first observe that the existing agent training corpus exhibits satisfactory results on held-in evaluation sets but fails to generalize to held-out sets. These agent-tuning works face severe formatting errors and are frequently stuck in the same mistake for a long while. We analyze that the poor generalization ability comes from overfitting to several manual agent environments and a lack of adaptation to new situations. They struggle with the wrong action steps and can not learn from the experience but just memorize existing observation-action relations. Inspired by the insight, we propose a novel AgentRefine framework for agent-tuning. The core idea is to enable the model to learn to correct its mistakes via observation in the trajectory. Specifically, we propose an agent synthesis framework to encompass a diverse array of environments and tasks and prompt a strong LLM to refine its error action according to the environment feedback. AgentRefine significantly outperforms state-of-the-art agent-tuning work in terms of generalization ability on diverse agent tasks. It also has better robustness facing perturbation and can generate diversified thought in inference. Our findings establish the correlation between agent generalization and self-refinement and provide a new paradigm for future research.","tags":["\"Agent\""],"title":"AgentRefine:Enhancing Agent Generalization through Refinement Tuning","type":"publication"},{"authors":["Weihao Zeng","YuzhenHuang","Lulu Zhao","YijunWang","ZifeiShan","JunxianHe"],"categories":[],"content":"","date":1738454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"150d40057459b7a71a77adff31c2a5d4","permalink":"https://pris-nlp.github.io/en/publication/b-star-monitoring-and-balancing-exploration-and-exploitation-in-self-taught-reasoners/","publishdate":"2024-12-23T03:58:34.366191Z","relpermalink":"/en/publication/b-star-monitoring-and-balancing-exploration-and-exploitation-in-self-taught-reasoners/","section":"publication","summary":"In the absence of extensive human-annotated data for complex reasoning tasks, self-improvement -- where models are trained on their own outputs -- has emerged as a primary method for enhancing performance. However, the critical factors underlying the mechanism of these iterative self-improving methods remain poorly understood, such as under what conditions self-improvement is effective, and what are the bottlenecks in the current iterations. In this work, we identify and propose methods to monitor two pivotal factors in this iterative process:(1) the model's ability to generate sufficiently diverse responses (exploration); and (2) the effectiveness of external rewards in distinguishing high-quality candidates from lower-quality ones (exploitation). Using mathematical reasoning as a case study, we begin with a quantitative analysis to track the dynamics of exploration and exploitation, discovering that a model's exploratory capabilities rapidly deteriorate over iterations, and the effectiveness of exploiting external rewards diminishes as well. Motivated by these findings, we introduce B-STaR, a Self-Taught Reasoning framework that autonomously adjusts configurations across iterations to Balance exploration and exploitation, thereby optimizing the self-improving effectiveness based on the current policy model and available rewards. Our experiments on mathematical reasoning, coding, and commonsense reasoning demonstrate that B-STaR not only enhances the model's exploratory capabilities throughout training but also achieves a more effective balance between exploration and exploitation, leading to superior performance.","tags":["\"LLMs\"","\"Reasoning\""],"title":"B-STaR:Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners","type":"publication"},{"authors":["Pei Wang","Yanan Wu","ZekunWang","JiahengLiu","Xiaoshuai Song","ZhongyuanPeng","KenDeng","ChenchenZhang","JiakaiWang","JunranPeng","GeZhang","HangyuGuo","ZhaoxiangZhang","WenboSu","BoZheng"],"categories":[],"content":"","date":1738454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"cdc2a5d4706fe5665bc9fed0384440eb","permalink":"https://pris-nlp.github.io/en/publication/mtu-bench-a-multi-granularity-tool-use-benchmark-for-large-language-models/","publishdate":"2024-10-15T15:46:17.366191Z","relpermalink":"/en/publication/mtu-bench-a-multi-granularity-tool-use-benchmark-for-large-language-models/","section":"publication","summary":"Large Language Models (LLMs) have displayed massive improvements in reasoning and decision-making skills and can hold natural conversations with users. Recently, many tool-use benchmark datasets have been proposed. However, existing datasets have the following limitations:(1). Insufficient evaluation scenarios (e.g., only cover limited tool-use scenes). (2). Extensive evaluation costs (e.g., GPT API costs). To address these limitations, in this work, we propose a multi-granularity tool-use benchmark for large language models called MTU-Bench. For the \"multi-granularity\" property, our MTU-Bench covers five tool usage scenes (i.e., single-turn and single-tool, single-turn and multiple-tool, multiple-turn and single-tool, multiple-turn and multiple-tool, and out-of-distribution tasks). Besides, all evaluation metrics of our MTU-Bench are based on the prediction results and the ground truth without using any GPT or human evaluation metrics. Moreover, our MTU-Bench is collected by transforming existing high-quality datasets to simulate real-world tool usage scenarios, and we also propose an instruction dataset called MTU-Instruct data to enhance the tool-use abilities of existing LLMs. Comprehensive experimental results demonstrate the effectiveness of our MTU-Bench. Code and data will be released at https://github.com/MTU-Bench-Team/MTU-Bench.","tags":["\"LLMs\"","\"Benchmark\""],"title":"MTU-Bench:A Multi-granularity Tool-Use Benchmark for Large Language Models","type":"publication"},{"authors":["Dayuan Fu","JianzhaoHuang","SiyuanLu","Guanting Dong","Yejie Wang","Keqing He","Weiran Xu"],"categories":[],"content":"","date":1738454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"abda1067193a582a7bc62bdf1fd307d8","permalink":"https://pris-nlp.github.io/en/publication/preact-prediction-enhances-agents-planning-ability/","publishdate":"2024-02-18T18:47:28.366191Z","relpermalink":"/en/publication/preact-prediction-enhances-agents-planning-ability/","section":"publication","summary":"Addressing the disparity between forecasts and actual results can enable individuals to expand their thought processes and stimulate self-reflection, thus promoting accurate planning. In this research, we present PreAct, an agent framework that integrates prediction, reasoning, and action. By utilizing the information derived from predictions, the large language model (LLM) agent can provide a wider range and more strategically focused reasoning. This leads to more efficient actions that aid the agent in accomplishing intricate tasks. Our experimental results show that PreAct surpasses the ReAct method in completing complex tasks and that PreAct’s performance can be further improved when paired with other memory or selection strategy techniques. We presented the model with varying quantities of historical predictions and discovered that these predictions consistently enhance LLM planning. The variances in single-step reasoning between PreAct and ReAct indicate that PreAct indeed has benefits in terms of diversity and strategic orientation over ReAct.","tags":["\"LLMs\"","\"agent\""],"title":"PreAct:Prediction Enhances Agent's Planning Ability","type":"publication"},{"authors":["Xiaoshuai Song","Muxi Diao","Guanting Dong","Zhengyang Wang","YujiaFu","RunqiQiao","Zhexu Wang","Dayuan Fu","HuangxuanWu","BinLiang","Weihao Zeng","Yejie Wang","Zhuoma GongQue","Jianing Yu","QiunaTan","Weiran Xu"],"categories":[],"content":"","date":1718150400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"9cafe241d827aa126f33e584c26e09d4","permalink":"https://pris-nlp.github.io/en/publication/cs-bench-a-comprehensive-benchmark-for-large-language-models-towards-computer-science-mastery/","publishdate":"2024-06-12T18:47:28.366191Z","relpermalink":"/en/publication/cs-bench-a-comprehensive-benchmark-for-large-language-models-towards-computer-science-mastery/","section":"publication","summary":"Computer Science (CS) stands as a testament to the intricacies of human intelligence, profoundly advancing the development of artificial intelligence and modern society. However, the current community of large language models (LLMs) overly focuses on benchmarks for analyzing specific foundational skills (e.g. mathematics and code generation), neglecting an all-round evaluation of the computer science field. To bridge this gap, we introduce CS-Bench, the first bilingual (Chinese-English) benchmark dedicated to evaluating the performance of LLMs in computer science. CS-Bench comprises approximately 5K meticulously curated test samples, covering 26 subfields across 4 key areas of computer science, encompassing various task forms and divisions of knowledge and reasoning. Utilizing CS-Bench, we conduct a comprehensive evaluation of over 30 mainstream LLMs, revealing the relationship between CS performance and model scales. We also quantitatively analyze the reasons for failures in existing LLMs and highlight directions for improvements, including knowledge supplementation and CS-specific reasoning. Further cross-capability experiments show a high correlation between LLMs' capabilities in computer science and their abilities in mathematics and coding. Moreover, expert LLMs specialized in mathematics and coding also demonstrate strong performances in several CS subfields. Looking ahead, we envision CS-Bench serving as a cornerstone for LLM applications in the CS field and paving new avenues in assessing LLMs' diverse reasoning capabilities. The CS-Bench data and evaluation code are available at at https://github.com/csbench/csbench.","tags":["\"LLMs\"","\"Benchmark\""],"title":"CS-Bench:A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery","type":"publication"},{"authors":["Weihao Zeng","Keqing He","Yejie Wang","Dayuan Fu","Weiran Xu"],"categories":[],"content":"","date":1714608000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"d08fcedcfd0a0afdf448adb0c49a3585","permalink":"https://pris-nlp.github.io/en/publication/boottod-bootstrap-task-oriented-dialogue-representations-by-aligning-diverse-responses/","publishdate":"2024-05-02T18:47:28.366191Z","relpermalink":"/en/publication/boottod-bootstrap-task-oriented-dialogue-representations-by-aligning-diverse-responses/","section":"publication","summary":"Pre-trained language models have been successful in many scenarios. However, their usefulness in task-oriented dialogues is limited due to the intrinsic linguistic differences between general text and task-oriented dialogues. Current task-oriented dialogue pre-training methods rely on a contrastive framework, which faces challenges such as selecting true positives and hard negatives, as well as lacking diversity. In this paper, we propose a novel dialogue pre-training model called BootTOD. It learns task-oriented dialogue representations via a self-bootstrapping framework. Unlike contrastive counterparts, BootTOD aligns context and context+response representations and dismisses the requirements of contrastive pairs. BootTOD also uses multiple appropriate response targets to model the intrinsic one-to-many diversity of human conversations. Experimental results show that BootTOD outperforms strong TOD baselines on diverse downstream dialogue tasks.","tags":["\"LLMs\"","\"task-oriented dialogue\""],"title":"BootTOD:Bootstrap Task-oriented Dialogue Representations by Aligning Diverse Responses","type":"publication"},{"authors":["Weihao Zeng","Dayuan Fu","Keqing He","Yejie Wang","Yukai Xu","Weiran Xu"],"categories":[],"content":"","date":1711843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"bb27868db1aadcfc9042f479392dfa9a","permalink":"https://pris-nlp.github.io/en/publication/divtod-unleashing-the-power-of-llms-for-diversifying-task-oriented-dialogue-representations/","publishdate":"2024-03-31T18:47:28.366191Z","relpermalink":"/en/publication/divtod-unleashing-the-power-of-llms-for-diversifying-task-oriented-dialogue-representations/","section":"publication","summary":"Language models pre-trained on general text have achieved impressive results in diverse fields. Yet, the distinct linguistic characteristics of task-oriented dialogues (TOD) compared to general text limit the practical utility of existing language models. Current task-oriented dialogue pre-training methods overlook the one-to-many property of conversations, where multiple responses can be appropriate given the same conversation context. In this paper, we propose a novel dialogue pre-training model called DivTOD, which collaborates with LLMs to learn diverse task-oriented dialogue representations. DivTOD guides LLMs in transferring diverse knowledge to smaller models while removing domain knowledge that contradicts task-oriented dialogues. Experiments show that our model outperforms strong TOD baselines on various downstream dialogue tasks and learns the intrinsic diversity of task-oriented dialogues.","tags":["\"LLMs\"","\"task-oriented dialogue\""],"title":"DivTOD:Unleashing the Power of LLMs for Diversifying Task-Oriented Dialogue Representations","type":"publication"},{"authors":["Lixiong Qin","MeiWang","XuannanLiu","YuhangZhang","WeiDeng","Xiaoshuai Song","Weiran Xu","WeihongDeng"],"categories":[],"content":"","date":1710374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"4b3069495c27e0775e7a46d8db7d98dc","permalink":"https://pris-nlp.github.io/en/publication/faceptor-a-generalist-model-for-face-perception/","publishdate":"2024-03-14T18:47:28.366191Z","relpermalink":"/en/publication/faceptor-a-generalist-model-for-face-perception/","section":"publication","summary":"With the comprehensive research conducted on various face analysis tasks, there is a growing interest among researchers to develop a unified approach to face perception. Existing methods mainly discuss unified representation and training, which lack task extensibility and application efficiency. To tackle this issue, we focus on the unified model structure, exploring a face generalist model. As an intuitive design, Naive Faceptor enables tasks with the same output shape and granularity to share the structural design of the standardized output head, achieving improved task extensibility. Furthermore, Faceptor is proposed to adopt a well-designed single-encoder dual-decoder architecture, allowing task-specific queries to represent new-coming semantics. This design enhances the unification of model structure while improving application efficiency in terms of storage overhead. Additionally, we introduce Layer-Attention into Faceptor, enabling the model to adaptively select features from optimal layers to perform the desired tasks. Through joint training on 13 face perception datasets, Faceptor achieves exceptional performance in facial landmark localization, face parsing, age estimation, expression recognition, binary attribute classification, and face recognition, achieving or surpassing specialized methods in most tasks. Our training framework can also be applied to auxiliary supervised learning, significantly improving performance in data-sparse tasks such as age estimation and expression recognition. The code and models will be made publicly available at https://github.com/lxq1000/Faceptor.","tags":["\"computer vision\"","\"foundation model\""],"title":"Faceptor:A Generalist Model for Face Perception","type":"publication"},{"authors":["Pei Wang","Yejie Wang","Muxi Diao","Keqing He","Guanting Dong","Weiran Xu"],"categories":[],"content":"","date":1708128000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"24f48dfcdff3b7eded215d403c9b8b19","permalink":"https://pris-nlp.github.io/en/publication/multi-perspective-consistency-enhances-confidence-estimation-in-large-language-models/","publishdate":"2024-02-17T18:47:28.366191Z","relpermalink":"/en/publication/multi-perspective-consistency-enhances-confidence-estimation-in-large-language-models/","section":"publication","summary":"In the deployment of large language models (LLMs), accurate confidence estimation is critical for assessing the credibility of model predictions. However, existing methods often fail to overcome the issue of overconfidence on incorrect answers. In this work, we focus on improving the confidence estimation of large language models. Considering the fragility of self-awareness in language models, we introduce a Multi-Perspective Consistency (MPC) method. We leverage complementary insights from different perspectives within models (MPC-Internal) and across different models (MPC-Across) to mitigate the issue of overconfidence arising from a singular viewpoint. The experimental results on eight publicly available datasets show that our MPC achieves state-of-the-art performance. Further analyses indicate that MPC can mitigate the problem of overconfidence and is effectively scalable to other models.","tags":["\"LLMs\"","\"confidence\""],"title":"Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models","type":"publication"},{"authors":["Yejie Wang","Keqing He","Guanting Dong","Pei Wang","Weihao Zeng","Muxi Diao","Yutao Mu","MengdiZhang","JingangWang","XunliangCai","Weiran Xu"],"categories":[],"content":"","date":1707868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"3450b531a602b90d4f25b45d1d86969b","permalink":"https://pris-nlp.github.io/en/publication/dolphcoder-echo-locating-code-large-language-models-with-diverse-and-multi-objective-instruction-tuning/","publishdate":"2024-02-14T18:47:28.366191Z","relpermalink":"/en/publication/dolphcoder-echo-locating-code-large-language-models-with-diverse-and-multi-objective-instruction-tuning/","section":"publication","summary":"Code Large Language Models (Code LLMs) have demonstrated outstanding performance in code-related tasks. Several instruction tuning approaches have been proposed to boost the code generation performance of pre-trained Code LLMs. In this paper, we introduce a diverse instruction model (DolphCoder) with self-evaluating for code generation. It learns diverse instruction targets and combines a code evaluation objective to enhance its code generation ability. Our model achieves superior performance on the HumanEval and MBPP benchmarks, demonstrating new insights for future code instruction tuning work. Our key findings are:(1) Augmenting more diverse responses with distinct reasoning paths increases the code capability of LLMs. (2) Improving one's ability to evaluate the correctness of code solutions also enhances their ability to create it.","tags":["\"LLMs\"","\"code generation\""],"title":"DolphCoder:Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning","type":"publication"},{"authors":["Xiaoshuai Song","Zhengyang Wang","Keqing He","Guanting Dong","Yutao Mu","Jinxu Zhao","Weiran Xu"],"categories":[],"content":"","date":1707782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"9c54a5ad6a9d64e21da6693874a74837","permalink":"https://pris-nlp.github.io/en/publication/knowledge-editing-on-black-box-large-language-models/","publishdate":"2024-02-13T18:47:28.366191Z","relpermalink":"/en/publication/knowledge-editing-on-black-box-large-language-models/","section":"publication","summary":"Knowledge editing (KE) aims to efficiently and precisely modify the behavior of large language models (LLMs) to update specific knowledge without negatively influencing other knowledge. Current research primarily focuses on white-box LLMs editing, overlooking an important scenario:black-box LLMs editing, where LLMs are accessed through interfaces and only textual output is available. In this paper, we first officially introduce KE on black-box LLMs and then propose a comprehensive evaluation framework to overcome the limitations of existing evaluations that are not applicable to black-box LLMs editing and lack comprehensiveness. To tackle privacy leaks of editing data and style over-editing in current methods, we introduce a novel postEdit framework, resolving privacy concerns through downstream post-processing and maintaining textual style consistency via fine-grained editing to original responses. Experiments and analysis on two benchmarks demonstrate that postEdit outperforms all baselines and achieves strong generalization, especially with huge improvements on style retention (average +20.82%↑).","tags":["\"LLMs\"","\"knowledge editing \""],"title":"Knowledge Editing on Black-box Large Language Models","type":"publication"},{"authors":["Guanting Dong","Zechen Wang","Jinxu Zhao","GangZhao","Daichi Guo","Dayuan Fu","Tingfeng Hui","Chen Zeng","Keqing He","Xuefeng Li","Liwen Wang","XinyueCui","Weiran Xu"],"categories":[],"content":"","date":1697760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"7376e6c1bd045e2f7b4abce9f079af9f","permalink":"https://pris-nlp.github.io/en/publication/a-multi-task-semantic-decomposition-framework-with-task-specific-pre-training-for-few-shot-ner/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/a-multi-task-semantic-decomposition-framework-with-task-specific-pre-training-for-few-shot-ner/","section":"publication","summary":"The objective of few-shot named entity recognition is to identify named entities with limited labeled instances. Previous works have primarily focused on optimizing the traditional token-wise classification framework, while neglecting the exploration of information based on NER data characteristics. To address this issue, we propose a Multi-Task Semantic Decomposition Framework via Joint Task-specific Pre-training (MSDP) for few-shot NER. Drawing inspiration from demonstration-based and contrastive learning, we introduce two novel pre-training tasks: Demonstration-based Masked Language Modeling (MLM) and Class Contrastive Discrimination. These tasks effectively incorporate entity boundary information and enhance entity representation in Pre-trained Language Models (PLMs). In the downstream main task, we introduce a multi-task joint optimization framework with the semantic decomposing method, which facilitates the model to integrate two different semantic information for entity classification. Experimental results of two few-shot NER benchmarks demonstrate that MSDP consistently outperforms strong baselines by a large margin. Extensive analyses validate the effectiveness and generalization of MSDP.","tags":["\"NER\""],"title":"A multi-task semantic decomposition framework with task-specific pre-training for few-shot ner","type":"publication"},{"authors":["Pei Wang","Keqing He","Yutao Mu","Xiaoshuai Song","Yanan Wu","JingangWang","YunsenXian","XunliangCai","Weiran Xu"],"categories":[],"content":"","date":1697760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"9cb70088f52c5c923c220200ed741e7a","permalink":"https://pris-nlp.github.io/en/publication/app-adaptive-prototypical-pseudo-labeling-for-few-shot-ood-detection/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/app-adaptive-prototypical-pseudo-labeling-for-few-shot-ood-detection/","section":"publication","summary":"Detecting out-of-domain (OOD) intents from user queries is essential for a task-oriented dialogue system. Previous OOD detection studies generally work on the assumption that plenty of labeled IND intents exist. In this paper, we focus on a more practical few-shot OOD setting where there are only a few labeled IND data and massive unlabeled mixed data that may belong to IND or OOD. The new scenario carries two key challenges: learning discriminative representations using limited IND data and leveraging unlabeled mixed data. Therefore, we propose an adaptive prototypical pseudo-labeling (APP) method for few-shot OOD detection, including a prototypical OOD detection framework (ProtoOOD) to facilitate low-resource OOD detection using limited IND data, and an adaptive pseudo-labeling method to produce high-quality pseudo OOD\\\u0026IND labels. Extensive experiments and analysis demonstrate the effectiveness of our method for few-shot OOD detection.","tags":["\"OOD\"","\"intent detection\""],"title":"APP：Adaptive Prototypical Pseudo-Labeling for Few-shot OOD Detection","type":"publication"},{"authors":["Guanting Dong","Rumei Li","SiruiWang","YupengZhang","YunsenXian","Weiran Xu"],"categories":[],"content":"","date":1697760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"479555fd3497bbf16871edfc438beed0","permalink":"https://pris-nlp.github.io/en/publication/bridging-the-kb-text-gap-leveraging-structured-knowledge-aware-pre-training-for-kbqa/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/bridging-the-kb-text-gap-leveraging-structured-knowledge-aware-pre-training-for-kbqa/","section":"publication","summary":"Knowledge Base Question Answering (KBQA) aims to answer natural language questions with factual information such as entities and relations in KBs. However, traditional Pre-trained Language Models (PLMs) are directly pre-trained on large-scale natural language corpus, which poses challenges for them in understanding and representing complex subgraphs in structured KBs. To bridge the gap between texts and structured KBs, we propose a Structured Knowledge-aware Pre-training method (SKP). In the pre-training stage, we introduce two novel structured knowledge-aware tasks, guiding the model to effectively learn the implicit relationship and better representations of complex subgraphs. In downstream KBQA task, we further design an efficient linearization strategy and an interval attention mechanism, which assist the model to better encode complex subgraphs and shield the interference of irrelevant subgraphs during reasoning respectively. Detailed experiments and analyses on WebQSP verify the effectiveness of SKP, especially the significant improvement in subgraph retrieval (+4.08% H@10).","tags":["\"KQBA\""],"title":"Bridging the KB-Text Gap: Leveraging Structured Knowledge-aware Pre-training for KBQA","type":"publication"},{"authors":["Xiaoshuai Song","Yutao Mu","Keqing He","YueyanQiu","Pei Wang","Weiran Xu"],"categories":[],"content":"","date":1697760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"5340f58ea8dce43f329fb63a77848679","permalink":"https://pris-nlp.github.io/en/publication/continual-generalized-intent-discovery-marching-towards-dynamic-and-open-world-intent-recognition/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/continual-generalized-intent-discovery-marching-towards-dynamic-and-open-world-intent-recognition/","section":"publication","summary":"In a practical dialogue system, users may input out-of-domain (OOD) queries. The Generalized Intent Discovery (GID) task aims to discover OOD intents from OOD queries and extend them to the in-domain (IND) classifier. However, GID only considers one stage of OOD learning, and needs to utilize the data in all previous stages for joint training, which limits its wide application in reality. In this paper, we introduce a new task, Continual Generalized Intent Discovery (CGID), which aims to continuously and automatically discover OOD intents from dynamic OOD data streams and then incrementally add them to the classifier with almost no previous data, thus moving towards dynamic intent recognition in an open world. Next, we propose a method called Prototype-guided Learning with Replay and Distillation (PLRD) for CGID, which bootstraps new intent discovery through class prototypes and balances new and old intents through data replay and feature distillation. Finally, we conduct detailed experiments and analysis to verify the effectiveness of PLRD and understand the key challenges of CGID for future research.","tags":["\"OOD\"","\"Dialogue System\""],"title":"Continual Generalized Intent Discovery: Marching Towards Dynamic and Open-world Intent Recognition","type":"publication"},{"authors":["Guanting Dong","Tingfeng Hui","Zhuoma GongQue","Jinxu Zhao","Daichi Guo","GangZhao","Keqing He","Weiran Xu"],"categories":[],"content":"","date":1697760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"9d56725f61b62bafaf7a8b8003972adb","permalink":"https://pris-nlp.github.io/en/publication/demonsf-a-multi-task-demonstration-based-generative-framework-for-noisy-slot-filling-task/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/demonsf-a-multi-task-demonstration-based-generative-framework-for-noisy-slot-filling-task/","section":"publication","summary":"Recently, prompt-based generative frameworks have shown impressive capabilities in sequence labeling tasks. However, in practical dialogue scenarios, relying solely on simplistic templates and traditional corpora presents a challenge for these methods in generalizing to unknown input perturbations. To address this gap, we propose a multi-task demonstration based generative framework for noisy slot filling, named DemoNSF. Specifically, we introduce three noisy auxiliary tasks, namely noisy recovery (NR), random mask (RM), and hybrid discrimination (HD), to implicitly capture semantic structural information of input perturbations at different granularities. In the downstream main task, we design a noisy demonstration construction strategy for the generative framework, which explicitly incorporates task-specific information and perturbed distribution during training and inference. Experiments on two benchmarks demonstrate that DemoNSF outperforms all baseline methods and achieves strong generalization. Further analysis provides empirical guidance for the practical application of generative frameworks.","tags":null,"title":"DemoNSF: A Multi-task Demonstration-based Generative Framework for Noisy Slot Filling Task","type":"publication"},{"authors":["Xiaoshuai Song","Keqing He","Pei Wang","Guanting Dong","Yutao Mu","JingangWang","YunsenXian","XunliangCai","Weiran Xu"],"categories":[],"content":"","date":1697760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"58fbd68b21d5214e3305e178dc9a4db6","permalink":"https://pris-nlp.github.io/en/publication/large-language-models-meet-open-world-intent-discovery-and-recognition-an-evaluation-of-chatgpt/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/large-language-models-meet-open-world-intent-discovery-and-recognition-an-evaluation-of-chatgpt/","section":"publication","summary":"The tasks of out-of-domain (OOD) intent discovery and generalized intent discovery (GID) aim to extend a closed intent classifier to open-world intent sets, which is crucial to task-oriented dialogue (TOD) systems. Previous methods address them by fine-tuning discriminative models. Recently, although some studies have been exploring the application of large language models (LLMs) represented by ChatGPT to various downstream tasks, it is still unclear for the ability of ChatGPT to discover and incrementally extent OOD intents. In this paper, we comprehensively evaluate ChatGPT on OOD intent discovery and GID, and then outline the strengths and weaknesses of ChatGPT. Overall, ChatGPT exhibits consistent advantages under zero-shot settings, but is still at a disadvantage compared to fine-tuned models. More deeply, through a series of analytical experiments, we summarize and discuss the challenges faced by LLMs including clustering, domain-specific understanding, and cross-domain in-context learning scenarios. Finally, we provide empirical guidance for future directions to address these challenges.","tags":["\"OOD\"","\"GID\""],"title":"Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT","type":"publication"},{"authors":["Guanting Dong","Jinxu Zhao","Tingfeng Hui","Daichi Guo","WenlongWan","BoqiFeng","YueyanQiu","Zhuoma GongQue","Keqing He","Zechen Wang","Weiran Xu"],"categories":[],"content":"","date":1697760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"5432e8b28b35114103c61ea8b3b220a5","permalink":"https://pris-nlp.github.io/en/publication/revisit-input-perturbation-problems-for-llms-a-unified-robustness-evaluation-framework-for-noisy-slot-filling-task/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/revisit-input-perturbation-problems-for-llms-a-unified-robustness-evaluation-framework-for-noisy-slot-filling-task/","section":"publication","summary":"We utilize a multi-level data augmentation method (character, word, and sentence levels) to construct a candidate data pool, and carefully design two ways of automatic task demonstration construction strategies (instance-level and entity-level) with various prompt templates. Our aim is to assess how well various robustness methods of LLMs perform in real-world noisy scenarios. The experiments have demonstrated that the current open-source LLMs generally achieve limited perturbation robustness performance. Based on these experimental observations, we make some forward-looking suggestions to fuel the research in this direction..","tags":["\"LLMs\""],"title":"Revisit input perturbation problems for llms: A unified robustness evaluation framework for noisy slot filling task","type":"publication"},{"authors":["YuxiangWu","Guanting Dong","Weiran Xu"],"categories":[],"content":"","date":1697760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"208e8b66941c5be92052ffd95bd507db","permalink":"https://pris-nlp.github.io/en/publication/semantic-parsing-by-large-language-models-for-intricate-updating-strategies-of-zero-shot-dialogue-state-tracking/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/semantic-parsing-by-large-language-models-for-intricate-updating-strategies-of-zero-shot-dialogue-state-tracking/","section":"publication","summary":"Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring and annotating task-oriented dialogues, which can be time consuming and costly. However, DST extends beyond simple slot-filling and requires effective updating strategies for tracking dialogue state as conversations progress. In this paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to introduce additional intricate updating strategies in zero-shot DST. Our approach reformulates the DST task by leveraging powerful Large Language Models (LLMs) and translating the original dialogue text to JSON through semantic parsing as an intermediate state. We also design a novel framework that includes more modules to ensure the effectiveness of updating strategies in the text-to-JSON process. Experimental results demonstrate that our approach outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to existing ICL methods.","tags":["\"DST\"","\"LLMs\""],"title":"Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking","type":"publication"},{"authors":["Guanting Dong","Zechen Wang","Liwen Wang","Daichi Guo","Dayuan Fu","YuxiangWu","Chen Zeng","Xuefeng Li","Tingfeng Hui","Keqing He","XinyueCui","Qixiang Gao","Weiran Xu"],"categories":[],"content":"","date":1683417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"0d6f66a55b785dc123df38b163009a5b","permalink":"https://pris-nlp.github.io/en/publication/a-prototypical-semantic-decoupling-method-via-joint-contrastive-learning-for-few-shot-named-entity-recognition/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/a-prototypical-semantic-decoupling-method-via-joint-contrastive-learning-for-few-shot-named-entity-recognition/","section":"publication","summary":"Few-shot named entity recognition (NER) aims at identifying named entities based on only few labeled instances. Most existing prototype-based sequence labeling models tend to memorize entity mentions which would be easily confused by close prototypes. In this paper, we proposed a Prototypical Semantic Decoupling method via joint Contrastive learning (PSDC) for few-shot NER. Specifically, we decouple class-specific prototypes and contextual semantic prototypes by two masking strategies to lead the model to focus on two different semantic information for inference. Besides, we further introduce joint contrastive learning objectives to better integrate two kinds of decoupling information and prevent semantic collapse. Experimental results on two few-shot NER benchmarks demonstrate that PSDC consistently outperforms the previous SOTA methods in terms of overall performance. Extensive analysis further validates the effectiveness and generalization of PSDC.","tags":["\" \""],"title":"A Prototypical Semantic Decoupling Method via Joint Contrastive Learning for Few-Shot Named Entity Recognition","type":"publication"},{"authors":["Yutao Mu","Xiaoshuai Song","Keqing He","Chen Zeng","Pei Wang","JingangWang","YunsenXian","Weiran Xu"],"categories":[],"content":"","date":1683417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"1076e815addcdf9480dd2179af38a39a","permalink":"https://pris-nlp.github.io/en/publication/decoupling-pseudo-label-disambiguation-and-representation-learning-for-generalized-intent-discovery/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/decoupling-pseudo-label-disambiguation-and-representation-learning-for-generalized-intent-discovery/","section":"publication","summary":"Generalized intent discovery aims to extend a closed-set in-domain intent classifier to an open-world intent set including in-domain and out-of-domain intents. The key challenges lie in pseudo label disambiguation and representation learning. Previous methods suffer from a coupling of pseudo label disambiguation and representation learning, that is, the reliability of pseudo labels relies on representation learning, and representation learning is restricted by pseudo labels in turn. In this paper, we propose a decoupled prototype learning framework (DPL) to decouple pseudo label disambiguation and representation learning. Specifically, we firstly introduce prototypical contrastive representation learning (PCL) to get discriminative representations. And then we adopt a prototype-based label disambiguation method (PLD) to obtain pseudo labels. We theoretically prove that PCL and PLD work in a collaborative fashion and facilitate pseudo label disambiguation. Experiments and analysis on three benchmark datasets show the effectiveness of our method.","tags":["\" \""],"title":"Decoupling Pseudo Label Disambiguation and Representation Learning for Generalized Intent Discovery","type":"publication"},{"authors":["Weihao Zeng","Keqing He","Yejie Wang","Chen Zeng","JingangWang","YunsenXian","Weiran Xu"],"categories":[],"content":"","date":1683417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"0d35525fa06f69bc093714f3d5def5b6","permalink":"https://pris-nlp.github.io/en/publication/futuretod-teaching-future-knowledge-to-pre-trained-language-model-for-task-oriented-dialogue/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/futuretod-teaching-future-knowledge-to-pre-trained-language-model-for-task-oriented-dialogue/","section":"publication","summary":"Pre-trained language models based on general text enable huge success in the NLP scenario. But the intrinsical difference of linguistic patterns between general text and task-oriented dialogues makes existing pre-trained language models less useful in practice. Current dialogue pre-training methods rely on a contrastive framework and face the challenges of both selecting true positives and hard negatives. In this paper, we propose a novel dialogue pre-training model, FutureTOD, which distills future knowledge to the representation of the previous dialogue context using a self-training framework. Our intuition is that a good dialogue representation both learns local context information and predicts future information. Extensive experiments on diverse downstream dialogue tasks demonstrate the effectiveness of our model, especially the generalization, robustness, and learning discriminative dialogue representations capabilities.","tags":["\" \""],"title":"FutureTOD: Teaching Future Knowledge to Pre-trained Language Model for Task-Oriented Dialogue","type":"publication"},{"authors":["Xuefeng Li","Liwen Wang","Guanting Dong","Keqing He","JinzhengZhao","Hao Lei","JiachiLiu","Weiran Xu"],"categories":[],"content":"","date":1683417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"3f5b6fe104482d3673307d1ed9316ff5","permalink":"https://pris-nlp.github.io/en/publication/generative-zero-shot-prompt-learning-for-cross-domain-slot-filling-with-inverse-prompting/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/generative-zero-shot-prompt-learning-for-cross-domain-slot-filling-with-inverse-prompting/","section":"publication","summary":"Zero-shot cross-domain slot filling aims to transfer knowledge from the labeled source domain to the unlabeled target domain. Existing models either encode slot descriptions and examples or design handcrafted question templates using heuristic rules, suffering from poor generalization capability or robustness. In this paper, we propose a generative zero-shot prompt learning framework for cross-domain slot filling, both improving generalization and robustness than previous work. Besides, we introduce a novel inverse prompting strategy to distinguish different slot types to avoid the multiple prediction problem, and an efficient prompt-tuning strategy to boost higher performance by only training fewer prompt parameters. Experiments and analysis demonstrate the effectiveness of our proposed framework, especially huge improvements (+13.44% F1) on the unseen slots.","tags":["\" \""],"title":"Generative zero-shot prompt learning for cross-domain slot filling with inverse prompting","type":"publication"},{"authors":["Daichi Guo","Guanting Dong","Dayuan Fu","YuxiangWu","Chen Zeng","Tingfeng Hui","Liwen Wang","Xuefeng Li","Zechen Wang","Keqing He","XinyueCui","Weiran Xu"],"categories":[],"content":"","date":1683417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"8cdc900c617b305c38a4843bcf623ee2","permalink":"https://pris-nlp.github.io/en/publication/revisit-out-of-vocabulary-problem-for-slot-filling-a-unified-contrastive-framework-with-multi-level-data-augmentations/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/revisit-out-of-vocabulary-problem-for-slot-filling-a-unified-contrastive-framework-with-multi-level-data-augmentations/","section":"publication","summary":"In real dialogue scenarios, the existing slot filling model, which tends to memorize entity patterns, has a significantly reduced generalization facing Out-of-Vocabulary (OOV) problems. To address this issue, we propose an OOV robust slot filling model based on multi-level data augmentations to solve the OOV problem from both word and slot perspectives. We present a unified contrastive learning framework, which pull representations of the origin sample and augmentation samples together, to make the model resistant to OOV problems. We evaluate the performance of the model from some specific slots and carefully design test data with OOV word perturbation to further demonstrate the effectiveness of OOV words. Experiments on two datasets show that our approach outperforms the previous sota methods in terms of both OOV slots and words.","tags":["\" \""],"title":"Revisit Out-Of-Vocabulary Problem For Slot Filling: A Unified Contrastive Framework With Multi-Level Data Augmentations","type":"publication"},{"authors":["Weihao Zeng","Lulu Zhao","Keqing He","Ruotong Geng","JingangWang","WeiWu","Weiran Xu"],"categories":[],"content":"","date":1683417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"113bc19c9176775314687593e7a2dedf","permalink":"https://pris-nlp.github.io/en/publication/seen-to-unseen-exploring-compositional-generalization-of-multi-attribute-controllable-dialogue-generation/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/seen-to-unseen-exploring-compositional-generalization-of-multi-attribute-controllable-dialogue-generation/","section":"publication","summary":"Existing controllable dialogue generation work focuses on the single-attribute control and lacks generalization capability to out-of-distribution multiple attribute combinations. In this paper, we explore the compositional generalization for multi-attribute controllable dialogue generation where a model can learn from seen attribute values and generalize to unseen combinations. We propose a prompt-based disentangled controllable dialogue generation model, DCG. It learns attribute concept composition by generating attribute-oriented prompt vectors and uses a disentanglement loss to disentangle different attributes for better generalization. Besides, we design a unified reference-free evaluation framework for multiple attributes with different levels of granularities. Experiment results on two benchmarks prove the effectiveness of our method and the evaluation metric.","tags":["\" \""],"title":"Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation","type":"publication"},{"authors":["Qixiang Gao","MingyangSun","Yutao Mu","Chen Zeng","Weiran Xu"],"categories":[],"content":"","date":1683417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"e3fad7c033eaade411ec669d198af867","permalink":"https://pris-nlp.github.io/en/publication/value-type-the-bridge-to-a-better-dst-model/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/value-type-the-bridge-to-a-better-dst-model/","section":"publication","summary":"Value type of the slots can provide lots of useful information for DST tasks. However, it has been ignored in most previous works. In this paper, we propose a new framework for DST task based on these value types. Firstly, we extract the type of token from each turn. Specifically, we divide the slots in the dataset into 9 categories according to the type of slot value, and then train a Ner model to extract the corresponding type-entity from each turn of conversation according to the token. Secondly, we improve the attention mode which is integrated into value type information between the slot and the conversation history to help each slot pay more attention to the turns that contain the same value type. Meanwhile, we introduce a sampling strategy to integrate these types into the attention formula, which decrease the error of Ner model. Finally, we conduct a comprehensive experiment on two multi-domain task-oriented conversation datasets, MultiWOZ 2.1 and MultiWOZ 2.4. The ablation experimental results show that our method is effective on both datasets, which verify the necessity of considering the type of slot value.","tags":["\" \""],"title":"Value type-the bridge to a better DST model","type":"publication"},{"authors":["Yanan Wu","Zhiyuan Zeng","Keqing He","Yutao Mu","Pei Wang","Yuanmeng Yan","Weiran Xu"],"categories":[],"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"b1a5b6f2913daa5900000d4c02fbf279","permalink":"https://pris-nlp.github.io/en/publication/disentangling-confidence-score-distribution-for-out-of-domain-intent-detection-with-energy-based-learning/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/disentangling-confidence-score-distribution-for-out-of-domain-intent-detection-with-energy-based-learning/","section":"publication","summary":"Detecting Out-of-Domain (OOD) or unknown intents from user queries is essential in a task-oriented dialog system. Traditional softmax-based confidence scores are susceptible to the overconfidence issue. In this paper, we propose a simple but strong energy-based score function to detect OOD where the energy scores of OOD samples are higher than IND samples. Further,given a small set of labeled OOD samples, we introduce an energy-based margin objective for supervised OOD detection to explicitly distinguish OOD samples from INDs. Comprehensive experiments and analysis prove our method helps disentangle confidence score distributions of IND and OOD data.","tags":["OOD","intent detection"],"title":"Disentangling Confidence Score Distribution for Out-of-Domain Intent Detection with Energy-Based Learning","type":"publication"},{"authors":["GangZhao","Guanting Dong","YidongShi","HaolongYan","Weiran Xu","SiLi"],"categories":[],"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"339c0e8367b43e9df82c2839a4c37796","permalink":"https://pris-nlp.github.io/en/publication/entity-level-interaction-via-heterogeneous-graph-for-multimodal-named-entity-recognition/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/entity-level-interaction-via-heterogeneous-graph-for-multimodal-named-entity-recognition/","section":"publication","summary":"Multimodal Named Entity Recognition (MNER) faces two specific challenges: 1) How to capture useful entity-related visual information; 2) How to alleviate the interference of visual noise. Previous works have gained progress by improving interacting mechanisms or seeking for better visual features. However, existing methods neglect the integrity of entity semantics and conduct cross-modal interaction at token-level, which cuts apart the semantics of entities and makes non-entity tokens easily interfered with by irrelevant visual noise. Thus in this paper, we propose an end-to-end heterogeneous Graph-based Entity-level Interacting model (GEI) for MNER. GEI first utilizes a span detection subtask to obtain entity representations, which serve as the bridge between two modalities.Then, the heterogeneous graph interacting network interacts entity with object nodes to capture entity-related visual information, and fuses it into only entity-associated tokens to rid non-entity tokens of the visual noise. Experiments on two widely used datasets demonstrate the effectiveness of our method.","tags":["Multimodal NER"],"title":"Entity-level Interaction via Heterogeneous Graph for Multimodal Named Entity Recognition","type":"publication"},{"authors":["Qixiang Gao","Guanting Dong","Yutao Mu","Liwen Wang","Chen Zeng","Daichi Guo","MingyangSun","Weiran Xu"],"categories":[],"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"8817f58c4a66ec325a2545323f326cbe","permalink":"https://pris-nlp.github.io/en/publication/exploiting-domain-slot-related-keywords-description-for-few-shot-cross-domain-dialogue-state-tracking/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/exploiting-domain-slot-related-keywords-description-for-few-shot-cross-domain-dialogue-state-tracking/","section":"publication","summary":"Collecting dialogue data with domain-slot-value labels for dialogue state tracking (DST) could be a costly process. In this paper, we propose a novel framework based on domain-slot related description to tackle the challenge of few-shot cross-domain DST. Specifically, we design an extraction module to extract domain-slot related verbs and nouns in the dialogue. Then, we integrates them into the description, which aims to prompt the model to identify the slot information. Furthermore, we introduce a random sampling strategy to improve the domain generalization ability of the model. We utilize a pre-trained model to encode contexts and description and generates answers with an auto-regressive manner. Experimental results show that our approaches substantially outperform the existing few-shot DST methods on MultiWOZ and gain strong improvements on the slot accuracy comparing to existing slot description methods.","tags":["\"DST\""],"title":"Exploiting domain-slot related keywords description for Few-Shot Cross-Domain Dialogue State Tracking","type":"publication"},{"authors":["Yutao Mu","Pei Wang","Keqing He","Yanan Wu","JingangWang","WeiWu","Weiran Xu"],"categories":[],"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"6e29b8946217e1f97c8662d50351120f","permalink":"https://pris-nlp.github.io/en/publication/uninl-aligning-representation-learning-with-scoring-function-for-ood-detection-via-unified-neighborhood-learning/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/uninl-aligning-representation-learning-with-scoring-function-for-ood-detection-via-unified-neighborhood-learning/","section":"publication","summary":"Detecting out-of-domain (OOD) intents from user queries is essential for avoiding wrong operations in task-oriented dialogue systems. The key challenge is how to distinguish in-domain (IND) and OOD intents. Previous methods ignore the alignment between representation learning and scoring function, limiting the OOD detection performance. In this paper, we propose a unified neighborhood learning framework (UniNL) to detect OOD intents. Specifically, we design a K-nearest neighbor contrastive learning (KNCL) objective for representation learning and introduce a KNN-based scoring function for OOD detection. We aim to align representation learning with scoring function. Experiments and analysis on two benchmark datasets show the effectiveness of our method.","tags":["\"OOD\"","\"intent detection\""],"title":"UniNL: Aligning Representation Learning with Scoring Function for OOD Detection via Unified Neighborhood Learning","type":"publication"},{"authors":["Yutao Mu","Keqing He","Pei Wang","Yanan Wu","JingangWang","WeiWu","Weiran Xu"],"categories":[],"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"ea25bb5bcb7ec725b3592f1f8cada438","permalink":"https://pris-nlp.github.io/en/publication/watch-the-neighbors-a-unified-k-nearest-neighbor-contrastive-learning-framework-for-ood-intent-discovery/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/watch-the-neighbors-a-unified-k-nearest-neighbor-contrastive-learning-framework-for-ood-intent-discovery/","section":"publication","summary":"Discovering out-of-domain (OOD) intent is important for developing new skills in task-oriented dialogue systems. The key challenges lie in how to transfer prior in-domain (IND) knowledge to OOD clustering, as well as jointly learn OOD representations and cluster assignments. Previous methods suffer from in-domain overfitting problem, and there is a natural gap between representation learning and clustering objectives. In this paper, we propose a unified K-nearest neighbor contrastive learning framework to discover OOD intents. Specifically, for IND pre-training stage, we propose a KCL objective to learn inter-class discriminative features, while maintaining intra-class diversity, which alleviates the in-domain overfitting problem. For OOD clustering stage, we propose a KCC method to form compact clusters by mining true hard negative samples, which bridges the gap between clustering and representation learning. Extensive experiments on three benchmark datasets show that our method achieves substantial improvements over the state-of-the-art methods.","tags":["\"OOD\"","\"Intent Discovery\""],"title":"Watch the Neighbors: A Unified K-Nearest Neighbor Contrastive Learning Framework for OOD Intent Discovery","type":"publication"},{"authors":["Weihao Zeng","Keqing He","Zechen Wang","Dayuan Fu","Guanting Dong","Ruotong Geng","Pei Wang","JingangWang","ChaoboSun","WeiWu","Weiran Xu"],"categories":[],"content":"","date":1665964800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"98d677abfd74b423eab7c1ab21ad52ad","permalink":"https://pris-nlp.github.io/en/publication/semi-supervised-knowledge-grounded-pre-training-for-task-oriented-dialog-systems/","publishdate":"2022-10-17T13:55:51.366191Z","relpermalink":"/en/publication/semi-supervised-knowledge-grounded-pre-training-for-task-oriented-dialog-systems/","section":"publication","summary":"Recent advances in neural approaches greatly improve task-oriented dialogue (TOD) systems which assist users to accomplish their goals. However, such systems rely on costly manually labeled dialogs which are not available in practical scenarios. In this paper, we present our models for Track 2 of the SereTOD 2022 challenge, which is the first challenge of building semi-supervised and reinforced TOD systems on a large-scale real-world Chinese TOD dataset MobileCS. We build a knowledge-grounded dialog model to formulate dialog history and local KB as input and predict the system response. And we perform semi-supervised pre-training both on the labeled and unlabeled data. Our system achieves the first place both in the automatic evaluation and human interaction, especially with higher BLEU (+7.64) and Success (+13.6\\%) than the second place.","tags":["\"TOD\""],"title":"Semi-Supervised Knowledge-Grounded Pre-training for Task-Oriented Dialog Systems","type":"publication"},{"authors":["Yanan Wu","Zhiyuan Zeng","Keqing He","Yutao Mu","Pei Wang","Weiran Xu"],"categories":[],"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"457bf936e009e38f8ca6455f95f87741","permalink":"https://pris-nlp.github.io/en/publication/distribution-calibration-for-out-of-domain-detection-with-bayesian-approximation/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/distribution-calibration-for-out-of-domain-detection-with-bayesian-approximation/","section":"publication","summary":"Out-of-Domain (OOD) detection is a key component in a task-oriented dialog system, which aims to identify whether a query falls outside the predefined supported intent set. Previous softmax-based detection algorithms are proved to be overconfident for OOD samples. In this paper, we analyze overconfident OOD comes from distribution uncertainty due to the mismatch between the training and test distributions, which makes the model can’t confidently make predictions thus probably causing abnormal softmax scores. We propose a Bayesian OOD detection framework to calibrate distribution uncertainty using Monte-Carlo Dropout.Our method is flexible and easily pluggable into existing softmax-based baselines and gains 33.33% OOD F1 improvements with increasing only 0.41% inference time compared to MSP. Further analyses show the effectiveness of Bayesian learning for OOD detection.","tags":["\"OOD\"","\"intent detection\""],"title":"Distribution Calibration for Out-of-Domain Detection with Bayesian Approximation","type":"publication"},{"authors":["Yutao Mu","Keqing He","Yanan Wu","Pei Wang","JingangWang","WeiWu","YiHuang","JunlanFeng","Weiran Xu"],"categories":[],"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"4179d169681a60cb995417396542cc49","permalink":"https://pris-nlp.github.io/en/publication/generalized-intent-discovery-learning-from-open-world-dialogue-system/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/generalized-intent-discovery-learning-from-open-world-dialogue-system/","section":"publication","summary":"Traditional intent classification models are based on a pre-defined intent set and only recognize limited in-domain (IND) intent classes. But users may input out-of-domain (OOD) queries in a practical dialogue system. Such OOD queries can provide directions for future improvement. In this paper, we define a new task, Generalized Intent Discovery (GID), which aims to extend an IND intent classifier to an open-world intent set including IND and OOD intents.  We hope to simultaneously classify a set of labeled IND intent classes while discovering and recognizing new unlabeled OOD types incrementally. We construct three public datasets for different application scenarios and propose two kinds of frameworks, pipeline-based and end-to-end for future work. Further, We conduct exhaustive experiments and qualitative analysis to comprehend key challenges and provide new guidance for future GID research.","tags":["\"OOD\"","\"Intent Discovery\""],"title":"Generalized Intent Discovery: Learning from Open World Dialogue System","type":"publication"},{"authors":["Guanting Dong","Daichi Guo","Liwen Wang","Xuefeng Li","Zechen Wang","Chen Zeng","Keqing He","JinzhengZhao","Hao Lei","XinyueCui","YiHuang","JunlanFeng","Weiran Xu"],"categories":[],"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"17a70b06af60eab2d96e0935defff7a6","permalink":"https://pris-nlp.github.io/en/publication/pssat-a-perturbed-semantic-structure-awareness-transferring-method-for-perturbation-robust-slot-filling/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/pssat-a-perturbed-semantic-structure-awareness-transferring-method-for-perturbation-robust-slot-filling/","section":"publication","summary":"Most existing slot filling models tend to memorize inherent patterns of entities and corresponding contexts from training data. However, these models can lead to system failure or undesirable outputs when being exposed to spoken language perturbation or variation in practice. We propose a perturbed semantic structure awareness transferring method for training perturbation-robust slot filling models. Specifically, we introduce two MLM-based training strategies to respectively learn contextual semantic structure and word distribution from unsupervised language perturbation corpus. Then, we transfer semantic knowledge learned from upstream training procedure into the original samples and filter generated data by consistency processing. These procedures aim to enhance the robustness of slot filling models. Experimental results show that our method consistently outperforms the previous basic methods and gains strong generalization while preventing the model from memorizing inherent patterns of entities and contexts.","tags":["Slot filling"],"title":"PSSAT: A Perturbed Semantic Structure Awareness Transferring Method for Perturbation-Robust Slot Filling","type":"publication"},{"authors":["---"],"categories":[],"content":"Distribution Calibration for Out-of-Domain Detection with Bayesian Approximation 论文作者  吴亚楠1， 曾致远1， 何可清1， 牟宇滔， 王霈， 徐蔚然 2\n发表会议  COLING2022 Long Paper\n论文简介 域外(out-of-domain, OOD)检测是任务型对话系统应对开放世界的重要要求，在正确识别预定义意图基础上，旨在检测用户输入query是否超出预定义意图。然而，传统的基于softmax的域外检测算法在域外样本识别上存在过自信问题。本文结合理论分析发现，训练分布与测试分布之间不匹配，使得模型不能自信地做出预测，从而导致softmax分数异常，换句话说，缓解分布不确定性是解决过自信问题的关键。基于此，我们提出了基于贝叶斯估计的域外检测算法，在推理阶段，利用蒙特卡罗Dropout多次传导求期望，以校准分布不确定性，将实际错误的尖锐域外分布校准为接近理想的均匀分布，以便更好的区分域内和域外数据。我们的方法灵活适用于现有的基于概率的域外检测算法，相比于MSP，我们的方法在不增加任何模型参数，仅增加0.41%推理时间消耗上，可以获得了33.33%的OOD F1指标提升。文章中大量的实验和分析进一步证明了贝叶斯学习在OOD检测中的有效性。\n模型整体结构\n代码  github\n Generalized Intent Discovery: Learning from Open World Dialogue System 论文作者  牟宇滔1， 何可清1， 吴亚楠， 王霈，王金刚，武威，黄毅，冯俊兰， 徐蔚然 2\n发表会议  COLING2022 Long Paper\n论文简介 传统的意图分类模型基于领域专家预定义好的域内（IND）意图集，并且只能识别有限的域内意图。然而当对话系统线上部署之后，用户可能会输入域外（OOD）查询。这些OOD数据为系统未来进一步开发提供潜在发展方向。在本文中，我们定义了一个新任务——泛化意图发现（GID），旨在将IND意图分类器的识别范围扩展到包括IND意图和新出现的OOD意图的开放世界意图集中。具体地，我们希望训练一个统一的模型以端到端的方式同时学习分类有标注的域内意图类，以及增量式地从无标注OOD数据中发现和识别的新的域外意图类，以便自动扩展分类器的识别范围，通俗地说就是自动扩充分类头。为了能够在更真实地场景下讨论GID任务，我们构建了三个基准数据集，分别模拟单领域，多领域和跨领域场景，并在此基础上衍生出两个变体数据集，分别模拟真实环境中OOD噪声和OOD类别不平衡场景。在方法上，我们提出了两个框架，pipeline和端到端框架，以便后续GID研究使用。此外，我们还做了大量地分析实验，综合地讨论和分析了GID任务的关键挑战，并为这个方向未来的研究提供了新的指导。\nGID任务示意图 pipeline和end-to-end框架示意图\n代码  github\n PSSAT: A Perturbed Semantic Structure Awareness Transferring Method for Perturbation-Robust Slot Filling 论文作者  董冠霆1， 郭岱驰1， 王礼文1， 李雪峰1， 王泽晨， 曾晨， 何可清，赵金政， 雷浩，崔馨月，黄毅，冯俊兰, 徐蔚然 2\n发表会议  COLING2022 Short Paper\n论文简介 大多数现有的槽填充模型倾向于从训练数据中记忆实体的固有模式与对应的上下文。然而，当这些模型暴露于真实场景下的口语扰动时，可能会导致性能下降或产生期望之外的输出。我们提出了一种扰动语义结构感知迁移方法，来训练出对扰动具有鲁棒性的槽填充模型。具体来说，我们引入了两种基于MLM的训练策略，分别从含有无监督语言扰动的语料库中学习上下文语义结构和实体分布。然后，将上游预训练过程中学习到的语义知识迁移到原始样本中，并通过一致性处理模块过滤低质量的增强数据。这些程序旨在提高槽填充模型的鲁棒性。实验结果表明，我们的方法始终优于以往的基本方法，并在防止模型记忆实体和上下文固有模式的同时，获得了较强的泛化性。\nPSSAT框架示意图 扰动鲁棒填槽框架示意图\n文章链接  Click here\n   Equal Contribution\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Corresponding Author\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"1a23fca661fb4bf04d5c2ac798373f07","permalink":"https://pris-nlp.github.io/en/post/221001wyj/","publishdate":"2022-10-01T00:00:00Z","relpermalink":"/en/post/221001wyj/","section":"post","summary":" ","tags":null,"title":"Recent research overview（2022.10.01）","type":"post"},{"authors":["Lulu Zhao","Fujia Zheng","Weihao Zeng","Keqing He","Ruotong Geng","HuixingJiang","WeiWu","Weiran Xu"],"categories":[],"content":"","date":1657497600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"f0dc50d81bac1f831ea38bee5884f9d5","permalink":"https://pris-nlp.github.io/en/publication/adpl-adversarial-prompt-based-domain-adaptation-for-dialogue-summarization-with-knowledge-disentanglement/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/adpl-adversarial-prompt-based-domain-adaptation-for-dialogue-summarization-with-knowledge-disentanglement/","section":"publication","summary":"Traditional dialogue summarization models rely on a large-scale manually-labeled corpus, lacking generalization ability to new domains, and domain adaptation from a labeled source domain to an unlabeled target domain is important in practical summarization scenarios. However, existing domain adaptation works in dialogue summarization generally require large-scale pre-training using exten- sive external data. To explore the lightweight fine-tuning methods, in this paper, we propose an efficient Adversarial Disentangled Prompt Learning (ADPL) model for domain adaptation in dialogue sum- marization. We introduce three kinds of prompts including domain- invariant prompt (DIP), domain-specific prompt (DSP), and task- oriented prompt (TOP). DIP aims to disentangle and transfer the shared knowledge from the source domain and target domain in an adversarial way, which improves the accuracy of prediction about domain-invariant information and enhances the ability for generalization to new domains. DSP is designed to guide our model to focus on domain-specific knowledge using domain-related features. TOP is to capture task-oriented knowledge to generate high-quality summaries. Instead of fine-tuning the whole pre-trained language model (PLM), we only update the prompt networks but keep PLM fixed. We conduct zero-shot experiments and build domain adaptation benchmarks on two multi-domain dialogue summarization datasets, TODSum and QMSum. Adequate experiments and analysis prove our method significantly outperforms full-parameter fine-tuning with even fewer parameters.","tags":["Dialogue Summarization"],"title":"ADPL: Adversarial Prompt-based Domain Adaptation for Dialogue Summarization with Knowledge Disentanglement","type":"publication"},{"authors":["Lulu Zhao","Fujia Zheng","Weihao Zeng","Keqing He","Weiran Xu","HuixingJiang","WeiWu","Yanan Wu"],"categories":[],"content":"","date":1657411200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"c461c5cb665e1cea0e191d64bd5da9bc","permalink":"https://pris-nlp.github.io/en/publication/domain-oriented-prefix-tuning-towards-efficient-and-generalizable-fine-tuning-for-zero-shot-dialogue-summarization/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/domain-oriented-prefix-tuning-towards-efficient-and-generalizable-fine-tuning-for-zero-shot-dialogue-summarization/","section":"publication","summary":"The most advanced abstractive dialogue summarizers lack generalization ability on new domains and the existing researches for domain adaptation in summarization generally rely on large-scale pre-trainings. To explore the lightweight fine-tuning methods for domain adaptation of dialogue summarization, in this paper, we propose an efficient and generalizable Domain-Oriented Prefix-tuning model, which utilizes a domain word initialized prefix module to alleviate domain entanglement and adopts discrete prompts to guide the model to focus on key contents of dialogues and enhance model generalization. We conduct zero-shot experiments and build domain adaptation benchmarks on two multi-domain dialogue summarization datasets, TODSum and QMSum. Adequate experiments and qualitative analysis prove the effectiveness of our methods.","tags":["Dialogue Summarization"],"title":"Domain-Oriented Prefix-Tuning: Towards Efficient and Generalizable Fine-tuning for Zero-Shot Dialogue Summarization","type":"publication"},{"authors":["Yanan Wu","Keqing He","Yuanmeng Yan","Qixiang Gao","Zhiyuan Zeng","Fujia Zheng","Lulu Zhao","HuixingJiang","WeiWu","Weiran Xu"],"categories":[],"content":"","date":1657411200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"be72105cfcb1048557227ba54a49ffa7","permalink":"https://pris-nlp.github.io/en/publication/revisit-overconfidence-for-ood-detection-reassigned-contrastive-learning-with-adaptive-class-dependent-threshold/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/revisit-overconfidence-for-ood-detection-reassigned-contrastive-learning-with-adaptive-class-dependent-threshold/","section":"publication","summary":"Detecting Out-of-Domain (OOD) or unknown intents from user queries is essential in a task-oriented dialog system. A key challenge of OOD detection is the overconfidence of neural models. In this paper, we comprehensively analyze overconfidence and classify it into two perspectives: over-confident OOD and in-domain (IND). Then according to intrinsic reasons, we respectively propose a novel reassigned contrastive learning (RCL) to discriminate IND intents for over-confident OOD and an adaptive class-dependent local threshold mechanism to separate similar IND and OOD intents for over-confident IND. Experiments and analyses show the effectiveness of our proposed method for both aspects of overconfidence issues.","tags":["OOD","Intent detection"],"title":"Revisit Overconfidence for OOD Detection: Reassigned Contrastive Learning with Adaptive Class-dependent Threshold","type":"publication"},{"authors":["Xuefeng Li","Hao Lei","Liwen Wang","Guanting Dong","JinzhengZhao","JiachiLiu","Weiran Xu","ChunyunZhang"],"categories":[],"content":"","date":1653177600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"1795bae7c7635aed33742795ba6f94d3","permalink":"https://pris-nlp.github.io/en/publication/a-robust-contrastive-alignment-method-for-multi-domain-text-classification/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/a-robust-contrastive-alignment-method-for-multi-domain-text-classification/","section":"publication","summary":"Multi-domain text classification can automatically classify texts in various scenarios. Due to the diversity of human languages, texts with the same label in different domains may differ greatly, which brings challenges to the multidomain text classification. Current advanced methods use the private-shared paradigm, capturing domain-shared features by a shared encoder, and training a private encoder for each domain to extract domain-specific features. However, in realistic scenarios, these methods suffer from inefficiency as new domains are constantly emerging. In this paper, we propose a robust contrastive alignment method to align text classification features of various domains in the same feature space by supervised contrastive learning. By this means, we only need two universal feature extractors to achieve multidomain text classification. Extensive experimental results show that our method performs on par with or sometimes better than the state-of-the-art method, which uses the complex multi-classifier in a private-shared framework","tags":["Feature Alignment","Contrastive Learning","Robust Training"],"title":"A Robust Contrastive Alignment Method For Multi-Domain Text Classification","type":"publication"},{"authors":["Yutao Mu","Keqing He","Yanan Wu","Zhiyuan Zeng","Hong Xu","HuixingJiang","WeiWu","Weiran Xu"],"categories":[],"content":"","date":1653177600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"4e74438f8bc75bed704e4ba7d62b06b8","permalink":"https://pris-nlp.github.io/en/publication/disentangled-knowledge-transfer-for-ood-intent-discovery-with-unified-contrastive-learning/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/disentangled-knowledge-transfer-for-ood-intent-discovery-with-unified-contrastive-learning/","section":"publication","summary":"Discovering Out-of-Domain(OOD) intents is essential for developing new skills in a task-oriented dialogue system. The key challenge is how to transfer prior IND knowledge to OOD clustering. Different from existing work based on shared intent representation, we propose a novel disentangled knowledge transfer method via a unified multi-head contrastive learning framework. We aim to bridge the gap between IND pre-training and OOD clustering. Experiments and analysis on two benchmark datasets show the effectiveness of our method.","tags":["OOD","Intent Discovery"],"title":"Disentangled Knowledge Transfer for OOD Intent Discovery with Unified Contrastive Learning","type":"publication"},{"authors":["Zechen Wang"],"categories":[],"content":"Debiased Contrastive Learning of Unsupervised Sentence Representations 文章链接： https://arxiv.org/abs/2205.00656\n代码： https://github.com/RUCAIBox/DCLR\n摘要和引言 最近，对比学习被证明在改进预训练的语言模型（PLM）以获得高质量的句子表征方面是有效的。它的目的是拉近正样本以提高对齐度，同时推开不相关的负样本以提高整个表示空间的统一性。然而，以前的工作大多采用从一个batch内或从训练数据中随机抽取样本作为负样本。这样的方式可能会造成采样偏差，即不适当的负样本（如伪负样本和各向异性表征）被用来学习句子表征，这将损害表征空间的均匀性。为了解决这个问题，我们提出了一个新的框架DCLR（Debiased Contrastive Learning of unsupervised sentence Representations）来减轻这些不适当的负样本的影响。在DCLR中，我们设计了一种实例加权的方法来惩罚错误的负样本，并生成基于噪声的负样本来保证表示空间的统一性。在七个语义文本相似性任务上的实验表明，我们的方法比竞争性基线更有效。\n近来，预训练语言模型被广泛用于语义表征的实现方法，在多个NLP任务上取得了显著的表现。然而，一些研究发现，由PLM产生的原始句子表征在方向上并不是均匀分布的，而是在向量空间中形成一个狭窄的锥体（Ethayarajh，2019），这在很大程度上限制了其表现力。为了解决这个问题，对比学习（Chen等人，2020）已经被采用来完善PLM产生的句子表征。它将语义上接近的样本拉到一起，以提高一致性，同时将负样本推开，以提高整个表征空间的统一性。对于正样本，以前的工作在原句上应用了数据增强策略（Yan等人，2021），以产生高度相似的变化。而由于缺乏负样本的Golden Label，负面例子通常是从批处理或训练数据中随机抽取的（例如，批内负样本（Gao等人，2021））。\n虽然这种负样本的抽样方式简单方便，但它可能导致抽样偏差，影响句子表征的学习。首先，抽样的负样本很可能是虚假的负样本，而这些负样本在语义上确实与原句接近。如图1所示，根据SimCSE模型（Gao等人，2021年），给定一个输入句子，大约一半的批量内负样本与原句的余弦相似度超过0.7。通过简单地推开这些采样的负样本，很可能会伤害到句子表征的语义。其次，由于各向异性问题（Ethayarajh, 2019），采样负样本的表征来自PLMs所跨越的狭窄表征锥，不能完全反映表征空间的整体语义。因此，只依靠这些表征来学习句子表征的统一性目标是次优的。\n为了解决上述问题，我们的目标是开发一种更好的对比学习方法，采用去偏的负采样策略。核心思想是改进随机抽样策略以缓解抽样偏差问题。首先，在我们的框架中，我们设计了一个实例加权的方法，以惩罚训练中的伪负样本。我们采用了一个补充模型来评估每个负样本与原句之间的相似性，然后为相似性较高的负样本分配较低的权重。通过这种方式，我们可以检测到语义上接近的假负样本，并进一步减少其影响。其次，我们根据随机的高斯噪声随机初始化新的负样本，以模拟整个语义空间内的抽样，并设计一个基于梯度的算法来优化基于噪声的负样本，以达到最不均匀的形式。通过与不均匀的基于噪声的负样本进行对比，我们可以扩展句子表征的空间，并提高表征空间的均匀性。\n为此，我们提出了DCLR，这是一个针对无监督句子表征的去偏向对立学习的一般框架。在我们的方法中，我们首先从高斯分布中初始化基于噪声的负样本，并利用基于梯度的算法来更新新的负样本，同时考虑代表空间的均匀性。然后，我们采用辅助模型来产生这些基于噪声的负样本和随机抽样的负样本的权重，其中错误的负样本将受到惩罚。最后，我们通过dropout（Gao等人，2021）来增加正面的例子，并将它们与上述加权的负面例子结合起来进行对比学习。我们证明了我们的DCLR在使用BERT（Devlin等人，2019）和RoBERTa（Liu等人，2019）的七个语义文本相似性（STS）任务上优于一些竞争基线。\n这项工作的目的是利用未标记的语料来学习有效的句子表征，可以直接用于下游任务，例如语义文本相似性任务（Agirre等人，2015）。给定一组输入句子X={x1, x2, . . . , xn}，我们的目标是以一种无监督的方式为每个句子xi学习一个代表hi∈Rd。为了简单起见，我们用一个参数化的函数hi = f(xi)来表示这个过程。\n在这项工作中，我们主要关注的是使用基于BERT的PLMs（Devlin等人，2019；Liu等人，2019）来生成句子表征。按照现有的工作（Li等人，2020年；Yan等人，2021年），我们通过我们提出的无监督学习方法在未标记的语料库上微调PLM。之后，对于每个句子xi，我们用微调后的PLM进行编码，并将最后一层的[CLS]标记的表示作为其句子表示hi。\n方法 我们根据高斯分布初始化基于噪声的负样本，并迭代更新这些负样本，以达到非均匀性最大化。然后，我们利用一个辅助模型为所有的负样本（即随机抽样和基于噪声的负样本）产生权重。最后，我们将加权的负样本和增强的正面例子结合起来，进行对比学习。我们的DCLR的概述见图2。\n生成基于噪声的负样本 我们的目标是在训练过程中，在PLMs的句子表征空间之外生成新的负样本，以减轻PLMs各向异性问题带来的采样偏差（Etha- yarajh，2019）。对于每个输入句子xi，我们首先从高斯分布中初始化k个噪声向量作为样本表征： $$ {\\hat h_1, \\hat h_2,\\cdots, \\hat h_k}\\sim \\mathcal{N}(0, \\sigma^2) $$ 其中σ是标准差。由于这些向量是从这样的高斯分布中随机初始化的，因此它们在整个语义空间中是均匀分布的。通过学习与这些新的负样本进行对比，有利于句子表征的均匀性。\n为了进一步提高新负样本的质量，我们考虑迭代更新负样本以捕捉整个语义空间中的非均匀性点。受VAT（Miy- ato等人，2017；Zhu等人，2020）的启发，我们设计了一个非均匀性损失最大化目标，以产生梯度来改善这些负样本。非均匀性损失被表示为基于噪声的负样本${\\hat h_j}$和原句的正样本$ (h_i,h_i^+) $之间的对比损失：\n$$ L_U(h_i,h_i^+,{\\hat h})=-\\log\\frac{e^{{\\rm sim}(h_i,h_i^+)/\\tau_u}}{\\sum_{\\hat h_j \\in{\\hat h}}e^{{\\rm sim}(h_i, \\hat h_i)/\\tau_u}} $$ 其中$\\tau_u$是温度超参数，${\\rm sim}(h_i, h_i^+)$是余弦相似度$\\frac{h_i^\\top h_i^+}{||h_i||\\cdot||h_i^+||}$。在此基础上，对于每个负样本$\\hat h_j\\in{\\hat h}$，我们用$t$步梯度下降法优化它 $$ \\hat h_j=\\hat h_j+\\beta g(\\hat h_j)/||g(\\hat h_j)||_2\\\ng(\\hat h_j)=\\nabla_{\\hat h_j}L_U(h_i,h_i^+,{\\hat h}) $$\n这样一来，基于噪声的负样本将被优化到句子表征空间的非均匀点。通过学习与这些负样本的对比，可以进一步提高表示空间的均匀性，这对有效的句子表示是至关重要的。\n实例加权的对比学习 除了上述基于噪声的负样本，我们还遵循现有的工作（Yan等人，2021；Gao等人，2021），采用其他batch内表征作为负样本${\\tilde h^-}$。然而，正如前面所讨论的，采样的负样本可能包含与正样本有相似语义的例子（即伪负样本）。\n为了缓解这个问题，我们提出了一种实例加权的方法来惩罚伪负样本。由于我们无法获得真实的标签或语义相似性，我们利用一个补充模型来产生每个负数的权重。我们采用最先进的SimCSE（Gao等人，2021）作为补充模型。给定一个来自${\\tilde h^-}$或${\\hat h}$的负样本$h^-$和原始句子$h_i$的表征，我们利用辅助模型来产生权重： $$ \\alpha_{h^-}=\\begin{cases} 0,{\\rm sim}_C(h_i,h^-)\\ge\\phi\\\n1,{\\rm sim}_C(h_i,h^-)\u0026lt;\\phi \\end{cases} $$ 其中${\\rm sim}_C$是辅助模型评估的余弦相似度。这样一来，与原句的语义相似度较高的负样本将被视为伪负样本，并被赋予0的权重以示惩罚。 基于这些权重，我们用去掉交叉熵的对比学习损失函数来优化句子表征，即 $$ L=-\\log\\frac{e^{{\\rm sim}(h_i,h_i^+)/\\tau}}{\\sum_{h^- \\in{\\hat h}\\cup{\\tilde h^-}}e^{{\\rm sim}(h_i, h^-)/\\tau}} $$ 如上所述，我们的方法旨在重新消除关于负面的抽样偏差的影响，并且对各种正样本的增强方法（例如，token cutoff和dropout）是不关心的。由于基于噪声的负样本是从高斯分布中初始化的，并不对应于真实的句子，因此它们是高度自信的负样本，以扩大表示空间。通过学习与它们的对比，对比目标的学习将不会受到来自PLMs的各向异性表征的限制。因此，句子表征可以跨越更广泛的语义空间，表征语义空间的统一性也可以得到改善。\n此外，我们的实例加权方法还缓解了随机抽样策略造成的伪负样本问题。在补充模型的帮助下，那些与原句语义相似的伪负样本句子将被检测出来并受到惩罚。\n实验 按照以前的工作（Kim等人，2021；Gao等人，2021），我们对七个标准的语义文本相似度任务（Semantic Textual Similarity，STS）进行了实验。对于所有这些任务，我们使用SentEval工具包（Conneau and Kiela, 2018）进行评估。\n我们在7个STS任务中评估了我们的方法。STS 2012-2016（Agirre等人，2012，2013，2014，2015，2016），STS Benchmark（Cer等人，2017）和SICK-Relatedness（Marelli等人，2014）。这些数据集包含成对的两个句子，其相似度分数从0到5进行标注。黄金标注和句子表征预测的分数之间的相关性由Spearman关联度来衡量。根据以往工作的建议（Gao等人，2021；Reimers和Gurevych，2019），我们直接计算所有STS任务的句子嵌入之间的余弦相似度。\n基线方法 我们将DCLR与竞争性的无监督句子表示学习方法进行比较，包括非BERT和基于BERT的方法。\n GloVe：把GloVe的词嵌入平均作为句子表征。 USE：一个Transformer模型，训练目标是在一段话中重建周围的句子。 CLS，Mean, First-Last AVG分别采用[CLS]嵌入、token表征的平均池化、第一层和最后一层的平均表征作为句子表征。 Flow 将平均池化应用在各层表征上，并将输出映射到高斯空间上作为句子表征。 Whitening使用whitening操作来完善表征，降低维度。 Contrastive(BT)使用对比学习与回译进行数据增强，以提高句子的代表性。 ConSERT探讨了各种文本增强策略，用于句子表征的对比学习。 SG-OPT提出了一种具有自我引导机制的自适应学习方法，以改善PLM的句子嵌入。 SimCSE提出了一个利用dropout进行数据增强的对比学习框架。  使用从维基百科中随机抽取的1,000,000个句子作为训练语料库。\n实验分析 为了验证我们的框架在PLM上的有效性，我们选择BERT-base和RoBERTa-base作为基础模型。表1显示了不同方法在七个STS任务上的结果。根据这些结果，我们可以发现，非BERT方法(即GloVe和USE)大部分都比基于PLM表示的基线(即CLS，Mean和First-Last AVG)要好。原因是直接利用PLM的原始表征很容易受到各向异性的影响。在非BERT方法中，USE优于Glove。一个潜在的原因是USE使用Transformer模型对句子进行编码，这比简单的平均GloVe 嵌入更有效。\n对于其他基于PLM的方法，首先，我们可以看到flow和whiteening取得了类似的结果，并在一定程度上超过了基于原始表征的方法。这两种方法采用了特定的改进策略来完善PLMs的表征。第二，基于对比学习的方法在大多数情况下优于其他基线。对比学习可以提高语义相关的正样本对之间的一致性和使用负样本的表征空间的均匀性，从而产生更好的句子表征。此外，SimCSE在所有基线中表现最好。这表明dropout是一种比其他方法更有效的正样本增强方法，因为它很少伤害到句子的语义。\n最后，DCLR在大多数情况下都比所有的基线表现得更好，包括基于对比学习的方法。由于这些方法大多利用随机抽样的负样本（如batch内负样本）来学习所有句子表征的统一性，它可能会导致抽样偏差，如假负样本和各向异性的表征。与这些方法不同的是，我们的框架采用了一种实例加权的方法来惩罚错误的负样本，并采用了一种基于梯度的算法来生成基于噪声的负样本。这样一来，采样偏差问题可以得到缓解，我们的模型可以更好地学习均匀性，以提高句子表征的质量。\n扩展实验 由于我们提出的DCLR是一个通用的框架，主要集中在无监督的句子代表的锥形学习的负采样上，它可以应用于其他依靠不同的正样本增强策略的方法。因此，在这一部分中，我们进行了实验，以检验我们的框架是否能够通过以下积极的数据增强策略带来改进：（1）Token Shuffling，随机打乱输入句子中的token顺序；（2）Feature/Token/Span Cutoff（Yan等人，2021），随机置零输入中的特征/token/token span；（3）Dropout，类似于SimCSE（Gao等人，2021）。\n如图3所示，我们的DCLR可以提高所有这些增强策略的性能，它显示了我们的框架与各种增强策略的有效性。此外，在所有的变体中，Dropout策略取得了最好的性能。这表明Dropout是一种更有效的增强高质量正样本的方法，也更适合于我们的方法。\n消融实验 如表2所示，删除每个组件都会导致性能下降。这表明，在我们的框架中，实例加权法和基于噪声的负样本都很重要。但是，去除实例加权方法会导致更大的性能下降。其原因可能是错误的负样本对句子表示学习有较大的影响。此外，我们准备了三种变体进行进一步的比较：（1）Random Noise直接生成基于噪声的负样本，而没有基于梯度的优化；（2）Knowledge Distillation（Hinton等人，2015）利用SimCSE作为教师模型，在训练期间将知识提炼到学生模型中；（3）Self Instance Weighting采用模型本身作为补充模型来生成权重。从表2中，我们可以看到这些变化并没有像原始的DCLR那样表现良好。这些结果表明，第4节中提出的设计更适合我们的DCLR框架。\n均匀性分析 均匀性是句子表征的一个理想特性，描述了表征的均匀分布程度。为了验证我们框架的均匀性的改善，我们比较了DCLR和SimCSE在训练期间使用BERT-base的均匀性损失曲线。\n按照SimCSE（Gao等人，2021年），我们利用以下函数来评估均匀性： $$ \\ell_{uniform}\\triangleq \\mathop{\\mathbb E}_{x_i,x_j \\sim p_{data}}\\exp(-2||f(x_i)-f(x_j)||^2) $$ 这个损失的数值越小，说明均匀性越好。如图4所示，几乎在整个训练过程中，DCLR的均匀性损失要比SimCSE的低得多。此外，我们可以看到，随着训练的进行，DCLR的均匀性损失下降得更快，而SimCSE的均匀性损失则没有明显的下降趋势。这可能是因为我们的DCLR在表示空间之外对基于噪声的负样本进行了采样，这可以更好地提高句子表示的均匀性。\n少样本设置下的表现 为了验证DCLR在数据稀缺情况下的可靠性和稳健性，我们使用BERT-base作为骨干模型进行了少样本实验。我们通过从100%到极小规模（即0.3%）的不同数量的可用训练数据训练我们的模型。我们报告了在STS-B和SICK-R任务上评估的结果。\n如图5所示，我们的方法在不同比例的训练数据下取得了稳定的结果。在数据比例为0.3%的最极端设置下，我们的模型在STS-B和SICK-R上的性能分别只下降了9%和4%。这些结果揭示了我们的方法在数据稀缺的情况下的稳健性和有效性。这种特性在现实世界的应用中是很重要的。\n总结 在本文中，我们提出了DCLR，一个用于无监督的去偏句子表征学习学习框架。我们的核心思想是缓解由随机负抽样策略引起的抽样偏差。为了实现这一目标，在我们的框架中，我们采用了一种实例加权的方法来惩罚训练过程中的错误负样本，并产生了基于噪声的负样本，以减轻各向异性的PLM衍生代表的影响。在七个STS任务上的实验结果表明，我们的方法优于几个有竞争力的基线。在未来，我们将探索其他方法来减少句子表征的对比性学习中的偏差。此外，我们还将考虑将我们的方法用于多语言或多模态表征学习。\n","date":1652918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652929200,"objectID":"450f9660520feafa7616b067ca300fcb","permalink":"https://pris-nlp.github.io/en/post/220519wzc/","publishdate":"2022-05-19T00:00:00Z","relpermalink":"/en/post/220519wzc/","section":"post","summary":" ","tags":null,"title":"Debiased Contrastive Learning of Unsupervised Sentence Representations","type":"post"},{"authors":["Weihao Zeng"],"categories":[],"content":"本文介绍一下我们组在面向领域迁移的对话摘要任务上的工作。\n1. Motivations  当前的对话摘要模型往往缺乏在新领域上的泛化性，因为大规模的生成式预训练模型往往需要大量的人工标注的黄金摘要，在 few/no labeled 的场景下无法扩展到新的领域。 当前研究摘要领域迁移的方法需要耗时的预训练和大规模额外的语料库。他们仅关注沉重的预训练步骤而不是轻量化的微调过程。  2. Contributions  我们第一个探索面向领域迁移的对话摘要任务的fine-tuning方法，并且在TODSum(TODSum是我们提出的对话摘要数据集 TODSum)和QMSum两个数据集上建立了实用且全面的benchmarks. 提出了轻量且有效的面向领域的PrezZfix-tuning的模型，该模型使用领域词初始化的prefix模块以及离散的prompt来从大规模预训练模型中交互式地提取知识。 进行了充分的实验和定量分析来证明了我们提出的方法的有效性，并且讨论了面向领域迁移的对话摘要所存在的挑战。  3. Methodology 模型结构包括Domain-Oriented Prefix，Prompt Encoder以及Decoder三个部分。\n3.1 Domain-Oriented Prefix 为了缓解领域耦合的问题，我们提出了domain-oriented的prefix模块来从源域和目标域中获取共享的知识。 采用two-step构建Domain-Oriented Prefix，包括初始化和参数化。\n3.1.1 Initialization  利用LDA主题模型从对话文本中提取每个领域的关键词，并且将他们拼接起来构成domain word（prefix）序列$x_{dw}$ 随机初始化domain word序列组成可学习的矩阵$M_{\\theta}\\in R^{|x_{dw}|d_{m}}$  3.1.2 Parametrization  利用MLP和预训练的BART模型分别得到domain word序列的表示，重新训练MLP使用MSE loss使得MLP的输出与预训练的BART的decoder hidden states相同，以此从预训练模型中解藕出领域知识。 在更新MLP参数的过程中保持预训练的BART的参数固定。 得到MLP的参数初始化，并使用预训练好的MLP来映射prefix表示的初始化embeddings.  3.2 Prompt Encoder 3.2.1 Discrete Prompts  将TODSum数据集中的对话状态和QMSum中的queries作为离散的Prompts 对于对话状态这种结构化的信息，将结构化的信息转化为文本序列。  3.2.2 Transformer Layer  将离散的prompt序列$x_{dp}$以及对话文本序列$x_{d}$作为encoder的输入序列。 通过修改添加domain-oriented prefix序列的键值对来修改self- attention机制。  3.3 Decoder 将Prefix模块也加到decoder上，以类似的方式修改cross-attention和self- attention机制。\n3.4 Training Strategy 采用如下的训练目标更新梯度：\n在训练过程中中固定BART的参数，而更新prefix的参数。在训练时使用来自源域的领域词作为prefix序列。当训练完成以后，保存domain-oriented的prefix模块的参数，而丢弃掉预训练好的BART模块的参数。 在测试的过程中，目标域的领域词则被MLP模块映射为prefix表示。\n4. Experimental Setup 4.1 Datasets 在两个multi-domain对话摘要数据集上评估了模型的效果。\n4.1.1 TODSum TODSum是基于经典对话数据集MultiWOZ提出的task-oriented对话摘要数据集。根据领域信息，数据集可以被划为5个领域：restaurant，hotel，attraction，taxi以及train. 在实验时，选择5个域中的4个域作为源域，剩下的域作为目标域，从源域中抽取200个样本作为验证集，源域的剩余数据作为训练集，目标域的数据作为测试集。\n4.1.2 QMSum QMSum数据集包括上千条会议录音数据，包括三个领域：academic，committee以及product. 采用类似于TODSum数据集的处理方式。\n4.2 Main Results 4.2.1 Results on TODSum 可以看到，Prefix-Tuning相比较BART，BART w. DS.,表现要差，是因为对话文本很长且复杂，仅使用fine-tuning参数的20%很难理解领域知识，以及识别对话中的关键内容。在与Prefix-tuning具有相同量级的参数下，DOP- tuning在5个领域都有了较大的提升。这表明由领域词初始化的prefix模块以及有对话状态组成的离散的prompts发挥了重要的作用。除此之外，模型比全参数fine-tuning的模型BART要好，说明模型可以有效地从源域和目标域中解藕出知识。上述结果表示，在有限的数据情况下，模型仍然可以达到SOTA的结果。\n4.2.2 Results on QMSum 整体表现的趋势与在TODSum数据集上的表现一致，但是可以看出在Rouges的分数相对而言较低，是因为领域并没有明显的领域词，导致了严重的领域耦合的问题。除此之外，由于会议文本过长，很难从对话中捕捉核心内容。总的来说，这些结果表明多领域设置对于会议摘要是非常必要且有意义的。\n5. Qualitative Analysis 5.1 Number of Domain Words 探究领域词数量的影响，可以看到领域词数量在140时使rouge达到了峰值，当低于140时，效果降低，说明参数量不足影响了模型的表现。当领域词数量超过阈值时，模型的表现下降，说明了太长的prefix序列给BART增加了负担，并且引入了额外的噪声。但是，领域词数量的变化并没有对模型的表现的有太大的影响（只有2～3%的起伏），说明了domain-oriented prefix模块和有效性和模型的鲁棒性。\n5.2 Quality of Domain Words 探究领域词的质量的影响，将领域词的中的一定比例的词语以与领域无关的词汇替代。可以看到，随着更多的噪声被引入，模型受到更大的影响且表现下降。当噪声的比例超过100%时，模型的表现甚至比Prefix- Tuning糟糕。这是因为，我们使用完全无关的词汇去初始化Prefix模块，相比较随机初始化引入了更多的噪声影响了DOP的表现。从这一点看，引入高质量的领域词有利于领域解藕，高质量的领域词对摘要生成是重要的。\n5.3 Ablation Study 研究了domain-oriented initialization和discrete prompts的影响。同时去掉两个模块与原始prefix-tuning相同。可以看到去除prefix- tuning中的domain- oriented初始化会使模型表现严重下降，说明domain word信息在面对新领域时引入相关知识的重要性。同时，移除离散的prompts也会使模型表现更糟糕（但仍然会好于Prefix-Tuning），说明离散的prompts能让模型更关注对话中核心的内容进而提升模型的表现。\n5.4 Effect of Prefix Module in Encoder and Decoder 由于DOP-method在encoder和decoder中均引入了prefix模块，研究两个部分的prefix模块对模型表现的影响。可以看到，当两个部分的prefix被移除后，模型的表现均下降，说明了两个模块的prefix都是必要且高效的。\n一个有趣的现象是移除encoder的prefix的影响要小于移除decoder的prefix的影响。一个比较合理的解释是在encoder和decoder端的prefix的作用是不一样的。在encoder端的prefix主要帮助模型理解对话，而decoder端的prefix主要帮助模型生成。因此，对于摘要生成，decoder端的prefix模块对模型更有用。\n5.5 Human Evaluation 对模型进行了人工评估。\n表中显示，所有模型的流畅程度都较高，说明在较强的backbone上微调的抽象摘要模型能够生成更流畅的句子。在事实一致性上，DOP以及BART ws DS好于Prefix-tuning的表现，说明对话状态信息能引导模型更关注与核心的信息，例如槽值和意图。初次之外，DOP-tuning在领域相关性上的表现超过了其他基线模型。说明了domain-oriented模块在提升模型识别领域相关特征以及从源域和目标域解藕出知识的能力。\n5.6 Effect of Training Data 5.6.1 Performance in Few-shot Settings 对于TODSum数据集，固定源域的数据规模，将目标域的数据加入训练数据。可以看到随着目标域数据的增加，BART w. DS和DOP的表现提升，但DOP-tuning始终好于BART w. DS. 说明目标域的知识增加可以让模型学到目标域的信息。\n5.6.2 Effect of Source Domain Data Size 保持zero-shot的设置不变，调整源域数据的规模\n可以看到随着数据规模的减小，BART w. DS的表现变差，而DOP-tuning能够相对优秀地保持稳定。说明DOP-tuning对数据规模不太敏感，并且具有一定程度的鲁棒性。这与主实验的结果一致，模型在有限和unseen data上表现优异。\n5.7 Prefix Length vs. Input Length 研究Prefix Length和input Length的关系，具体而言source input length，target input length以及对应的optimal prefix length的关系。可以得出更长的inputs可能更青睐于更短的prefix.\n6. Challenges 总结了抽象对话摘要的低资源领域迁移的挑战：\n Confusion between domains with high similarity 对于词汇表高度重合的领域，如restaurant和hotel，train和taxi，模型会产生domain-confusing句子。以hotel- restaurant对作为例子，当restaurant作为目标域，“book a restaurant room that can accommodate 3 people”会被生成，这样的句子其实更应该存在hotel领域中。但需要注意的是，这种challenge并不会影响关键因素的准确率，但language style则是不合适的。 Information dispersion 由于对话数据通常是长序列，因此模型很难对长对话中的所有方面都能pay attention，因此会产生对话中关键元素的注意力的偏差，尤其是在轻量和小参数训练的场景下。  7. Conclusion 在本文中，我们提出了基于高效且可泛化的微调方法面向领域的domain-oriented prefix-tuning模型解决对话摘要中的领域迁移的方法。使用领域词初始化的prefix模块能够从源域解藕出目标域的知识，而离散的prompts能够提升模型的泛化性。在zero-shot和few-shot下的实验说明我们的方法在两个数据集下取得了巨大的进步。\n","date":1650844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650844800,"objectID":"8b0c0ee82c75b5a39ba41fc75e80cd0f","permalink":"https://pris-nlp.github.io/en/post/220426zwh/","publishdate":"2022-04-25T00:00:00Z","relpermalink":"/en/post/220426zwh/","section":"post","summary":" ","tags":["Prompt Learning","文本生成"],"title":"DOP-Tuning: 面向对话摘要领域自适应的轻量级微调方法","type":"post"},{"authors":["---"],"categories":[],"content":"ADPL: Adversarial Prompt-based Domain Adaptation for Dialogue Summarization with Knowledge Disentanglement Authors  赵璐璐1， 郑馥嘉1， 曾伟豪， 何可清， 耿若彤，江会星，武威， 徐蔚然2\nConference  SIGIR 2022 Full Paper\nIntroduction 领域自适应是机器学习中的一个基本任务。在本文中，我们研究对话摘要任务中的领域迁移问题，试图借助源域的有标注数据迁移到无标注或少标注的目标域，进而提升低资源目标域下对话摘要的生成效果，可用于解决实际场景中小业务数据匮乏的挑战。传统的对话摘要领域迁移方法往往依赖于大规模领域语料，借助于预训练来学习领域间知识。该方法的缺点是实际语料收集难，对算力要求高，针对每一个目标域都需要进行耗时的预训练过程，效率低。因此，本文从微调的角度出发，提出了一种轻量级的解耦知识迁移方法ADPL，无需大规模的预训练过程，仅仅利用源域数据和少量的无标注目标域数据，即可实现高质量的对话摘要生成。具体来说，我们基于prompt learning的思想，针对对话摘要任务中的领域迁移问题，提出了三种特定的prompt结构：domain-invariant prompt (DIP), domain-specific prompt (DSP), 和task-oriented prompt (TOP)，其中 DIP 用来捕获领域间的共享特征，DSP用来建模领域特有知识，TOP用来促进生成流畅的摘要。在训练中，我们仅仅更新这些prompt相关的参数就可以实现领域间知识的解耦和迁移，相比较之前的预训练方法，训练高效环保，对机器的显存要求显著降低。同时，我们基于两个大规模的对话摘要数据集QMSum和TODSum构建了对话摘要领域迁移评测集，在两个评测集上取得了一致的最优效果，实验结果和消融分析都证明了本文提出方法的有效性。\nADPL整体框架示意图  Revisit Overconfidence for OOD Detection: Reassigned Contrastive Learning with Adaptive Class-dependent Threshold Authors  吴亚楠1， 何可清1， 严渊蒙， 高琪翔， 曾致远， 郑馥嘉， 赵璐璐，江会星，武威， 徐蔚然2\nConference  NAACL 2022 Main Conference, Long Paper\nIntroduction 在面向任务的对话系统中，域外意图（out-of-domain, OOD）检测是必不可少的。它旨在检测用户查询是否超出预定义意图范围（in-domain, IND），以避免执行错误的操作。由于OOD意图标注的复杂性，大多数工作都集中在无监督OOD检测上，即没有标记OOD数据，只有标记IND数据。而当前无监督域外检测方法均忽略了域外检测中的关键性挑战——神经网络的过自信问题。\n在本文中，我们对过自信问题进行了深入分析，并将其拆解为两方面：过自信OOD和过自信IND。基于此，我们分别提出了一种新的重分配对比学习(RCL)来区分语义相似的IND类别之间的意图表示，以缓解过自信OOD问题，以及一种自适应的类局部阈值机制来区分相似的IND和OOD样本，以缓解过自信IND问题。\n具体来说，对于过自信OOD问题，我们首先使用预训练的意图分类器在易混淆的IND类型中构建hard对比对（其中，具有相同真实标签但不同的预测标签称为hard positive pair，不同真实标签但相同预测标签称为hard negative pair），然后，基于构建的对比对训练一个新的模型，并通过监督对比学习来学习相似IND类别的鉴别性意图表示；对于过自信IND问题，不同于传统MSP、GDA使用单一全局阈值的方式，为考虑IND和OOD类别之间的关联性，我们为每个意图类别自适应赋予一个独立阈值，有效区分语义高度相似的IND和OOD样本，缓解过自信OOD问题。多个数据集上实验和分析证明了本文提出方法的有效性。\n Domain-Oriented Prefix-Tuning: Towards Efficient and Generalizable Fine-tuning for Zero-Shot Dialogue Summarization Authors  赵璐璐1， 郑馥嘉1， 曾伟豪， 何可清， 徐蔚然2，江会星，武威， 吴亚楠\nConference  NAACL 2022\nIntroduction 现实生活中经常面临到新领域中数据标注稀缺的问题，为新领域进行标注耗时耗力，因此利用有限的源域注释数据为目标域开发低资源对话摘要模型至关重要。当前的生成式对话摘要方法缺乏对新领域的泛化能力，而现有的在摘要领域自适应问题上的研究通常是依赖于大规模的二次预训练。\n为了探索对话摘要领域自适应的轻量级微调方法，在本文中，我们提出了一种高效且可泛化的面向领域的Prefix-tuning模型（Domain-Oriented Prefix-tuning，DOP），在冻结的预训练模型的基础上，结合连续的prefix和离散的prompt表示，提高模型的领域自适应能力。具体来说，我们使用无监督 LDA提取的领域词来初始化连续提示向量，以获得前缀模块的初始参数和表示。我们还添加了一个面向领域的键值对前缀序列来增强经典注意力层，以交互方式获取知识并实现优化。除此之外，我们使用dialogue state和query作为离散prompt，引导模型关注对话中关键内容并增强对新领域的泛化能力。\n我们在两个多域对话摘要数据集 TODSum 和 QMSum 上进行零样本迁移实验并建立领域自适应benchmark， 充分的实验和定性分析证明了我们方法的有效性。\n Disentangled Knowledge Transfer for OOD Intent Discovery with Unifified Contrastive Learning Authors  牟宇滔1， 何可清1， 吴亚楠1， 曾致远， 徐红，江会星，武威， 徐蔚然2\nConference  ACL 2022\nIntroduction 本文研究域外(OOD)意图发现任务，该任务旨在将新出现的未知意图的样本按意图语义聚成不同类簇，这有助于任务型对话系统发展新的技能。不同于传统的文本聚类任务，域外意图发现需要考虑如何利用已知的域内(IND)意图类别的先验知识，帮助新意图的聚类。因此相关方法都遵循一个两阶段框架：IND预训练和OOD聚类。其中的关键挑战在于如何将IND先验知识迁移到OOD聚类上。之前的方法普遍存在一个问题，将IND预训练过程当作分类任务，采用一个交叉熵分类损失，模型学习到如何分类IND样本，但是下游我们需要对OOD聚类。两个阶段不同的学习目标使得存在一个天然的语义鸿沟，使得IND到OOD的知识迁移变得困难。此外，我们观察到之前的方法只迁移BERT输出的一个共享意图表征，考虑到表征具有高度耦合性，这样一个表征可能不利于OOD聚类。例如，在IND预训练的编码器中存在实例级(instance-level)和聚类级(class-level)的知识，解耦不同级别的知识有助于更好地知识迁移。为了解决这样的问题，我们建立了一个统一的多头对比学习框架，并在此基础上提出了一个新颖的解耦知识迁移方法(DKT)，以便迁移解耦IND意图知识用于OOD聚类。我们意在弥合IND预训练和OOD聚类两个阶段的语义鸿沟。两个基准数据集的实验和分析显示了我们方法的有效性。\n  Equal Contribution\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Corresponding Author\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1649376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649376000,"objectID":"ab892d00f3720d6a844743fb5e1358ee","permalink":"https://pris-nlp.github.io/en/post/220408wzc/","publishdate":"2022-04-08T00:00:00Z","relpermalink":"/en/post/220408wzc/","section":"post","summary":" ","tags":null,"title":"Recent research overview（2022.04.08）","type":"post"},{"authors":["Zechen Wang"],"categories":[],"content":"本文是一片关于NLP模型鲁棒性的综述，统一地介绍了如何定义、衡量和提升NLP模型的鲁棒性。\nNLP模型鲁棒性的定义 模型在$(x, y) \\sim \\mathcal{D}$上训练，在$(x', y')\\sim \\mathcal{D'} \\ne \\mathcal{D}$ 上测试，可以用模型在$\\mathcal{D'}$上的性能（如准确率）衡量模型的鲁棒性。\n可以粗略地将现有的模型鲁棒性的文献分为两类：$\\mathcal{D'}$是对输入合成扰动，或者 $\\mathcal{D'}$是自然发生的分布转移。\n对对抗攻击的鲁棒性 对抗攻击是指故意精心制造噪声来欺骗模型做出错误的预测，之前在CV领域被广泛探索， 后来扩展到NLP领域。对抗样本的生成主要建立在这样一个观察之上，即我们可以生成对人类有意义的样本(例如，通过用人类察觉不到的变化干扰样本)，同时改变对该样本的模型预测。 对抗攻击主要建立在人类可以理解大量同义词或者忽略字母的确切顺序，但机器不能。\n现在大多数CV研究都做了一个相对简单的假设，即在$x$上加有界摄动得到的$x'$的金标应该保持不变， 即$y'=y$，模型的鲁棒行为应该是$f(x')=f(x)$。其中的摄动可以是token和字符的交换，释义， 不改变语义的对抗规则，或者添加干扰因素。\n然而，这个标签不变的假设可能并不总是成立的，有人研究了几种现有的文本扰动技术，但发现 一大部分扰动样本都改变了标签（尽管是在保留标签的假设下）或者结果的标签在人类标注者中 存在高度分歧。\n还有一个相似的概念是语义保留，是指$(x, x')$之间的语义是不变的，而上面的标签保留是指 $(y, y')$是不变的。\n对分布转移的鲁棒性 现有的鲁棒性定义更接近域泛化或OOD泛化的概念，其中测试集（不管有无标签）都是在训练的时候 不可获取的。 在NLP背景下，对自然分布转移的鲁棒性也意味着模型的性能不会因为语法错误、方言、说话者和 语言的差异或者为同一任务不同领域新收集的数据集而降低。\n另一个密切相关的研究方向是公平性，例如，在指代消解、职业分类、神经机器翻译等任务中观察到了性别刻板印象或偏差。\n联系和共同主题 合成 上述两个分类都可以归为一个框架中，即$\\mathcal{D'}$代表合成分布转移（通过对抗攻击）或自然分布转移。 两者的联系仍有待探索。在CV领域，有研究表面对合成分布转移的鲁棒性可能对自然分布转移鲁棒性贡献 很少甚至没有贡献。\n为了更好地理解模型为什么缺乏鲁棒性，一些现有的工作认为这是因为模型有时利用虚假的特征和标签之间的 关系，而非真正的关系。其中虚假的特征通常定义为并不真正影响任务标签的特征。它们和任务标签有联系，但是不能转移到更加有挑战性的测试条件或OOD数据。一些其他的工作将其定义为适用于大多数情况但不适用于一般情况的规则。这些虚假的关系有时被称为数据集偏差或群体转移。此外，有证据表明，控制模型在虚假特征下的学习将提高模型在分布转移中的性能。还有学者讨论了对抗鲁棒性和伪特征学习之间的联系。还有人通过将模型在分布转移或对抗性攻击中缺乏鲁棒性的原因归因于模型对虚假特征的学习，提出了连接这些领域的理论性的讨论。\n此外，在某些应用中，模型的鲁棒性也可以与模型的不稳定性或者模型的不确定度估计很差联系起来。 对于此，有人提出了贝叶斯方法、基于dropout和基于组装的实现方法。最近，Ovadia等人已经证明，模型的不确定性估计可能在分配转移下显著降低，并呼吁应当通过对OOD数据给出更低的不确定性估计，确保 模型“知道它什么时候不知道”。\n识别非鲁棒 随着鲁棒性在自然语言处理文献中得到越来越多的关注，各行各业都提出了识别自然语言处理模型鲁棒性失效的方法。现有工作可以根据故障的识别方式大致分类，其中很大一部分工作依赖于人类先验和对现有NLP模型的错误分析，其他一些工作线采用基于模型的方法。为了更准确地度量自然语言处理模型的鲁棒性，通常将识别出的鲁棒性失效模式组织成具有挑战性/对抗性的基准数据集。表1展示了用于识别模型鲁棒性失效的常用扰动类型（数据集），在表2中，我们总结了每个NLP任务的常见鲁棒性基准（benchmark）。\n人类先验和错误分析驱动的 按任务分，包括NLI（Natural Language Inference），QA和神经机器翻译。\nNLI Naik等人抽样错误分类的例子，并分析其潜在的错误来源，然后将其归类为常见错误原因的类型。这些错误类型将作为构建压力测试集的基础，以进一步评估NLI模型是否具有做出真实推理决策的能力，还是仅仅依赖于复杂的模式匹配。Gururangan等人发现，目前的NLI模型很可能仅依靠假设来识别标签，而Poliak等人提供了类似的补充，即我们采用仅假设的模型可以优于一组强基线。\nQA 有人提出通过在段落末尾串联一个敌对的干扰句来生成敌对的QA例子。Miller等人为斯坦福问答数据集(SQuAD)构建了四个新的测试集，并发现大多数问答系统未能推广到这一新的数据。他们也希望对自然分布转移进行新的评估指标。\n机器翻译 Belinkov和Bisk发现，当面对嘈杂数据时，基于字符的神经机器翻译(NMT)模型是脆弱的，很容易不稳定，其中噪音(例如，打字、拼写错误等)是使用可能的词汇替换合成的。使用包含人工引入语法错误的句子或随机合成噪音的增强训练数据可以使系统对这种虚假模式更加稳健。另一方面，有人展示了另一种方法，通过限制字符的输入空间，使模型有可能感知数据输入错误和拼写错误。\n基于模型的 这种方法有的是任务不确定的，有的是输入不可知的。 这种方法通过训练一个额外的模型来捕获偏差。例如，在视觉问题回答中，Clark等人训练一个朴素模型来预测基于问题的原型答案，而不考虑上下文;Utama等人提出学习一个仅使用数据集偏差相关特征的有偏模型。此外，Culotta的目标是通过训练分类器来识别模型中的捷径，从而从人类标注的例子中更好地区分虚假的相关性和真实的相关性。\n提升模型鲁棒性 根据人工干预的位置和方式，这些方法可以分为数据驱动的、基于模型和训练方案的、基于归纳先验和最后的因果干预。\n数据驱动（数据增强） 如Mixup, MixText, CutOut, AugMix, HiddenCut。这类缓解方法是在数据层面上操作的，往往很难解释如何以及为什么起作用。\n基于模型和训练策略 预训练 最近的研究表明，预训练是提高NLP模型非分布鲁棒性的有效方法，这可能是因为其自我监督的目标，以及使用了大量不同的训练前数据，这些数据鼓励从少量的测试样本中归纳出一般化的结果，从而抵消了虚假的相关性。有研究显示一些其他因素也可以促进稳健的准确性，包括更大的模型尺寸、更多的微调数据和更长的微调。Taori等人在视觉领域也进行了类似的观察，其中作者发现，与现有文献提出的各种鲁棒性干预相比，使用更大、更多样化的数据集的训练在多个情况下提供了更好的鲁棒性。\n更好地利用少数群体的样本训练 还有一些工作建议通过更好地使用少数例子来简化模型，例如，在训练分布中代表性不足的例子，或更难学习的例子。例如，Yaghoobzadeh等人提出，首先根据全部数据对模型进行微调，然后只对少数例子进行微调。一般来说，强调样本子集的训练策略对模型来说特别难学习，有时也被称为DRO（distributional robust optimization，分布式鲁棒优化）组。DRO的扩展主要是讨论如何识别被认为是少数样本。例如，Nam等人通过强调模型的早期决策来训练另一个模型;Lahoti等人也使用另一个模型来识别对主模型具有挑战性的样本;Liu等人提出通过对在第一次训练时损失较大的少数例子增加权重，对模型进行第二次训练。\n基于归纳的实现方法 另一个思路是引入归纳偏差(即正则化假设空间)，迫使模型丢弃一些虚假的特征。这与基于人先验的识别方法密切相关，因为这些人的先验知识通常可以用额外的正则化器来重新制定训练目标。为了实现这一目标，通常需要首先构造一个侧组件来通知主模型有偏差的特征，然后根据侧组件来正则化主模型。这个侧组件的构造通常依赖于失调特征是什么的先验知识。然后，可以建立相应的方法来应对特征。类似地，Clark等人提出用一个显式捕获偏差的模型进行集成，其中主模型与这个仅偏差（bias-only）模型一起训练，这样主模型就不被鼓励使用偏差。最近的工作表明，通过更好地校准仅偏差模型，基于集成的方法可以进一步改进。\n总之，这种方法大概是在不同的领域/分布中训练小的经验损失，以迫使模型对特定领域的虚假特征不变。\n干预因果关系 Srivastava利用人类对因果关系的常识知识，以潜在的未测量变量来增加训练样本，并提出了一种基于DRO的方法，以使模型对分布转移具有鲁棒性。Veitch等人提出学习依赖于数据因果结构的估计反事实不变量预测，并表明它可以帮助减少文本分类中的虚假相关性。\n提升鲁棒性策略的联系 大体可分为3类：\n 利用大量数据预训练模型 学习跨领域/环境的不变表示或预测 基于具体的虚假/偏见模式的数据  有趣的是，统计研究表明，许多缓解方法都有相同的鲁棒机器学习泛化误差界。\n","date":1640563200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640574000,"objectID":"bdef4b81a64b9fec2dcb1be53c6e3d4c","permalink":"https://pris-nlp.github.io/en/post/211227wzc/","publishdate":"2021-12-27T00:00:00Z","relpermalink":"/en/post/211227wzc/","section":"post","summary":" ","tags":null,"title":"Measure and Improve Robustness in NLP Models: A Survey","type":"post"},{"authors":["Yuejie Lei","Fujia Zheng","Yuanmeng Yan","Keqing He","Weiran Xu"],"categories":[],"content":"","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"5a9fadb5279182c5f605d4bb596c178e","permalink":"https://pris-nlp.github.io/en/publication/a-finer-grain-universal-dialogue-semantic-structures-based-model-for-abstractive-dialogue-summarization/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/a-finer-grain-universal-dialogue-semantic-structures-based-model-for-abstractive-dialogue-summarization/","section":"publication","summary":"Although abstractive summarization models have achieved impressive results on document summarization tasks, their performance on dialogue modeling is much less satisfactory due to the crude and straight methods for dialogue encoding. To address this question, we pro pose a novel end-to-end Transformer-based model FinDS for abstractive dialogue summarization that leverages Finer-grain universal Dialogue semantic Structures to model dialogue and generates better summaries. Experiments on the SAMsum dataset show that FinDS outperforms various dialogue summarization approaches and achieves new state of-the-art (SOTA) ROUGE results. Finally, we apply FinDS to a more complex scenario, showing the robustness of our model. We also release our source code.","tags":["\"Dialogue Summarization\""],"title":"A Finer-grain Universal Dialogue Semantic Structures based Model For Abstractive Dialogue Summarization","type":"publication"},{"authors":["Liwen Wang","Xuefeng Li","JiachiLiu","Keqing He","Yuanmeng Yan","Weiran Xu"],"categories":[],"content":"","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"385842321aef2a5899368efefd05b945","permalink":"https://pris-nlp.github.io/en/publication/bridge-to-target-domain-by-prototypical-contrastive-learning-and-label-confusion-re-explore-zero-shot-learning-for-slot-filling/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/bridge-to-target-domain-by-prototypical-contrastive-learning-and-label-confusion-re-explore-zero-shot-learning-for-slot-filling/","section":"publication","summary":"Zero-shot cross-domain slot filling alleviates the data dependence in the case of data scarcity in the target domain, which has aroused extensive research. However, as most of the existing methods do not achieve effective knowledge transfer to the target domain, they just fit the distribution of the seen slot and show poor performance on unseen slot in the target domain. To solve this, we propose a novel approach based on prototypical contrastive learning with a dynamic label confusion strategy for zero-shot slot filling. The prototypical contrastive learning aims to reconstruct the semantic constraints of labels, and we introduce the label confusion strategy to establish the label dependence between the source domains and the target domain on-the-fly. Experimental results show that our model achieves significant improvement on the unseen slots, while also set new state-of-the-arts on slot filling task.","tags":["\"Slot filling\""],"title":"Bridge to Target Domain by Prototypical Contrastive Learning and Label Confusion:Re-explore Zero-Shot Learning for Slot Filling","type":"publication"},{"authors":["Lulu Zhao","Weihao Zeng","Weiran Xu","郭军"],"categories":[],"content":"","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"531badd442f99f521abe7590685d8731","permalink":"https://pris-nlp.github.io/en/publication/give-the-truth-incorporate-semantic-slot-into-abstractive-dialogue-summarization/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/give-the-truth-incorporate-semantic-slot-into-abstractive-dialogue-summarization/","section":"publication","summary":"Abstractive dialogue summarization suffers from a lots of factual errors, which are due to scattered salient elements in the multi-speaker information interaction process. In this work, we design a heterogeneous semantic slot graph with a slot-level mask cross-attention to enhance the slot features for more correct summarization. We also propose a slot-driven beam search algorithm in the decoding process to give priority to generating salient elements in a limited length by “filling-in-the-blanks”. Besides, an adversarial contrastive learning assisting the training process is introduced to improve the generalization of our model. Experimental performance on different types of factual errors shows the effectiveness of our methods and human evaluation further verifies the results.","tags":["\"Dialogue Summarization\""],"title":"Give the Truth:Incorporate Semantic Slot into Abstractive Dialogue Summarization","type":"publication"},{"authors":["Zhiyuan Zeng","JiazeChen","Weiran Xu","LeiLi"],"categories":[],"content":"","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"012f1047329d4dc68a563f4b8fb07aa2","permalink":"https://pris-nlp.github.io/en/publication/gradient-based-adversarial-factual-consistency-evaluation-for-abstractive-summarization/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/gradient-based-adversarial-factual-consistency-evaluation-for-abstractive-summarization/","section":"publication","summary":"Neural abstractive summarization systems have gained significant progress in recent years. However, excessive abstractiveness inevitably leads to factual errors, which poses a challenge to the robustness of the systems. In this paper, we proposed an efficient weak-supervised adversarial data augmentation approach to form the factual consistency dataset. Based on the artificial dataset, we train an evaluation model that can not only make accurate and robust factual consistency discrimination but is also capable of making interpretable factual errors tracing by backpropagated gradient distribution on token embeddings. Experiments and analysis conduct on public annotated summarization and factual consistency datasets demonstrate our approach effective and reasonable.","tags":["\"Factual Consistency Evaluation\""],"title":"Gradient-Based Adversarial Factual Consistency Evaluation for Abstractive Summarization","type":"publication"},{"authors":["Yuanmeng Yan","RumeiLi","SiruiWang","HongzhiZhang","ZanDaoguang","FuzhengZhang","WeiWu","Weiran Xu"],"categories":[],"content":"","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"19f4ec05789a769e39f069d1d9dc707c","permalink":"https://pris-nlp.github.io/en/publication/large-scale-relation-learning-for-question-answering-over-knowledge-bases-with-pre-trained-language-models/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/large-scale-relation-learning-for-question-answering-over-knowledge-bases-with-pre-trained-language-models/","section":"publication","summary":"The key challenge of question answering over knowledge bases (KBQA) is the inconsistency between the natural language questions and the reasoning paths in the knowledge base (KB). Recent graph-based KBQA methods are good at grasping the topological structure of the graph but often ignore the textual information carried by the nodes and edges. Meanwhile, pre-trained language models learn massive open-world knowledge from the large corpus, but it is in the natural language form and not structured. To bridge the gap between the natural language and the structured KB, we propose three relation learning tasks for BERT-based KBQA, including relation extraction, relation matching, and relation reasoning. By relation-augmented training, the model learns to align the natural language expressions to the relations in the KB as well as reason over the missing connections in the KB. Experiments on WebQSP show that our method consistently outperforms other baselines, especially when the KB is incomplete.","tags":["\"KBQA\""],"title":"Large-Scale Relation Learning for Question Answering over Knowledge Bases with Pre-trained Language Models","type":"publication"},{"authors":["Yuanmeng Yan","RumeiLi","SiruiWang","FuzhengZhang","WeiWu","Weiran Xu"],"categories":[],"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"d17ed1832a6b7f76a113da7a17518417","permalink":"https://pris-nlp.github.io/en/publication/consert-a-contrastive-framework-for-self-supervised-sentence-representation-transfer/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/consert-a-contrastive-framework-for-self-supervised-sentence-representation-transfer/","section":"publication","summary":"Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pre-trained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised SEntence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8\\% relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new state-of-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.","tags":["\"Semantic Textual Similarity\""],"title":"ConSERT:A Contrastive Framework for Self-Supervised Sentence Representation Transfer","type":"publication"},{"authors":["Zhiyuan Zeng","Keqing He","Yuanmeng Yan","Zijun Liu","Yanan Wu","Hong Xu","HuixingJiang","Weiran Xu"],"categories":[],"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"f3ebe8e02e929c931daa39b8b5a87907","permalink":"https://pris-nlp.github.io/en/publication/modeling-discriminative-representations-for-out-of-domain/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/modeling-discriminative-representations-for-out-of-domain/","section":"publication","summary":"Detecting Out-of-Domain (OOD) or unknown intents from user queries is essential in a task oriented dialog system. A key challenge of OOD detection is to learn discriminative se mantic features. Traditional cross-entropy loss only focuses on whether a sample is correctly classified, and does not explicitly distinguish the margins between categories. In this pa per, we propose a supervised contrastive learn ing objective to minimize intra-class variance by pulling together in-domain intents belong ing to the same class and maximize inter-class variance by pushing apart samples from differ ent classes. Besides, we employ an adversar ial augmentation mechanism to obtain pseudo diverse views of a sample in the latent space. Experiments on two public datasets prove the effectiveness of our method capturing discrim inative representations for OOD detection.","tags":["\"OOD\"","\"intent detection\""],"title":"Modeling Discriminative Representations for Out-of-Domain Detection with Supervised Contrastive Learning","type":"publication"},{"authors":["Yanan Wu","Zhiyuan Zeng","Keqing He","Hong Xu","Yuanmeng Yan","HuixingJiang","Weiran Xu"],"categories":[],"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"b4a06f42b461c513bbb4e5bf80a038dd","permalink":"https://pris-nlp.github.io/en/publication/novel-slot-detection-a-benchmark-for-discovering-unknown-slot-types/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/novel-slot-detection-a-benchmark-for-discovering-unknown-slot-types/","section":"publication","summary":"Existing slot filling models can only recognize pre-defined in-domain slot types from a limited slot set. In the practical application, a reliable dialogue system should know what it does not know. In this paper, we introduce a new task, Novel Slot Detection (NSD), in the task-oriented dialogue system. NSD aims to discover unknown or out-of-domain slot types to strengthen the capability of a dialogue system based on in-domain training data. Besides, we construct two public NSD datasets propose several strong NSD baselines, and establish a benchmark for future work. Finally, we conduct exhaustive experiments and qualitative analysis to comprehend key challenges and provide new guidance for future directions","tags":["\"OOD\"","\"intent detection\""],"title":"Novel Slot Detection:A Benchmark for Discovering Unknown Slot Types in the Task-Oriented Dialogue System","type":"publication"},{"authors":["Sihong Liu","JinchaoZhang","Keqing He","Weiran Xu","JieZhou"],"categories":[],"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"25a89a2bccac6ffea3953c05b415c1d6","permalink":"https://pris-nlp.github.io/en/publication/scheduled-dialog-policy-learning-an-automatic-curriculum-learning-framework-for-task-oriented-dialog-system/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/scheduled-dialog-policy-learning-an-automatic-curriculum-learning-framework-for-task-oriented-dialog-system/","section":"publication","summary":"","tags":["\"dialog management\""],"title":"Scheduled Dialog Policy Learning:An Automatic Curriculum Learning Framework for Task-oriented Dialog System","type":"publication"},{"authors":["Zhiyuan Zeng","Hong Xu","Keqing He","Yuanmeng Yan","Sihong Liu","Zijun Liu","Weiran Xu"],"categories":[],"content":"","date":1622937600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"c53d36414582d8c3d75e9bf70cabc618","permalink":"https://pris-nlp.github.io/en/publication/adversarial-generative-distance-based-classifier-for-robust-out-of-domain-detection/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/adversarial-generative-distance-based-classifier-for-robust-out-of-domain-detection/","section":"publication","summary":"Detecting out-of-domain (OOD) intents is critical in a task-oriented dialog system. Existing methods rely heavily on extensive manually labeled OOD samples and lack robustness. In this paper, we propose an efficient adversarial attack mechanism to augment hard OOD samples and design a novel generative distance-based classifier to detect OOD samples instead of a traditional threshold-based discriminator classifier. Experiments on two public benchmark datasets show that our method can consistently outperform the baselines with a statistically significant margin.","tags":["\"OOD\"","\"intent detection\""],"title":"Adversarial Generative Distance-Based Classifier for Robust Out-of-Domain Detection","type":"publication"},{"authors":["Zhiyuan Zeng","Keqing He","Yuanmeng Yan","Hong Xu","Weiran Xu"],"categories":[],"content":"","date":1622937600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"482f1bd9d93aed37194f0973986c3c4d","permalink":"https://pris-nlp.github.io/en/publication/adversarial-self-supervised-learning-for-out-of-domain-detection/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/adversarial-self-supervised-learning-for-out-of-domain-detection/","section":"publication","summary":"Detecting out-of-domain (OOD) intents is crucial for the deployed task-oriented dialogue system. Previous unsupervised OOD detection methods only extract discriminative features of different in-domain intents while supervised counterparts can directly distinguish OOD and in-domain intents but require ex tensive labeled OOD data. To combine the benefits of both types, we propose a self-supervised contrastive learning framework to model discriminative semantic features of both in-domain intents and OOD intents from unlabeled data. Besides, we introduce an adversarial augmentation neural module to improve the efficiency and robustness of contrastive learning. Experiments on two public benchmark datasets show that our method can consistently outperform the baselines with a statistically significant margin.","tags":["\"OOD\"","\"intent detection\""],"title":"Adversarial Self-Supervised Learning for Out-of-Domain Detection","type":"publication"},{"authors":["Liwen Wang","Yuanmeng Yan","Keqing He","Yanan Wu","Weiran Xu"],"categories":[],"content":"","date":1622937600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622937600,"objectID":"b4f0a702fb1f1f8042fe315a418ee6c1","permalink":"https://pris-nlp.github.io/en/publication/dynamically-disentangling-social-bias-from-task-oriented-representations-with-adversarial-attack/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/dynamically-disentangling-social-bias-from-task-oriented-representations-with-adversarial-attack/","section":"publication","summary":"Representation learning is widely used in NLP for a vast range of tasks. However, representations derived from text corpora often reflect social biases. This phenomenon is pervasive and consistent across different neural models, causing serious concern. Previous methods mostly rely on a pre-specified, user-provided direction or suffer from unstable training. In this paper, we propose an adversarial disentangled debiasing model to dynamically decouple social bias attributes from the intermediate representations trained on the main task. We aim to denoise bias information while training on the downstream task, rather than completely remove social bias and pursue static unbiased representations. Experiments show the effectiveness of our method, both on the effect of debiasing and the main task performance.","tags":["\"Social Bias\"","\"Debias\""],"title":"Dynamically Disentangling Social Bias from Task-Oriented Representations with Adversarial Attack","type":"publication"},{"authors":["Yuejie Lei","Yuanmeng Yan","Zhiyuan Zeng","Keqing He","XimingZhang","Weiran Xu"],"categories":[],"content":"","date":1622937600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"24c52aa27079cad3f87ff08feb16ffcf","permalink":"https://pris-nlp.github.io/en/publication/hierarchical-speaker-aware-sequence-to-sequence-model-for-dialogue-summarization/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/hierarchical-speaker-aware-sequence-to-sequence-model-for-dialogue-summarization/","section":"publication","summary":"","tags":["\"Dialogue Summarization\""],"title":"Hierarchical Speaker-aware Sequence-to-sequence Model for Dialogue Summarization","type":"publication"},{"authors":["GuangChen","XintongGu","SiLi","YajingXu","Weiran Xu"],"categories":[],"content":"","date":1620432000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620432000,"objectID":"dff53f196c9fa8e89f6523d449108d7a","permalink":"https://pris-nlp.github.io/en/patent/%E5%9F%BA%E4%BA%8E%E7%96%91%E9%97%AE%E8%AF%8D%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%97%AE%E9%A2%98%E7%94%9F%E6%88%90%E6%96%B9%E6%B3%95%E5%8F%8A%E7%94%9F%E6%88%90%E7%B3%BB%E7%BB%9F/","publishdate":"2021-05-08T00:00:00Z","relpermalink":"/en/patent/%E5%9F%BA%E4%BA%8E%E7%96%91%E9%97%AE%E8%AF%8D%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%97%AE%E9%A2%98%E7%94%9F%E6%88%90%E6%96%B9%E6%B3%95%E5%8F%8A%E7%94%9F%E6%88%90%E7%B3%BB%E7%BB%9F/","section":"patent","summary":"","tags":[],"title":"基于疑问词分类器的神经网络问题生成方法及生成系统","type":"patent"},{"authors":["Lulu Zhao","Weiran Xu","ShengGao","JunGuo"],"categories":[],"content":"","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"c0c28cf3e1a16d838b506f7e58e34c35","permalink":"https://pris-nlp.github.io/en/publication/utilizing-graph-neural-networks-to-improving-dialogue-based-relation-extraction/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/utilizing-graph-neural-networks-to-improving-dialogue-based-relation-extraction/","section":"publication","summary":"Relation extraction has been an active research interest in the field of Natural Language Processing (NLP). The past works primarily focused on a corpus of formal text which is inherently non-dialogic. Recently, the dialogue-based relation extraction task, which detects relations among speaker-aware entities scattering in dialogues, has been gradually arousing people’s attention. Some sequence-based neural methods have been carried out to obtain the relevant information. However, identifying cross-sentence relations remains unsolved, especially in the context of a specific-domain dialogue system. In this paper, we propose a Relational Attention Enhanced Graph Convolutional Network (RAEGCN), which constructs the whole dialogue as a semantic interactive graph by emphasizing the speaker-related information and leveraging various inter-sentence dependencies. A dense connectivity mechanism is also introduced to empower the multi-hop relational reasoning across sentences, which can capture both local and non-local features simultaneously. Experiments show the significant superiority and robustness of our model on a real-world dataset DialogRE, as compared with previous approaches.","tags":["\"Dialogue Relation Extraction \""],"title":"Utilizing Graph Neural Networks to Improving Dialogue-based Relation Extraction","type":"publication"},{"authors":["Keqing He","Yuanmeng Yan","Weiran Xu"],"categories":[],"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"370fcbc724df4459ab28ea34406f0218","permalink":"https://pris-nlp.github.io/en/publication/from-context-aware-to-knowledge-aware-boosting-oov-tokens-recognition-in-slot-tagging-with-background-knowledge/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/from-context-aware-to-knowledge-aware-boosting-oov-tokens-recognition-in-slot-tagging-with-background-knowledge/","section":"publication","summary":"Neural-based context-aware models for slot tagging tasks in language understanding have achieved state-of-the-art performance, especially deep contextualized models, such as ELMo, BERT. However, the presence of out-of-vocab (OOV) words significantly degrades the performance of neural-based models, especially in a few-shot scenario. In this paper, we propose a novel knowledge-aware slot tagging model to integrate contextual representation of input text and the large-scale lexical background knowledge. Besides, we use multi-level graph attention to explicitly reason via lexical relations. We aim to leverage both linguistic regularities covered by deep language models (LM) and high-quality background knowledge derived from curated knowledge bases (KB). Consequently, our model could infer rare and unseen words in the test dataset by incorporating contextual semantics learned from the training dataset and lexical relations from ontology. The experiments show that our proposed knowledge integration mechanism achieves consistent improvements across settings with different sizes of training data on two public benchmark datasets. We also show through detailed analysis that incorporating background knowledge effectively alleviates issues of data scarcity.","tags":["\"Slot taggingContextual\"","\"representationBackground\"","\"knowledgeKnowledge\"","\"IntegrationMulti-level\"","\"graph attention\""],"title":"From context-aware:to knowledge-aware Boosting OOV tokens recognition in slot tagging with background knowledge","type":"publication"},{"authors":["Keqing He","JinchaoZhang","Yuanmeng Yan","Weiran Xu","ChengNiu","JieZhou"],"categories":[],"content":"","date":1607385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"d2894e19605efc452a5ff05186792a64","permalink":"https://pris-nlp.github.io/en/publication/contrastive-zero-shot-learning-for-cross-domain-slot-filling-with-adversarial-attack/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/contrastive-zero-shot-learning-for-cross-domain-slot-filling-with-adversarial-attack/","section":"publication","summary":"Zero-shot slot filling has widely arisen to cope with data scarcity in target domains. However, previous approaches often ignore constraints between slot value representation and related slot description representation in the latent space and lack enough model robustness. In this paper, we propose a Contrastive Zero-Shot Learning with Adversarial Attack (CZSL-Adv) method for the cross-domain slot filling. The contrastive loss aims to map slot value contextual representations to the corresponding slot description representations. And we introduce an adversarial attack training strategy to improve model robustness. Experimental results show that our model significantly outperforms state-of-the-art baselines under both zero-shot and few-shot settings.","tags":["\"Slot Filling\""],"title":"Contrastive Zero-Shot Learning for Cross-Domain Slot Filling with Adversarial Attack","type":"publication"},{"authors":["Lulu Zhao","Weiran Xu","JunGuo"],"categories":[],"content":"","date":1607385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"d1f0e2d750144bca0c36c4654540daef","permalink":"https://pris-nlp.github.io/en/publication/improving-abstractive-dialogue-summarization-with-graph-structures-and-topic-words/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/improving-abstractive-dialogue-summarization-with-graph-structures-and-topic-words/","section":"publication","summary":"Recently, people have been beginning paying more attention to the abstractive dialogue summarization task. Since the information flows are exchanged between at least two interlocutors and key elements about a certain event are often spanned across multiple utterances, it is necessary for researchers to explore the inherent relations and structures of dialogue contents. However, the existing approaches often process the dialogue with sequence-based models, which are hard to capture long-distance inter-sentence relations. In this paper, we propose a Topic-word Guided Dialogue Graph Attention (TGDGA) network to model the dialogue as an interaction graph according to the topic word information. A masked graph self-attention mechanism is used to integrate cross-sentence information flows and focus more on the related utterances, which makes it better to understand the dialogue. Moreover, the topic word features are introduced to assist the decoding process. We evaluate our model on the SAMSum Corpus and Automobile Master Corpus. The experimental results show that our method outperforms most of the baselines.","tags":["\"Dialogue Summarization\""],"title":"Improving Abstractive Dialogue Summarization with Graph Structures and Topic Words","type":"publication"},{"authors":["Hong Xu","Keqing He","Yuanmeng Yan","Sihong Liu","Zijun Liu","Weiran Xu"],"categories":[],"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"c6801eecfd9d1668cc7b342e6d71f923","permalink":"https://pris-nlp.github.io/en/publication/a-deep-generative-distance-based-classifier-for-out-of-domain-detection-with-mahalanobis-space/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/a-deep-generative-distance-based-classifier-for-out-of-domain-detection-with-mahalanobis-space/","section":"publication","summary":"Detecting out-of-domain (OOD) input intents is critical in the task-oriented dialog system. Different from most existing methods that rely heavily on manually labeled OOD samples, we focus on the unsupervised OOD detection scenario where there are no labeled OOD samples except for labeled in-domain data. In this paper, we propose a simple but strong generative distance based classifier to detect OOD samples. We estimate the class-conditional distribution on feature spaces of DNNs via Gaussian discriminant analysis (GDA) to avoid over-confidence problems. And we use two distance functions, Euclidean and Mahalanobis distances, to measure the confidence score of whether a test sample belongs to OOD. Experiments on four benchmark datasets show that our method can consistently outperform the baselines.","tags":["\"OOD\"","\"intent detection\""],"title":"A Deep Generative Distance-Based Classifier for Out-of-Domain Detection with Mahalanobis Space","type":"publication"},{"authors":["Yuanmeng Yan","Keqing He","Hong Xu","Sihong Liu","FanyuMeng","MinHu","Weiran Xu"],"categories":[],"content":"","date":1605484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"c75348ca95a9866f3643a784a745dc4a","permalink":"https://pris-nlp.github.io/en/publication/adversarial-semantic-decoupling-for-recognizing-open-vocabulary-slots/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/adversarial-semantic-decoupling-for-recognizing-open-vocabulary-slots/","section":"publication","summary":"Open-vocabulary slots, such as file name, album name, or schedule title, significantly degrade the performance of neural-based slot filling models since these slots can take on values from a virtually unlimited set and have no semantic restriction nor a length limit. In this paper, we propose a robust adversarial model-agnostic slot filling method that explicitly decouples local semantics inherent in open-vocabulary slot words from the global context. We aim to depart entangled contextual semantics and focus more on the holistic context at the level of the whole sentence. Experiments on two public datasets show that our method consistently outperforms other methods with a statistically significant margin on all the open-vocabulary slots without deteriorating the performance of normal slots.","tags":["\"Slot Filling\""],"title":"Adversarial Semantic Decoupling for Recognizing Open-Vocabulary Slots","type":"publication"},{"authors":["YuanyuanQi","JiayueZhang","YansongLiu","Weiran Xu","JunGuo"],"categories":[],"content":"","date":1603065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"3a052120ec6fb7d2864d896b363f3f70","permalink":"https://pris-nlp.github.io/en/publication/cgtr-convolution-graph-topology-representation-for-document-ranking/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/cgtr-convolution-graph-topology-representation-for-document-ranking/","section":"publication","summary":"Contextualized neural language models have gained much attention in Information Retrieval (IR) with its ability to achieve better text understanding by capturing contextual structure. However, to achieve better document understanding, it is necessary to involve global structure of a document. In this paper, we take the advantage of Graph Convolutional Networks (GCN) to model global word-relation structure of a document to improve context-aware document ranking. We propose to build a graph for a document to model the global structure. The nodes and edges of the graph are constructed from contextual embeddings. Then we apply graph convolution on the graph to learning a new representation, and this representation covers both contextual and global structure information. The experimental results show that our method outperforms the state-of-the-art contextual language models, which demonstrate that incorporating global structure is useful for improving document ranking and GCN is an effective way to achieve it.","tags":["\"Text Understanding\"","\"Contextualized Neural Language Models\"","\"Graph\"","\"Convolution Networks\""],"title":"CGTR:Convolution Graph Topology Representation for Document Ranking","type":"publication"},{"authors":["Keqing He","Yuanmeng Yan","Hong Xu","Sihong Liu","Zijun Liu","Weiran Xu"],"categories":[],"content":"","date":1595721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"bbc3080227b4a6c079b736aa826676b2","permalink":"https://pris-nlp.github.io/en/publication/learning-label-relational-output-structure-for-adaptive-sequence-labeling/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/learning-label-relational-output-structure-for-adaptive-sequence-labeling/","section":"publication","summary":"Sequence labeling is a fundamental task of natural language understanding. Recent neural models for sequence labeling task achieve significant success with the availability of sufficient training data. However, in practical scenarios, entity types to be annotated even in the same domain are continuously evolving. To transfer knowledge from the source model pre-trained on previously annotated data, we propose an approach which learns label-relational output structure to explicitly capturing label correlations in the latent space. Additionally, we construct the target-to-source interaction between the source model M S and the target model M T and apply a gate mechanism to control how much information in M S and M T should be passed down. Experiments show that our method consistently outperforms the state-of-the-art methods with a statistically significant margin and effectively facilitates to recognize rare new entities in the target data especially.","tags":["\"adaptive sequence labeling\"","\"transfer learning\"","\"label-relational output structure\"","\"latent space\""],"title":"Learning Label-Relational Output Structure for Adaptive Sequence Labeling","type":"publication"},{"authors":["Keqing He","Yuanmeng Yan","Weiran Xu"],"categories":[],"content":"","date":1595116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"c477c292e6854b6702d1eb514b4230f7","permalink":"https://pris-nlp.github.io/en/publication/adversarial-cross-lingual-transfer-learning-for-slot-tagging-of-low-resource-languages/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/adversarial-cross-lingual-transfer-learning-for-slot-tagging-of-low-resource-languages/","section":"publication","summary":"Slot tagging is a key component in a task-oriented dialogue system. Conversational agents need to understand human input by training on large amounts of annotated data. However, most human languages are low-resource and lack annotated training data for slot tagging task. Therefore, we aim to leverage cross-lingual transfer learning from high-resource languages to low-resource ones. In this paper, we propose an adversarial cross-lingual transfer model with multi-level language shared and specific knowledge to improve the slot tagging task of low-resource languages. Our method explicitly separates the model into the language-shared part and language-specific part to transfer language-independent knowledge. To refine shared knowledge in the latent space, we add a language discriminator and employ adversarial training to reinforce feature separation. Besides, we adopt a novel multi-level feature transfer in an incremental and progressive way to acquire multi-granularity shared knowledge. To mitigate the discrepancies between the feature distributions of language specific and shared knowledge, we propose the neural adapters to fuse features from different sources. Experiments show that our proposed model consistently outperforms monolingual baseline with a statistically significant margin up to 2.09%, even higher improvement of 12.21% in the zero-shot setting. Further analysis demonstrates that our method could effectively alleviate data scarcity of low-resource languages.","tags":["\"slot tagging\"","\"cross-lingual transfer learning\"","\"language discriminator\"","\"representation\"","\"neural adapter\""],"title":"Adversarial Cross-Lingual Transfer Learning for Slot Tagging of Low-Resource Languages","type":"publication"},{"authors":["Keqing He","Yuanmeng Yan","Weiran Xu"],"categories":[],"content":"","date":1593907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"086ca0957f4abebb8754d1f4bff90ab3","permalink":"https://pris-nlp.github.io/en/publication/learning-to-tag-oov-tokens-by-integrating-contextual-representation-and-background-knowledge/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/learning-to-tag-oov-tokens-by-integrating-contextual-representation-and-background-knowledge/","section":"publication","summary":"Neural-based context-aware models for slot tagging have achieved state-of-the-art performance. However, the presence of OOV(out-of-vocab) words significantly degrades the performance of neural-based models, especially in a few-shot scenario. In this paper, we propose a novel knowledge-enhanced slot tagging model to integrate contextual representation of input text and the large-scale lexical background knowledge. Besides, we use multi-level graph attention to explicitly model lexical relations. The experiments show that our proposed knowledge integration mechanism achieves consistent improvements across settings with different sizes of training data on two public benchmark datasets.","tags":["\"Slot Filling\""],"title":"Learning to Tag OOV Tokens by Integrating Contextual Representation and Background Knowledge","type":"publication"},{"authors":["Pengda Qin","XinWang","WenhuChen","ChunyunZhang","Weiran Xu","WilliamYangWang"],"categories":[],"content":"","date":1581033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600522724,"objectID":"d46e35e66847a328424370fdd170fc76","permalink":"https://pris-nlp.github.io/en/publication/generative-adversarial-zero-shot-relation-learning-for-knowledge-grapths/","publishdate":"2020-09-19T13:38:44.301342Z","relpermalink":"/en/publication/generative-adversarial-zero-shot-relation-learning-for-knowledge-grapths/","section":"publication","summary":"Large-scale knowledge graphs (KGs) are shown to become more important in current information systems. To expand the coverage of KGs, previous studies on knowledge graph completion need to collect adequate training instances for newly-added relations. In this paper, we consider a novel formulation, zero-shot learning, to free this cumbersome curation. For newly-added relations, we attempt to learn their semantic features from their text descriptions and hence recognize the facts of unseen relations with no examples being seen. For this purpose, we leverage Generative Adversarial Networks (GANs) to establish the connection between text and knowledge graph domain:The generator learns to generate the reasonable relation embeddings merely with noisy text descriptions. Under this setting, zero-shot learning is naturally converted to a traditional supervised classification task. Empirically, our method is model-agnostic that could be potentially applied to any version of KG embeddings, and consistently yields performance improvements on NELL and Wiki dataset.","tags":["\"Knowledge Graph Completion\""],"title":"Generative Adversarial Zero-Shot Relation Learning for Knowledge Grapths","type":"publication"},{"authors":["Weiran Xu","Keqing He","Yanan Wu","Yuanmeng Yan","Hong Xu","Sihong Liu","Zijun Liu"],"categories":[],"content":"","date":1574035200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574035200,"objectID":"290198b503d62d1657204867f47da26e","permalink":"https://pris-nlp.github.io/en/patent/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E7%BB%93%E6%9E%84%E5%8C%96%E7%94%A8%E6%88%B7%E5%B1%9E%E6%80%A7%E6%8F%8F%E8%BF%B0%E7%9A%84%E4%B8%AA%E6%80%A7%E5%8C%96%E4%BB%BB%E5%8A%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/","publishdate":"2019-11-18T00:00:00Z","relpermalink":"/en/patent/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E7%BB%93%E6%9E%84%E5%8C%96%E7%94%A8%E6%88%B7%E5%B1%9E%E6%80%A7%E6%8F%8F%E8%BF%B0%E7%9A%84%E4%B8%AA%E6%80%A7%E5%8C%96%E4%BB%BB%E5%8A%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/","section":"patent","summary":"","tags":[],"title":"一种基于结构化用户属性描述的个性化任务型对话系统","type":"patent"},{"authors":["Hong Xu","Weiran Xu","Sihong Liu","Keqing He","Yuanmeng Yan","Zijun Liu","Yanan Wu"],"categories":[],"content":"","date":1571356800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571356800,"objectID":"5001708f0a28902a5ce679b5ab56b8ec","permalink":"https://pris-nlp.github.io/en/patent/%E4%B8%80%E7%A7%8D%E7%9F%A5%E8%AF%86%E9%A9%B1%E5%8A%A8%E7%9A%84%E6%98%93%E9%85%8D%E7%BD%AE%E4%BB%BB%E5%8A%A1%E5%AF%BC%E5%90%91%E5%9E%8B%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86%E6%96%B9%E6%B3%95%E4%B8%8E%E7%B3%BB%E7%BB%9F/","publishdate":"2019-10-18T00:00:00Z","relpermalink":"/en/patent/%E4%B8%80%E7%A7%8D%E7%9F%A5%E8%AF%86%E9%A9%B1%E5%8A%A8%E7%9A%84%E6%98%93%E9%85%8D%E7%BD%AE%E4%BB%BB%E5%8A%A1%E5%AF%BC%E5%90%91%E5%9E%8B%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86%E6%96%B9%E6%B3%95%E4%B8%8E%E7%B3%BB%E7%BB%9F/","section":"patent","summary":"","tags":[],"title":"一种知识驱动的易配置任务导向型对话管理方法与系统","type":"patent"},{"authors":["Daishi","SiLi","YinanSun","ShengGao","YajingXu","Weiran Xu","GuangChen"],"categories":[],"content":"","date":1562544000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562544000,"objectID":"533212db21ceb7d036aa5b3495271654","permalink":"https://pris-nlp.github.io/en/patent/%E5%BB%BA%E6%A8%A1%E5%AF%B9%E8%AF%9D%E8%BD%AE%E6%AC%A1%E4%BF%A1%E6%81%AF%E7%9A%84%E6%A3%80%E7%B4%A2%E5%BC%8F%E9%97%B2%E8%81%8A%E5%AF%B9%E8%AF%9D%E6%89%93%E5%88%86%E6%96%B9%E6%B3%95/","publishdate":"2019-07-08T00:00:00Z","relpermalink":"/en/patent/%E5%BB%BA%E6%A8%A1%E5%AF%B9%E8%AF%9D%E8%BD%AE%E6%AC%A1%E4%BF%A1%E6%81%AF%E7%9A%84%E6%A3%80%E7%B4%A2%E5%BC%8F%E9%97%B2%E8%81%8A%E5%AF%B9%E8%AF%9D%E6%89%93%E5%88%86%E6%96%B9%E6%B3%95/","section":"patent","summary":"","tags":[],"title":"建模对话轮次信息的检索式闲聊对话打分方法","type":"patent"},{"authors":["HuaXu"],"categories":null,"content":"Course Classification: Public Elective Courses of Tsinghua University\nLecturer: Hua Xu\nTarget Audience: All Undergraduate Students\nTeaching Time：2019 - Today\n","date":1546272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546272000,"objectID":"407bfb7ec4580d302372641bb7e5039e","permalink":"https://pris-nlp.github.io/en/talk/internet-product-design/","publishdate":"2019-01-01T00:00:00+08:00","relpermalink":"/en/talk/internet-product-design/","section":"talk","summary":"Public Elective Courses of Tsinghua University","tags":[],"title":"Internet Product Design","type":"talk"},{"authors":["Pengda Qin","Weiran Xu","WilliamYangWang"],"categories":[],"content":"","date":1530489600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530489600,"objectID":"fafd275c4e23a10134e1f87c3a77f158","permalink":"https://pris-nlp.github.io/en/publication/robust-distant-supervision-relation-extraction-via-deep-reinforcement-learning/","publishdate":"2020-08-19T13:55:51.366191Z","relpermalink":"/en/publication/robust-distant-supervision-relation-extraction-via-deep-reinforcement-learning/","section":"publication","summary":"Distant supervision has become the standard method for relation extraction. However, even though it is an efficient method, it does not come at no cost—The resulted distantly-supervised training samples are often very noisy. To combat the noise, most of the recent state-of-the-art approaches focus on selecting one-best sentence or calculating soft attention weights over the set of the sentences of one specific entity pair. However, these methods are suboptimal, and the false positive problem is still a key stumbling bottleneck for the performance. We argue that those incorrectly-labeled candidate sentences must be treated with a hard decision, rather than being dealt with soft attention weights. To do this, our paper describes a radical solution—We explore a deep reinforcement learning strategy to generate the false-positive indicator, where we automatically recognize false positives for each relation type without any supervised information. Unlike the removal operation in the previous studies, we redistribute them into the negative examples. The experimental results show that the proposed strategy significantly improves the performance of distant supervision comparing to state-of-the-art systems.","tags":null,"title":"Robust Distant Supervision Relation Extraction via Deep Reinforcement Learning","type":"publication"},{"authors":["Pengda Qin","Weiran Xu","WilliamYangWang"],"categories":[],"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600523752,"objectID":"d444e1320092a7e3d7f72b015fbaac55","permalink":"https://pris-nlp.github.io/en/publication/dsgan-generative-adversarial-training-for-distant-supervision-relation-extraction/","publishdate":"2020-09-19T13:55:51.366191Z","relpermalink":"/en/publication/dsgan-generative-adversarial-training-for-distant-supervision-relation-extraction/","section":"publication","summary":"Distant supervision can effectively label data for relation extraction, but suffers from the noise labeling problem. Recent works mainly perform soft bag-level noise reduction strategies to find the relatively better samples in a sentence bag, which is suboptimal compared with making a hard decision of false positive samples in sentence level. In this paper, we introduce an adversarial learning framework, which we named DSGAN, to learn a sentence-level true-positive generator. Inspired by Generative Adversarial Networks, we regard the positive samples generated by the generator as the negative samples to train the discriminator. The optimal generator is obtained until the discrimination ability of the discriminator has the greatest decline. We adopt the generator to filter distant supervision training dataset and redistribute the false positive instances into the negative set, in which way to provide a cleaned dataset for relation classification. The experimental results show that the proposed strategy significantly improves the performance of distant supervision relation extraction comparing to state-of-the-art systems.","tags":null,"title":"DSGAN: Generative Adversarial Training for Distant Supervision Relation Extraction","type":"publication"},{"authors":["SiLi","ZuyiBao","Weiran Xu","ShengGao"],"categories":[],"content":"","date":1492992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1492992000,"objectID":"9b07acc9b039169e280ab7a2cbc3d5f1","permalink":"https://pris-nlp.github.io/en/patent/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E5%B5%8C%E5%85%A5%E5%BC%8F%E8%A1%A8%E7%A4%BA%E7%9A%84%E8%87%AA%E9%80%82%E5%BA%94%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95/","publishdate":"2017-04-24T00:00:00Z","relpermalink":"/en/patent/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E5%B5%8C%E5%85%A5%E5%BC%8F%E8%A1%A8%E7%A4%BA%E7%9A%84%E8%87%AA%E9%80%82%E5%BA%94%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95/","section":"patent","summary":"","tags":[],"title":"一种基于嵌入式表示的自适应中文分词方法","type":"patent"},{"authors":["ZuyiBao","SiLi","Weiran Xu"],"categories":[],"content":"","date":1492992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1492992000,"objectID":"c9382e1521faef0f1c470943165b62e9","permalink":"https://pris-nlp.github.io/en/patent/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E9%9A%90%E5%A4%9A%E7%B2%92%E5%BA%A6%E5%B1%80%E9%83%A8%E7%89%B9%E5%BE%81%E7%9A%84%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95/","publishdate":"2017-04-24T00:00:00Z","relpermalink":"/en/patent/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E9%9A%90%E5%A4%9A%E7%B2%92%E5%BA%A6%E5%B1%80%E9%83%A8%E7%89%B9%E5%BE%81%E7%9A%84%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95/","section":"patent","summary":"","tags":[],"title":"一种基于隐多粒度局部特征的中文分词方法","type":"patent"},{"authors":["HuaXu"],"categories":null,"content":"Course Classification: Public Elective Courses of Tsinghua University\nLecturer: Hua Xu\nTarget Audience: All Undergraduate Students\nTeaching Time：2016 - Today\n","date":1451577600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451577600,"objectID":"050eb509d73d451227a478cbd25c314e","permalink":"https://pris-nlp.github.io/en/talk/intelligent-mobile-robot/","publishdate":"2016-01-01T00:00:00+08:00","relpermalink":"/en/talk/intelligent-mobile-robot/","section":"talk","summary":"Public Elective Courses of Tsinghua University","tags":[],"title":"Intelligent Mobile Robot: Design, Programming and Practice","type":"talk"},{"authors":["HuaXu"],"categories":null,"content":"Course Classification: Public Elective Courses of Tsinghua University\nLecturer: Hua Xu\nTarget Audience: All Graduate and Undergraduate Students\nTeaching Time：2013 - Today\n","date":1356969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356969600,"objectID":"e7d4ecfaf4b1f29cd65dd9ec01f1f6ad","permalink":"https://pris-nlp.github.io/en/talk/industrial-data-mining/","publishdate":"2013-01-01T00:00:00+08:00","relpermalink":"/en/talk/industrial-data-mining/","section":"talk","summary":"Public Elective Courses of Tsinghua University","tags":[],"title":"Industrial Data Mining","type":"talk"},{"authors":["HuaXu"],"categories":null,"content":"Course Classification: Public Elective Courses of Tsinghua University\nLecturer: Hua Xu\nTarget Audience: All Undergraduate Students\nTeaching Time：2011 - Today\n","date":1293811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293811200,"objectID":"3e7631952ecff1e05471581a1af8e97d","permalink":"https://pris-nlp.github.io/en/talk/data-mining-method-and-application/","publishdate":"2011-01-01T00:00:00+08:00","relpermalink":"/en/talk/data-mining-method-and-application/","section":"talk","summary":"Public Elective Courses of Tsinghua University","tags":[],"title":"Data Mining: Methods and Applications","type":"talk"},{"authors":["Weiran Xu","ZhanyiWang","DongxinLiu","QiFang"],"categories":[],"content":"","date":1258070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1258070400,"objectID":"8335423f963cfa0e1f8095291c070504","permalink":"https://pris-nlp.github.io/en/patent/%E4%B8%80%E7%A7%8D%E7%9F%AD%E4%BF%A1%E7%9A%84%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95%E5%92%8C%E8%AE%BE%E5%A4%87/","publishdate":"2009-11-13T00:00:00Z","relpermalink":"/en/patent/%E4%B8%80%E7%A7%8D%E7%9F%AD%E4%BF%A1%E7%9A%84%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95%E5%92%8C%E8%AE%BE%E5%A4%87/","section":"patent","summary":"","tags":[],"title":"一种短信的识别方法和设备","type":"patent"},{"authors":["Weiran Xu","DongxinLiu","ZhanyiWang","JiachunDu"],"categories":[],"content":"","date":1241049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1241049600,"objectID":"d64632bbdea2aae7e69200d4fc20b3ab","permalink":"https://pris-nlp.github.io/en/patent/%E4%B8%80%E7%A7%8D%E7%9F%AD%E4%BF%A1%E8%BF%87%E6%BB%A4%E7%9A%84%E6%96%B9%E6%B3%95%E5%92%8C%E8%A3%85%E7%BD%AE/","publishdate":"2009-04-30T00:00:00Z","relpermalink":"/en/patent/%E4%B8%80%E7%A7%8D%E7%9F%AD%E4%BF%A1%E8%BF%87%E6%BB%A4%E7%9A%84%E6%96%B9%E6%B3%95%E5%92%8C%E8%A3%85%E7%BD%AE/","section":"patent","summary":"","tags":[],"title":"一种短信过滤的方法和装置","type":"patent"},{"authors":["PuYang","JunGuo","Weiran Xu"],"categories":[],"content":"","date":1237334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1237334400,"objectID":"ed911f5b24e54d4ff36a511e61e1eee4","permalink":"https://pris-nlp.github.io/en/patent/%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%E9%9B%86%E7%BE%A4%E7%B3%BB%E7%BB%9F/","publishdate":"2009-03-18T00:00:00Z","relpermalink":"/en/patent/%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%E9%9B%86%E7%BE%A4%E7%B3%BB%E7%BB%9F/","section":"patent","summary":"","tags":[],"title":"分布式爬虫集群系统","type":"patent"},{"authors":["PuYang","JunGuo","Weiran Xu"],"categories":[],"content":"","date":1237334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1237334400,"objectID":"5e905ec1fdafe8f5121c2211943507e9","permalink":"https://pris-nlp.github.io/en/patent/%E9%98%B2%E5%81%87%E6%AD%BB%E7%88%AC%E8%99%AB%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%9E%84%E5%BB%BA%E6%96%B9%E6%B3%95/","publishdate":"2009-03-18T00:00:00Z","relpermalink":"/en/patent/%E9%98%B2%E5%81%87%E6%AD%BB%E7%88%AC%E8%99%AB%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%9E%84%E5%BB%BA%E6%96%B9%E6%B3%95/","section":"patent","summary":"","tags":[],"title":"防假死爬虫系统的构建方法","type":"patent"},{"authors":["PuYang","JunGuo","Weiran Xu"],"categories":[],"content":"","date":1227571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1227571200,"objectID":"beb355b6e727e985c23a61c317c7df5a","permalink":"https://pris-nlp.github.io/en/patent/%E5%90%8C%E8%AF%9D%E9%A2%98%E5%AE%9A%E4%BD%8D%E8%B7%9F%E8%B8%AA%E5%BC%8F%E8%AE%BA%E5%9D%9B%E7%88%AC%E8%99%AB%E7%B3%BB%E7%BB%9F/","publishdate":"2008-11-25T00:00:00Z","relpermalink":"/en/patent/%E5%90%8C%E8%AF%9D%E9%A2%98%E5%AE%9A%E4%BD%8D%E8%B7%9F%E8%B8%AA%E5%BC%8F%E8%AE%BA%E5%9D%9B%E7%88%AC%E8%99%AB%E7%B3%BB%E7%BB%9F/","section":"patent","summary":"","tags":[],"title":"同话题定位跟踪式论坛爬虫系统","type":"patent"},{"authors":["PuYang","JunGuo","Weiran Xu"],"categories":[],"content":"","date":1227571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1227571200,"objectID":"a0bce571fdebdebd57bd7324fcd6a4d2","permalink":"https://pris-nlp.github.io/en/patent/%E5%B8%83%E5%91%8A%E6%A0%8F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E7%9A%84%E7%88%AC%E8%99%AB%E7%B3%BB%E7%BB%9F%E6%9E%84%E5%BB%BA%E6%96%B9%E6%B3%95/","publishdate":"2008-11-25T00:00:00Z","relpermalink":"/en/patent/%E5%B8%83%E5%91%8A%E6%A0%8F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E7%9A%84%E7%88%AC%E8%99%AB%E7%B3%BB%E7%BB%9F%E6%9E%84%E5%BB%BA%E6%96%B9%E6%B3%95/","section":"patent","summary":"","tags":[],"title":"布告栏搜索引擎的爬虫系统构建方法","type":"patent"},{"authors":["PuYang","JunGuo","Weiran Xu"],"categories":[],"content":"","date":1227571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1227571200,"objectID":"5b0c68e2014051e68cfd76ee3adcbfe0","permalink":"https://pris-nlp.github.io/en/patent/%E5%B9%B6%E8%A1%8C%E5%BC%8F%E5%85%B3%E8%81%94%E5%B8%83%E5%91%8A%E6%A0%8F%E7%88%AC%E8%99%AB%E7%B3%BB%E7%BB%9F/","publishdate":"2008-11-25T00:00:00Z","relpermalink":"/en/patent/%E5%B9%B6%E8%A1%8C%E5%BC%8F%E5%85%B3%E8%81%94%E5%B8%83%E5%91%8A%E6%A0%8F%E7%88%AC%E8%99%AB%E7%B3%BB%E7%BB%9F/","section":"patent","summary":"","tags":[],"title":"并行式关联布告栏爬虫系统","type":"patent"},{"authors":["PuYang","JunGuo","Weiran Xu"],"categories":[],"content":"","date":1227571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1227571200,"objectID":"33a28ddc2ab4c3abd236925d2a51602e","permalink":"https://pris-nlp.github.io/en/patent/%E8%87%AA%E5%8A%A8%E5%8A%A8%E6%80%81%E6%9B%B4%E6%96%B0%E8%AE%BA%E5%9D%9B%E7%88%AC%E8%99%AB%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%9E%84%E5%BB%BA%E6%96%B9%E6%B3%95/","publishdate":"2008-11-25T00:00:00Z","relpermalink":"/en/patent/%E8%87%AA%E5%8A%A8%E5%8A%A8%E6%80%81%E6%9B%B4%E6%96%B0%E8%AE%BA%E5%9D%9B%E7%88%AC%E8%99%AB%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%9E%84%E5%BB%BA%E6%96%B9%E6%B3%95/","section":"patent","summary":"","tags":[],"title":"自动动态更新论坛爬虫系统的构建方法","type":"patent"},{"authors":["PuYang","JunGuo","Weiran Xu"],"categories":[],"content":"","date":1214524800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1214524800,"objectID":"d95c2984046fab93affb647ddd435e14","permalink":"https://pris-nlp.github.io/en/patent/%E5%86%85%E9%83%A8%E7%BD%91%E5%8F%AF%E5%AE%9A%E5%88%B6%E7%88%AC%E8%99%AB%E7%B3%BB%E7%BB%9F%E6%9E%84%E5%BB%BA%E6%96%B9%E6%B3%95/","publishdate":"2008-06-27T00:00:00Z","relpermalink":"/en/patent/%E5%86%85%E9%83%A8%E7%BD%91%E5%8F%AF%E5%AE%9A%E5%88%B6%E7%88%AC%E8%99%AB%E7%B3%BB%E7%BB%9F%E6%9E%84%E5%BB%BA%E6%96%B9%E6%B3%95/","section":"patent","summary":"","tags":[],"title":"内部网可定制爬虫系统构建方法","type":"patent"},{"authors":["PuYang","JunGuo","Weiran Xu"],"categories":[],"content":"","date":1214524800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1214524800,"objectID":"3d670f3ae8eb0472cae9fc7ab9698c31","permalink":"https://pris-nlp.github.io/en/patent/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%96%AD%E7%82%B9%E7%BB%AD%E4%BC%A0%E5%8F%AF%E5%AE%9A%E5%88%B6%E5%86%85%E9%83%A8%E7%BD%91%E7%88%AC%E8%99%AB%E7%B3%BB%E7%BB%9F/","publishdate":"2008-06-27T00:00:00Z","relpermalink":"/en/patent/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%96%AD%E7%82%B9%E7%BB%AD%E4%BC%A0%E5%8F%AF%E5%AE%9A%E5%88%B6%E5%86%85%E9%83%A8%E7%BD%91%E7%88%AC%E8%99%AB%E7%B3%BB%E7%BB%9F/","section":"patent","summary":"","tags":[],"title":"多线程断点续传可定制内部网爬虫系统","type":"patent"},{"authors":["RuifangLiu","ShengGao","Weiran Xu","XinZhou","XiaoxinHe"],"categories":[],"content":"","date":1210291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1210291200,"objectID":"8ca7a04f158ea4a16f469a89f85c0315","permalink":"https://pris-nlp.github.io/en/patent/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E6%B7%B7%E5%8F%A0%E4%BF%A1%E5%8F%B7%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%B7%B3%E9%A2%91%E4%BF%A1%E5%8F%B7%E5%88%A4%E5%88%AB%E6%96%B9%E6%B3%95/","publishdate":"2008-05-09T00:00:00Z","relpermalink":"/en/patent/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E6%B7%B7%E5%8F%A0%E4%BF%A1%E5%8F%B7%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%B7%B3%E9%A2%91%E4%BF%A1%E5%8F%B7%E5%88%A4%E5%88%AB%E6%96%B9%E6%B3%95/","section":"patent","summary":"","tags":[],"title":"一种基于混叠信号深度学习的跳频信号判别方法","type":"patent"},{"authors":["Weiran Xu","GangLiu","JunGuo","HonggangZhang"],"categories":[],"content":"","date":1040601600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1040601600,"objectID":"727b3252c03b39978223f9b4f00c0a0f","permalink":"https://pris-nlp.github.io/en/patent/%E5%9F%BA%E4%BA%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84%E6%96%87%E5%AD%97%E5%AD%97%E4%BD%93%E5%88%A4%E6%96%AD%E8%AE%BE%E5%A4%87%E5%8F%8A%E5%85%B6%E6%96%B9%E6%B3%95/","publishdate":"2002-12-23T00:00:00Z","relpermalink":"/en/patent/%E5%9F%BA%E4%BA%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84%E6%96%87%E5%AD%97%E5%AD%97%E4%BD%93%E5%88%A4%E6%96%AD%E8%AE%BE%E5%A4%87%E5%8F%8A%E5%85%B6%E6%96%B9%E6%B3%95/","section":"patent","summary":"","tags":[],"title":"基于贝叶斯分类器的文字字体判断设备及其方法","type":"patent"},{"authors":["GangLiu","Weiran Xu","JunGuo","RuihongZheng","HonggangZhang"],"categories":[],"content":"","date":1040601600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1040601600,"objectID":"dcb5184244faf499a588648530a2c9c3","permalink":"https://pris-nlp.github.io/en/patent/%E7%94%A8%E4%BA%8E%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB%E7%9A%84%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%E8%87%AA%E5%8A%A8%E6%8C%91%E9%80%89%E8%A3%85%E7%BD%AE%E5%8F%8A%E5%85%B6%E6%96%B9%E6%B3%95/","publishdate":"2002-12-23T00:00:00Z","relpermalink":"/en/patent/%E7%94%A8%E4%BA%8E%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB%E7%9A%84%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%E8%87%AA%E5%8A%A8%E6%8C%91%E9%80%89%E8%A3%85%E7%BD%AE%E5%8F%8A%E5%85%B6%E6%96%B9%E6%B3%95/","section":"patent","summary":"","tags":[],"title":"用于文字识别的训练样本自动挑选装置及其方法","type":"patent"},{"authors":["JunhuiDeng"],"categories":null,"content":"Course Classification: Tsinghua University Computer Department Undergraduate Professional Basic Course\nLecturer：Junhui Deng\nTarget Audience: Computer Science Undergraduate\nTeaching Time：2001 - 2006\n","date":978278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":978278400,"objectID":"b7caabb0e4bde45ecc0519114b6a051c","permalink":"https://pris-nlp.github.io/en/talk/data-structure-cs/","publishdate":"2001-01-01T00:00:00+08:00","relpermalink":"/en/talk/data-structure-cs/","section":"talk","summary":"Tsinghua University Computer Department Undergraduate Professional Basic Course","tags":[],"title":"Data Strcture","type":"talk"},{"authors":["JunhuiDeng"],"categories":null,"content":"Course Classification: Public Elective Courses of Tsinghua University\nLecturer：Junhui Deng\nTarget Audience: All Undergraduate Students\nTeaching Time：2002 - Today\n","date":978278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":978278400,"objectID":"970c56d891d3e1abdca0aaf1b5140c8d","permalink":"https://pris-nlp.github.io/en/talk/data-structure/","publishdate":"2001-01-01T00:00:00+08:00","relpermalink":"/en/talk/data-structure/","section":"talk","summary":"National Excellent Course, Public Elective Course of Tsinghua University","tags":[],"title":"Data Strcture","type":"talk"},{"authors":["JunhuiDeng","HuaXu"],"categories":null,"content":"Course Classification: Tsinghua University Computer Department Graduate Basic Theory Course\nLecturer：Junhui Deng, Hua Xu\nTarget Audience: All Graduate Students\nTeaching Time：1997 - Today\n","date":852048000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":852048000,"objectID":"f93308c8be1670e9b2dc795f64282a4a","permalink":"https://pris-nlp.github.io/en/talk/computational-geometry/","publishdate":"1997-01-01T00:00:00+08:00","relpermalink":"/en/talk/computational-geometry/","section":"talk","summary":"Tsinghua University Computer Department Graduate Basic Theory Course","tags":[],"title":"Computational Geometry","type":"talk"},{"authors":["ShiningFu","YuOuJiang","Songyan Liu","Zongai Xie"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"81cfb197ded20cb72e29f2b0f2d75a31","permalink":"https://pris-nlp.github.io/en/project/cqa%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/project/cqa%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/","section":"project","summary":"社区问答（community question answering, CQA）在近些年得到了广泛关注，随着百度知道、知乎、搜狗问问、Stack Overflow等社区问答网站的出现，越来越多的人选择网络社区来获取答案。CQA与一般QA任务的不同在于问题和答案是开放领域的，通常较长，包含多个句子，文本不是结构化的，且含有大量噪音。一般的CQA系统构建步骤如下：a)用搜索引擎比如Lucene先离线构建问题-答案对的索引；b)在线收到query后，初步召回一组候选答案构成的集合；c)用文本匹配算法和排序算法对候选答案重新排序并返回最佳答案。","tags":[],"title":"CQA系统设计与实现","type":"project"},{"authors":["XiusenGu","ChenDing"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"810a5371f9a51746b57fcc1908de410b","permalink":"https://pris-nlp.github.io/en/project/%E4%B8%8D%E8%89%AF%E8%8D%AF%E7%89%A9%E5%8F%8D%E5%BA%94%E6%8A%BD%E5%8F%96%E8%AF%84%E6%B5%8B/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/project/%E4%B8%8D%E8%89%AF%E8%8D%AF%E7%89%A9%E5%8F%8D%E5%BA%94%E6%8A%BD%E5%8F%96%E8%AF%84%E6%B5%8B/","section":"project","summary":"该评测主要对药品说明书中进行不良药物反应的实体抽取和关系抽取两个子任务，其中实体抽取以BiLSTM-CRF为主要模型，并融合了词向量和字向量作为表征；关系抽取针对标注语料少的问题，采用了对抗噪声的方法作为数据增强的手段。","tags":[],"title":"不良药物反应抽取评测","type":"project"},{"authors":["Jingbo Shi","Songyan Liu","Yuejie Lei","Zhiyuan Zeng","Yuanmeng Yan","Fujia Zheng","Xuefeng Li"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c6f8ce4e98dbc35bc3179674f3802755","permalink":"https://pris-nlp.github.io/en/project/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%BB%BA%E8%AE%BE%E9%A1%B9%E7%9B%AE%E8%BF%90%E7%BB%B4%E7%9F%A5%E8%AF%86%E5%AD%90%E4%BB%BB%E5%8A%A1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/project/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%BB%BA%E8%AE%BE%E9%A1%B9%E7%9B%AE%E8%BF%90%E7%BB%B4%E7%9F%A5%E8%AF%86%E5%AD%90%E4%BB%BB%E5%8A%A1/","section":"project","summary":"该项目针对的场景是客服为用户进行运维工作后，针对已形成的工单，从中获得按照故障原因以及解决措施的形式的运维知识，并且结构化表示运维知识以形成一个运维知识库；即该项目分为两部分，一部分是运维知识挖掘系统，另一部分是运维知识库的构建。","tags":[],"title":"人工智能建设项目——运维知识子任务","type":"project"},{"authors":["Jingbo Shi","Songyan Liu","Zongai Xie","Yufeng Li","JianingZhang","Zhiyuan Zeng","Yuejie Lei","JinzhengZhao","Fujia Zheng"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"74b30f4cfe76e1cd8e1fbeccc8fa6ab1","permalink":"https://pris-nlp.github.io/en/project/%E4%BC%9A%E8%AE%AE%E5%9C%BA%E6%99%AF%E8%87%AA%E5%8A%A8%E6%91%98%E8%A6%81%E7%B3%BB%E7%BB%9F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/project/%E4%BC%9A%E8%AE%AE%E5%9C%BA%E6%99%AF%E8%87%AA%E5%8A%A8%E6%91%98%E8%A6%81%E7%B3%BB%E7%BB%9F/","section":"project","summary":"该项目应用在多人会议场景，主要通过会议整体内容提炼以及各发言者主要观点总结，形成篇幅较短但涵盖主要信息的生成式会议摘要。通过阅读摘要能够在短时间内形成对会议内容以及个人观点的大体认知，有效提高了多人协作过程中理解和沟通的效率。系统整体建立在无监督的基础上，主要通过大规模预训练语言模型在资源相对丰富的新闻摘要数据集上进行微调后，迁移至会议场景进行生成式摘要任务。考虑到场景之间的差异，通过自监督方法对生成的摘要进行风格迁移，使之具备更佳的可读性。设计了发言者画像库，用于收集和使用代表性的个人发言，以更好的表征语言习惯、关注点等个人特征，促进模型更精确和高效的定位关键信息。","tags":[],"title":"会议场景自动摘要系统","type":"project"},{"authors":["Keqing He","Hong Xu","Sihong Liu","Yuanmeng Yan","Zijun Liu"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"47f4f1bd642991a4303782c9988dd74e","permalink":"https://pris-nlp.github.io/en/project/%E5%9C%A8%E7%BA%BF%E6%95%99%E8%82%B2%E5%9C%BA%E6%99%AF%E4%B8%8B%E7%9A%84%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E7%B3%BB%E7%BB%9F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/project/%E5%9C%A8%E7%BA%BF%E6%95%99%E8%82%B2%E5%9C%BA%E6%99%AF%E4%B8%8B%E7%9A%84%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E7%B3%BB%E7%BB%9F/","section":"project","summary":"在线教育场景下通过人机对话的方式来完成特定的任务，主要包括以下三个方面：自然语言理解（NLU）：包括领域识别、意图识别、槽位提取三个子模块。通过领域识别的输出触发不同的场景，由意图识别和槽位提取将自然语言转换成系统可以理解的结构化表示。对话管理（DM）：包括对话状态追踪和对话策略两个部分。对话状态追踪根据用户输入的结构化表示和历史对话信息，更新当前的对话状态；对话策略模块根据当前的对话状态和用户输入的结构化表示进行决策，输出系统回复的结构化表示。自然语言生成（NLG）：将系统输出的结构化表示转化为自然语言输出。","tags":[],"title":"在线教育场景下的智能客服系统","type":"project"},{"authors":["XiusenGu","YanLi"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5b3b76480963135db9c98f01a3746bc4","permalink":"https://pris-nlp.github.io/en/project/%E5%9F%BA%E4%BA%8E%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E7%9A%84%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/project/%E5%9F%BA%E4%BA%8E%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E7%9A%84%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/","section":"project","summary":"利用实体识别和关系抽取技术，从百度搜索返回的候选结果中抽取答案实体返回。可识别预定义的10种关系。项目中的关系抽取采用了TextCNN、SVM和模板匹配三种模型，并根据模型的置信度进行了集成。","tags":[],"title":"基于搜索引擎的问答系统","type":"project"},{"authors":["JiandongSun","XiusenGu","ChenliangLi","ChenDing","YanLi","YifanWang","XinyuanLi"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"446e2def38c930807628d2de922f952a","permalink":"https://pris-nlp.github.io/en/project/%E5%A4%9A%E6%BA%90%E5%A4%9A%E6%A8%A1%E6%80%81%E9%A2%91%E8%B0%B1%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/project/%E5%A4%9A%E6%BA%90%E5%A4%9A%E6%A8%A1%E6%80%81%E9%A2%91%E8%B0%B1%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/","section":"project","summary":"针对各种无线通信技术构建频谱知识图谱。知识图谱，是结构化的语义知识库，用于迅速描述物理世界中的概念及其相互关系，通过将数据粒度从document级别降到data级别，聚合大量知识，从而实现知识的快速响应和推理。知识图谱技术与各行业的深度融合已经成为一个重要趋势。该知识图谱将被用于实际信号的仿真，分离和识别。","tags":[],"title":"多源多模态频谱知识图谱","type":"project"},{"authors":["ChaoZhang","XiongxiYu","PuWang","BaohangZhan","JialeHong","Keqing He","Hong Xu","Sihong Liu"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"114f2d8ce970d0a4a871bf632f5e58da","permalink":"https://pris-nlp.github.io/en/project/%E5%A4%9A%E8%BD%AE%E6%9C%BA%E7%A5%A8%E9%A2%84%E8%AE%A2%E6%9F%A5%E8%AF%A2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/project/%E5%A4%9A%E8%BD%AE%E6%9C%BA%E7%A5%A8%E9%A2%84%E8%AE%A2%E6%9F%A5%E8%AF%A2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/","section":"project","summary":"以机票的预订以及查询为任务背景，构建的特定任务的多轮对话系统。其主要模块分为自然语言处理（NLU），对话管理（DM），自然语言生成（NLG），机票知识库，并通过PIPELINE的形式进行连接构成整个系统。其中NLU模块中的意图识别、实体识别；DM模块中的策略决策，均采用当前最新的神经网络模型得到，同时也为模型的泛化，提供可能。","tags":[],"title":"多轮机票预订查询对话系统","type":"project"},{"authors":["Keqing He","Sihong Liu","Hong Xu","Yuanmeng Yan","Zijun Liu","Liwen Wang","Yanan Wu"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"275fdff6079688df969792546513cdc3","permalink":"https://pris-nlp.github.io/en/project/%E5%AD%A6%E5%91%98%E5%9F%B9%E8%AE%AD%E6%A8%A1%E6%8B%9F%E7%94%A8%E6%88%B7%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/project/%E5%AD%A6%E5%91%98%E5%9F%B9%E8%AE%AD%E6%A8%A1%E6%8B%9F%E7%94%A8%E6%88%B7%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/","section":"project","summary":"该项目的落地场景为保险行业销售员培训系统，其中本项目的对话系统担任模拟客户的角色，通过和真实学员的对话交互来对学员的语言表述进行打分，借此提高保险销售学员的业务能力。整体对话系统采用经典管道结构，分为自然语言理解、对话管理和自然语言生成三个部分。其中自然语言处理除了基本的实体意图识别还加入了多意图识别的考量，对话管理部分采用分流程分阶段控制对话的规则设定。在项目后期，整体会加入情感分析，即学员的表达中蕴含的情感因素也会成为打分的评分点。","tags":[],"title":"学员培训模拟用户对话系统","type":"project"},{"authors":["Jingbo Shi","Songyan Liu","Zongai Xie","Yufeng Li","JianingZhang","YujieLei","Zhiyuan Zeng","Fujia Zheng","Xuefeng Li"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cb93aadb9f4c3912d2b775e5e6faec7b","permalink":"https://pris-nlp.github.io/en/project/%E5%B7%A5%E5%8D%95%E5%AF%B9%E8%AF%9D%E6%91%98%E8%A6%81%E9%A1%B9%E7%9B%AE/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/project/%E5%B7%A5%E5%8D%95%E5%AF%B9%E8%AF%9D%E6%91%98%E8%A6%81%E9%A1%B9%E7%9B%AE/","section":"project","summary":"该项目针对场景是用户与客服进行电话咨询，系统通过用户与客服之间的多轮对话通过摘要的方法生成相应的用户提出的问题以及客服给出的诊断以及解决方案的自然语言文本作为该次会话的工单记录。","tags":[],"title":"工单对话摘要项目","type":"project"},{"authors":["ChunyuMa","YufeiShen","Jingbo Shi"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bbb232daa62fc5e164cf68d426c654f3","permalink":"https://pris-nlp.github.io/en/project/%E6%83%85%E6%84%9F%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/project/%E6%83%85%E6%84%9F%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/","section":"project","summary":"本项目针对闲聊领域中情感对话的生成，旨在设计并实现一个能理解用户输入情感并给出恰当情感回复的聊天机器人。具体功能被划分为三个部分，首先需要充分理解用户输入所包含的情感；其实需要根据用户的输入情感计算出回复时应该包含的情感；最后根据用户输入生成一句包含目标情感的回复。","tags":[],"title":"情感对话系统","type":"project"},{"authors":["Keqing He","SongYan","Hong Xu","Sihong Liu","Yuanmeng Yan"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"361f03249fe398e87969a62e88960cb0","permalink":"https://pris-nlp.github.io/en/project/%E6%99%BA%E8%83%BD%E8%AF%AD%E9%9F%B3%E8%B4%A8%E6%A3%80%E7%B3%BB%E7%BB%9F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/project/%E6%99%BA%E8%83%BD%E8%AF%AD%E9%9F%B3%E8%B4%A8%E6%A3%80%E7%B3%BB%E7%BB%9F/","section":"project","summary":"针对某银行大型呼叫中心产生的海量非结构化录音内容进行自动化检测、分析和挖掘，极大地提升传统人工质检的效率与准确率，充分利用数据本身的价值信息。\n","tags":[],"title":"智能语音质检系统","type":"project"},{"authors":["ChaoZhang","XingxiYu","PuWang","BohangZhan","JialeHong","Keqing He","Hong Xu","Sihong Liu"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"288a0111f80c9908da59d6c1bfd1482c","permalink":"https://pris-nlp.github.io/en/project/%E7%89%B9%E5%AE%9A%E6%8A%A5%E4%BF%AE%E5%9C%BA%E6%99%AF%E4%B8%8B%E7%9A%84%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/project/%E7%89%B9%E5%AE%9A%E6%8A%A5%E4%BF%AE%E5%9C%BA%E6%99%AF%E4%B8%8B%E7%9A%84%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/","section":"project","summary":"系统最大的特点在于系统中各个模块的设置均可采用加载配置文件的方法来完成，同时加载不同领域的配置文件可以完成不同领域对话系统的切换。","tags":[],"title":"特定报修场景下的多轮对话系统","type":"project"},{"authors":["JiandongSun","ChenliangLi","WeijieLiu","ShiningPu","YuouJiang","BaohangZhan","XiongxiYu","JialeHong","Keqing He","Hong Xu","Sihong Liu","Zijun Liu"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c869831e135fbffc2412cdf832caa827","permalink":"https://pris-nlp.github.io/en/project/%E7%9F%A5%E8%AF%86%E5%BA%93%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/project/%E7%9F%A5%E8%AF%86%E5%BA%93%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/","section":"project","summary":"实现一个具有 QA 功能的知识搜索引擎。 该知识搜索引擎需要具有以下七个功能：                  – 计算:四则运算、费用计算、单位换算等; – 日历:跟日期、日期、干支等相关的提问;– 时间:问及日本及世界各地时刻的提问 – 定义:关于某一事物的定义提问 – 天气：关于天气进行提问 – 联想:由某个词语联想出来的事物 – 事实型:通过简短的语言来寻求简介回答的提问","tags":[],"title":"知识库问答系统","type":"project"},{"authors":["XiusenGu","ChenDing","ChaoZhang","ChunyuMa","PuWang"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"780c38932d9d87ec97af98e4dea3be9d","permalink":"https://pris-nlp.github.io/en/project/%E9%97%B2%E8%81%8A%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/project/%E9%97%B2%E8%81%8A%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/","section":"project","summary":"本系统包括三个垂直领域的任务型对话和闲聊对话，采用模块分层解耦设计，并配有聊天界面，是一个较完整、具备扩展性的智能对话系统。具体地，系统主要分为自然语言理解（NLU）模块、对话管理（DM）、自然语言生成（NLG）模块和前端界面四个模块。三个垂直领域包括电子产品、城市、明星，各个垂直领域会针对性地设置不同任务对话。项目中遇到了闲聊对话中的对话多样性问题，尝试利用了条件变分自动编码器；探索了情感在闲聊对话中的引入；在NLU的情感识别时，遇到了多分类问题中的数据不均衡问题，采用采样法、损失函数加权等方法缓解。","tags":[],"title":"闲聊对话系统","type":"project"},{"authors":["XiusenGu","ChenDing"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"73c8ac978bb9d06c337931368d65ee3b","permalink":"https://pris-nlp.github.io/en/project/%E9%9D%A2%E5%90%912022%E5%86%AC%E6%AE%8B%E5%A5%A5%E8%A7%86%E9%9A%9C%E4%BA%BA%E7%BE%A4%E7%9A%84%E5%A4%9A%E8%BD%AE%E4%BA%BA%E6%9C%BA%E5%AF%B9%E8%AF%9D%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/project/%E9%9D%A2%E5%90%912022%E5%86%AC%E6%AE%8B%E5%A5%A5%E8%A7%86%E9%9A%9C%E4%BA%BA%E7%BE%A4%E7%9A%84%E5%A4%9A%E8%BD%AE%E4%BA%BA%E6%9C%BA%E5%AF%B9%E8%AF%9D%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/","section":"project","summary":"该评测主要对药品说明书中进行不良药物反应的实体抽取和关系抽取两个子任务，其中实体抽取以BiLSTM-CRF为主要模型，并融合了词向量和字向量作为表征；关系抽取针对标注语料少的问题，采用了对抗噪声的方法作为数据增强的手段。","tags":[],"title":"面向2022冬残奥视障人群的多轮人机对话与情感分析","type":"project"}]